# -*- coding: utf-8 -*-
"""Untitled65.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgjgYUdY6fMgFrC3XxZzrkBupiGkGu38
"""

"""
DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN
Block A: Data Loading & Preprocessing
"""

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("=" * 80)
print("BLOCK A: DATA LOADING & PREPROCESSING")
print("=" * 80)

# ============================================================================
# 1. DATA LOADING
# ============================================================================
print("\n[1] Loading Data...")

# For Google Colab - upload file
from google.colab import files
uploaded = files.upload()

# Get the filename
filename = list(uploaded.keys())[0]
print(f"✓ File uploaded: {filename}")

# Load data based on file extension
if filename.endswith('.csv'):
    df = pd.read_csv(filename)
elif filename.endswith('.xlsx'):
    df = pd.read_excel(filename)
else:
    raise ValueError("File must be .csv or .xlsx format")

print(f"✓ Data loaded: {df.shape[0]:,} rows × {df.shape[1]} columns")
print(f"\nColumns: {list(df.columns)}")

# ============================================================================
# 2. DATE PARSING & VALIDATION
# ============================================================================
print("\n[2] Parsing Dates...")

# Parse Date column with automatic format inference
df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, utc=True)

# Convert to date only (remove time component if present)
df['Date'] = df['Date'].dt.date
df['Date'] = pd.to_datetime(df['Date'])

print(f"✓ Date range: {df['Date'].min()} to {df['Date'].max()}")
print(f"✓ Total days in dataset: {(df['Date'].max() - df['Date'].min()).days + 1}")

# ============================================================================
# 3. CREATE CONTINUOUS CALENDAR FOR EACH SKU
# ============================================================================
print("\n[3] Creating Continuous Calendar...")

# Get all unique SKUs and date range
all_skus = df['SKU_ID'].unique()
date_range = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='D')

print(f"✓ Unique SKUs: {len(all_skus):,}")
print(f"✓ Date range: {len(date_range)} days")

# Create complete calendar for all SKU-Date combinations
complete_calendar = pd.DataFrame(
    [(sku, date) for sku in all_skus for date in date_range],
    columns=['SKU_ID', 'Date']
)

print(f"✓ Complete calendar created: {len(complete_calendar):,} SKU-Date combinations")

# Merge with original data
df_original_size = len(df)
df = complete_calendar.merge(df, on=['SKU_ID', 'Date'], how='left')

print(f"✓ Data after merge: {len(df):,} rows ({len(df) - df_original_size:,} new rows added)")

# ============================================================================
# 4. HANDLE MISSING VALUES - UNITS_SOLD
# ============================================================================
print("\n[4] Handling Missing Values - Units_Sold...")

units_sold_na = df['Units_Sold'].isna().sum()
print(f"Missing Units_Sold: {units_sold_na:,} ({units_sold_na/len(df)*100:.2f}%)")

# Fill missing Units_Sold with 0 (no sales on those days)
df['Units_Sold'] = df['Units_Sold'].fillna(0).astype(int)
print("✓ Units_Sold: Missing values filled with 0")

# ============================================================================
# 5. HANDLE MISSING VALUES - INVENTORY_LEVEL
# ============================================================================
print("\n[5] Handling Missing Values - Inventory_Level...")

inventory_na = df['Inventory_Level'].isna().sum()
print(f"Missing Inventory_Level: {inventory_na:,} ({inventory_na/len(df)*100:.2f}%)")

# Sort by SKU and Date for proper forward-filling
df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)

# Forward-fill Inventory_Level within each SKU group
df['Inventory_Level'] = df.groupby('SKU_ID')['Inventory_Level'].fillna(method='ffill')

# Flag rows with true NA (couldn't be filled)
df['Inventory_NA_Flag'] = df['Inventory_Level'].isna().astype(int)
remaining_na = df['Inventory_Level'].isna().sum()

print(f"✓ Inventory_Level: Forward-filled within SKU groups")
print(f"  Remaining NA (flagged for review): {remaining_na:,}")

# Fill remaining NAs with 0 (assume starting inventory is 0 if no prior data)
df['Inventory_Level'] = df['Inventory_Level'].fillna(0).astype(int)

# ============================================================================
# 6. HANDLE MISSING VALUES - SUPPLIER_LEAD_TIME_DAYS
# ============================================================================
print("\n[6] Handling Missing Values - Supplier_Lead_Time_Days...")

lead_time_na = df['Supplier_Lead_Time_Days'].isna().sum()
print(f"Missing Supplier_Lead_Time_Days: {lead_time_na:,}")

# Calculate median lead time per supplier
supplier_median_lead_time = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].median()

# Impute missing values with supplier median
df['Supplier_Lead_Time_Days'] = df.apply(
    lambda row: supplier_median_lead_time[row['Supplier_ID']]
    if pd.isna(row['Supplier_Lead_Time_Days']) and row['Supplier_ID'] in supplier_median_lead_time
    else row['Supplier_Lead_Time_Days'],
    axis=1
)

print("✓ Supplier_Lead_Time_Days: Imputed with median per supplier")

# ============================================================================
# 7. FILL OTHER MISSING VALUES
# ============================================================================
print("\n[7] Filling Other Missing Values...")

# Fill categorical columns with forward-fill per SKU
categorical_cols = ['Warehouse_ID', 'Supplier_ID', 'Region']
for col in categorical_cols:
    if col in df.columns:
        df[col] = df.groupby('SKU_ID')[col].fillna(method='ffill')
        df[col] = df.groupby('SKU_ID')[col].fillna(method='bfill')

# Fill numeric columns
numeric_fill_cols = ['Reorder_Point', 'Order_Quantity', 'Unit_Cost', 'Unit_Price', 'Demand_Forecast']
for col in numeric_fill_cols:
    if col in df.columns:
        df[col] = df.groupby('SKU_ID')[col].fillna(method='ffill')
        df[col] = df.groupby('SKU_ID')[col].fillna(method='bfill')

# Fill binary flags with 0
flag_cols = ['Promotion_Flag', 'Stockout_Flag']
for col in flag_cols:
    if col in df.columns:
        df[col] = df[col].fillna(0).astype(int)

print("✓ All missing values handled")

# ============================================================================
# 8. VALIDATE UNITS_SOLD vs INVENTORY_LEVEL LOGIC
# ============================================================================
print("\n[8] Validating Units_Sold vs Inventory_Level Logic...")

# Check for impossible scenarios (sold more than inventory on stockout days)
df['Potential_Issue'] = (
    (df['Units_Sold'] > df['Inventory_Level']) &
    (df['Stockout_Flag'] == 0)
).astype(int)

issues_found = df['Potential_Issue'].sum()
print(f"Potential logic issues found: {issues_found:,} rows")

if issues_found > 0:
    print("  (Units_Sold > Inventory_Level but Stockout_Flag=0)")
    print("  → These will be monitored but not corrected automatically")

# ============================================================================
# 9. FINAL DATA QUALITY CHECK
# ============================================================================
print("\n[9] Final Data Quality Check...")

print("\nMissing values per column:")
missing_summary = df.isnull().sum()
missing_summary = missing_summary[missing_summary > 0]
if len(missing_summary) > 0:
    print(missing_summary)
else:
    print("✓ No missing values remaining")

print("\nData types:")
print(df.dtypes)

# ============================================================================
# 10. CHRONOLOGICAL TRAIN-TEST SPLIT (80-20)
# ============================================================================
print("\n[10] Creating Train-Test Split (80%-20% Chronological)...")

# Sort by date
df = df.sort_values('Date').reset_index(drop=True)

# Calculate split point
split_idx = int(len(df['Date'].unique()) * 0.8)
split_date = sorted(df['Date'].unique())[split_idx]

# Split data
train_df = df[df['Date'] < split_date].copy()
test_df = df[df['Date'] >= split_date].copy()

print(f"✓ Split date: {split_date}")
print(f"✓ Training set: {len(train_df):,} rows ({len(train_df)/len(df)*100:.1f}%)")
print(f"  Date range: {train_df['Date'].min()} to {train_df['Date'].max()}")
print(f"✓ Test set: {len(test_df):,} rows ({len(test_df)/len(df)*100:.1f}%)")
print(f"  Date range: {test_df['Date'].min()} to {test_df['Date'].max()}")

# ============================================================================
# 11. SUMMARY STATISTICS
# ============================================================================
print("\n[11] Summary Statistics...")

print("\nDataset Overview:")
print(f"Total rows: {len(df):,}")
print(f"Unique SKUs: {df['SKU_ID'].nunique():,}")
print(f"Unique Warehouses: {df['Warehouse_ID'].nunique():,}")
print(f"Unique Suppliers: {df['Supplier_ID'].nunique():,}")
print(f"Unique Regions: {df['Region'].nunique():,}")

print("\nKey Metrics:")
print(f"Total Units Sold: {df['Units_Sold'].sum():,.0f}")
print(f"Average Daily Sales per SKU: {df.groupby('SKU_ID')['Units_Sold'].mean().mean():.2f}")
print(f"Stockout Rate: {df['Stockout_Flag'].mean()*100:.2f}%")
print(f"Promotion Rate: {df['Promotion_Flag'].mean()*100:.2f}%")

print("\n" + "=" * 80)
print("BLOCK A COMPLETE ✓")
print("=" * 80)
print("\nKey variables created:")
print("  - df: Full preprocessed dataset")
print("  - train_df: Training set (80%)")
print("  - test_df: Test set (20%)")
print("  - split_date: Date separating train/test")
print("\nReady for Block B: Feature Engineering & Baseline")