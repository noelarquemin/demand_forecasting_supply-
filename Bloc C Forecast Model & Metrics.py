# -*- coding: utf-8 -*-
"""Untitled65.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgjgYUdY6fMgFrC3XxZzrkBupiGkGu38
"""

"""
DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN
Block C: Forecast Model & Metrics
"""

# ============================================================================
# IMPORTS (Additional for Block C)
# ============================================================================
from xgboost import XGBRegressor
from sklearn.model_selection import TimeSeriesSplit
import joblib

print("\n" + "=" * 80)
print("BLOCK C: FORECAST MODEL & METRICS")
print("=" * 80)

# ============================================================================
# 1. ADD NAIVE SEASONALITY BASELINE
# ============================================================================
print("\n[1] Adding Naive Seasonality Baseline (j-7)...")

# Add Naive Seasonality baseline to dataframe
df['Baseline_Naive_Season'] = df.groupby('SKU_ID')['Units_Sold'].shift(7)
df['Baseline_Naive_Season'] = df['Baseline_Naive_Season'].fillna(0)

print("‚úì Baseline_Naive_Season created (j-7 lag)")

# Update test_baseline
test_baseline = df[df['Date'] >= split_date].copy()

# Recalculate all baseline metrics
print("\n[1.1] Evaluating All Baselines on Test Set...")

baseline_results = []

# Na√Øve (j-1)
metrics = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_Naive'],
    'Na√Øve (j-1)'
)
if metrics:
    baseline_results.append(metrics)

# MA7
metrics = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_MA7'],
    'Moving Avg (7d)'
)
if metrics:
    baseline_results.append(metrics)

# Naive Seasonality (j-7)
metrics = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_Naive_Season'],
    'Na√Øve Season (j-7)'
)
if metrics:
    baseline_results.append(metrics)

# Dataset Forecast
metrics = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_Dataset'],
    'Dataset Forecast'
)
if metrics:
    baseline_results.append(metrics)

baseline_metrics_df = pd.DataFrame(baseline_results)
print("\n‚úì All Baseline Performance (Test Set):")
print(baseline_metrics_df.to_string(index=False))

# ============================================================================
# 2. PREPARE DATA FOR ML MODEL
# ============================================================================
print("\n" + "=" * 80)
print("[2] PREPARING DATA FOR ML MODEL")
print("=" * 80)

print("\n[2.1] Data Structure Validation...")

# Create clean modeling dataset (remove rows with NaN in features)
df_model = df.copy()

# Check for missing values in features
missing_features = df_model[all_model_features].isnull().sum()
missing_features = missing_features[missing_features > 0]

if len(missing_features) > 0:
    print(f"\n‚ö†Ô∏è Features with missing values:")
    print(missing_features)
    print("\nRemoving rows with missing feature values...")
    df_model = df_model.dropna(subset=all_model_features)
else:
    print("‚úì No missing values in features")

# Check for missing values in target
target_missing = df_model['Units_Sold'].isnull().sum()
if target_missing > 0:
    print(f"\n‚ö†Ô∏è Target has {target_missing} missing values, removing...")
    df_model = df_model.dropna(subset=['Units_Sold'])
else:
    print("‚úì No missing values in target")

print(f"\n‚úì Clean dataset: {len(df_model):,} rows")

# Split into train and test
train_df_model = df_model[df_model['Date'] < split_date].copy()
test_df_model = df_model[df_model['Date'] >= split_date].copy()

print(f"‚úì Training set: {len(train_df_model):,} rows")
print(f"‚úì Test set: {len(test_df_model):,} rows")

# Prepare feature matrices
print("\n[2.2] Preparing Feature Matrices...")

X_train = train_df_model[all_model_features].values
y_train = train_df_model['Units_Sold'].values

X_test = test_df_model[all_model_features].values
y_test = test_df_model['Units_Sold'].values

print(f"\n‚úì X_train shape: {X_train.shape}")
print(f"‚úì y_train shape: {y_train.shape}")
print(f"‚úì X_test shape: {X_test.shape}")
print(f"‚úì y_test shape: {y_test.shape}")

# Data type verification
print(f"\n‚úì X_train dtype: {X_train.dtype}")
print(f"‚úì y_train dtype: {y_train.dtype}")

# Check for inf/nan values
print(f"\n‚úì Inf values in X_train: {np.isinf(X_train).sum()}")
print(f"‚úì NaN values in X_train: {np.isnan(X_train).sum()}")
print(f"‚úì Inf values in y_train: {np.isinf(y_train).sum()}")
print(f"‚úì NaN values in y_train: {np.isnan(y_train).sum()}")

if np.isinf(X_train).sum() > 0 or np.isnan(X_train).sum() > 0:
    print("\n‚ö†Ô∏è WARNING: Invalid values detected in X_train!")
    # Replace inf with large number, nan with 0
    X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e10, neginf=-1e10)
    X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e10, neginf=-1e10)
    print("‚úì Invalid values replaced")

print("\n‚úì Data validation complete - ready for training")

# ============================================================================
# 3. MULTI-HORIZON TARGET PREPARATION
# ============================================================================
print("\n" + "=" * 80)
print("[3] MULTI-HORIZON FORECAST SETUP")
print("=" * 80)

horizons = [1, 7, 14]
print(f"\nForecasting horizons: {horizons} days (j+1, j+7, j+14)")

# Create future target variables for each horizon
print("\n[3.1] Creating Future Targets...")

for h in horizons:
    target_col = f'Units_Sold_j{h}'
    df_model[target_col] = df_model.groupby('SKU_ID')['Units_Sold'].shift(-h)
    print(f"‚úì Created target: {target_col} (Units_Sold shifted by -{h} days)")

# Remove rows where future targets are NaN
print("\n[3.2] Removing Rows with NaN Targets...")

initial_rows = len(df_model)
df_model_clean = df_model.dropna(subset=[f'Units_Sold_j{h}' for h in horizons])
removed_rows = initial_rows - len(df_model_clean)

print(f"‚úì Rows removed: {removed_rows:,} ({removed_rows/initial_rows*100:.2f}%)")
print(f"‚úì Remaining rows: {len(df_model_clean):,}")

# Update train/test splits with clean data
train_df_clean = df_model_clean[df_model_clean['Date'] < split_date].copy()
test_df_clean = df_model_clean[df_model_clean['Date'] >= split_date].copy()

print(f"\n‚úì Clean training set: {len(train_df_clean):,} rows")
print(f"‚úì Clean test set: {len(test_df_clean):,} rows")

# ============================================================================
# 4. XGBOOST MODEL TRAINING (MULTI-HORIZON)
# ============================================================================
print("\n" + "=" * 80)
print("[4] XGBOOST MODEL TRAINING")
print("=" * 80)

# XGBoost hyperparameters
xgb_params = {
    'n_estimators': 200,
    'max_depth': 6,
    'learning_rate': 0.1,
    'min_child_weight': 3,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'gamma': 0.1,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': 0
}

print(f"\nXGBoost Hyperparameters:")
for key, value in xgb_params.items():
    print(f"  {key}: {value}")

# Dictionary to store models and predictions
models = {}
predictions_test = {}
predictions_train = {}

print("\n" + "-" * 80)

# Train separate model for each horizon
for h in horizons:
    print(f"\n[4.{horizons.index(h)+1}] Training XGBoost for horizon j+{h}...")

    target_col = f'Units_Sold_j{h}'

    # Prepare data for this horizon
    X_train_h = train_df_clean[all_model_features].values
    y_train_h = train_df_clean[target_col].values

    X_test_h = test_df_clean[all_model_features].values
    y_test_h = test_df_clean[target_col].values

    print(f"  Training samples: {len(X_train_h):,}")
    print(f"  Test samples: {len(X_test_h):,}")
    print(f"  Features: {X_train_h.shape[1]}")

    # Train model
    print(f"  Training in progress...")
    model = XGBRegressor(**xgb_params)
    model.fit(X_train_h, y_train_h, verbose=False)

    # Predict on train and test sets
    y_pred_train = model.predict(X_train_h)
    y_pred_train = np.maximum(y_pred_train, 0)  # Non-negative constraint

    y_pred_test = model.predict(X_test_h)
    y_pred_test = np.maximum(y_pred_test, 0)  # Non-negative constraint

    # Store results
    models[f'j{h}'] = model
    predictions_train[f'j{h}'] = y_pred_train
    predictions_test[f'j{h}'] = y_pred_test

    # Add predictions to dataframes
    train_df_clean[f'Pred_XGB_j{h}'] = y_pred_train
    test_df_clean[f'Pred_XGB_j{h}'] = y_pred_test

    # Calculate metrics
    metrics_test = calculate_metrics(y_test_h, y_pred_test, f'XGBoost j+{h}')

    print(f"  ‚úì Model trained successfully")
    print(f"  Test Set Performance:")
    print(f"    RMSE: {metrics_test['RMSE']:.2f}")
    print(f"    MAE: {metrics_test['MAE']:.2f}")
    print(f"    MAPE: {metrics_test['MAPE']:.2f}%")
    print(f"    WAPE: {metrics_test['WAPE']:.2f}%")

print("\n" + "-" * 80)
print("‚úì All horizon models trained successfully")

# ============================================================================
# 5. TIME-SERIES CROSS-VALIDATION
# ============================================================================
print("\n" + "=" * 80)
print("[5] TIME-SERIES CROSS-VALIDATION")
print("=" * 80)

print("\n[5.1] Walk-Forward Validation (Expanding Window)...")
print("Using TimeSeriesSplit with 5 folds")

n_splits = 5
tscv = TimeSeriesSplit(n_splits=n_splits)

cv_results = []

for h in horizons:
    print(f"\n  Validating horizon j+{h}...")

    target_col = f'Units_Sold_j{h}'
    X = train_df_clean[all_model_features].values
    y = train_df_clean[target_col].values

    fold_scores = {'RMSE': [], 'MAE': [], 'MAPE': [], 'WAPE': []}

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_fold_train, X_fold_val = X[train_idx], X[val_idx]
        y_fold_train, y_fold_val = y[train_idx], y[val_idx]

        # Train model on fold
        model_cv = XGBRegressor(**xgb_params)
        model_cv.fit(X_fold_train, y_fold_train, verbose=False)

        # Predict on validation fold
        y_fold_pred = model_cv.predict(X_fold_val)
        y_fold_pred = np.maximum(y_fold_pred, 0)

        # Calculate metrics
        metrics = calculate_metrics(y_fold_val, y_fold_pred, f'Fold {fold}')

        if metrics:
            fold_scores['RMSE'].append(metrics['RMSE'])
            fold_scores['MAE'].append(metrics['MAE'])
            fold_scores['MAPE'].append(metrics['MAPE'])
            fold_scores['WAPE'].append(metrics['WAPE'])

    # Calculate average and std across folds
    if fold_scores['RMSE']:
        avg_metrics = {
            'Horizon': f'j+{h}',
            'RMSE_mean': np.mean(fold_scores['RMSE']),
            'RMSE_std': np.std(fold_scores['RMSE']),
            'MAE_mean': np.mean(fold_scores['MAE']),
            'MAPE_mean': np.mean(fold_scores['MAPE']),
            'WAPE_mean': np.mean(fold_scores['WAPE']),
        }
        cv_results.append(avg_metrics)

        print(f"    Cross-Validation Results:")
        print(f"      RMSE: {avg_metrics['RMSE_mean']:.2f} ¬± {avg_metrics['RMSE_std']:.2f}")
        print(f"      MAE:  {avg_metrics['MAE_mean']:.2f}")
        print(f"      WAPE: {avg_metrics['WAPE_mean']:.2f}%")

cv_results_df = pd.DataFrame(cv_results)

print("\n‚úì Cross-Validation Summary:")
print(cv_results_df.to_string(index=False))

# ============================================================================
# 6. COMPREHENSIVE TECHNICAL METRICS
# ============================================================================
print("\n" + "=" * 80)
print("[6] TECHNICAL METRICS COMPARISON")
print("=" * 80)

def calculate_smape(y_true, y_pred):
    """Calculate Symmetric Mean Absolute Percentage Error"""
    mask = ~(pd.isna(y_true) | pd.isna(y_pred))
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]

    if len(y_true_clean) == 0:
        return np.nan

    denominator = (np.abs(y_true_clean) + np.abs(y_pred_clean)) / 2
    smape = np.mean(np.abs(y_true_clean - y_pred_clean) / (denominator + 1e-10)) * 100
    return smape

# Compare all models for j+1 horizon
print("\n[6.1] Model Comparison for j+1 Horizon...")

comparison_results = []

# Get test data
test_eval = test_df_clean.copy()
target_j1 = test_eval['Units_Sold_j1']

# Baseline: Na√Øve (j-1)
naive_pred_j1 = test_eval['Baseline_Naive']
if not naive_pred_j1.isna().all():
    metrics = calculate_metrics(target_j1, naive_pred_j1, 'Na√Øve (j-1)')
    if metrics:
        metrics['SMAPE'] = calculate_smape(target_j1.values, naive_pred_j1.values)
        comparison_results.append(metrics)

# Baseline: MA7
ma7_pred_j1 = test_eval['Baseline_MA7']
if not ma7_pred_j1.isna().all():
    metrics = calculate_metrics(target_j1, ma7_pred_j1, 'MA (7d)')
    if metrics:
        metrics['SMAPE'] = calculate_smape(target_j1.values, ma7_pred_j1.values)
        comparison_results.append(metrics)

# Baseline: Na√Øve Seasonality (j-7)
season_pred_j1 = test_eval['Baseline_Naive_Season']
if not season_pred_j1.isna().all():
    metrics = calculate_metrics(target_j1, season_pred_j1, 'Na√Øve Season (j-7)')
    if metrics:
        metrics['SMAPE'] = calculate_smape(target_j1.values, season_pred_j1.values)
        comparison_results.append(metrics)

# Baseline: Dataset
dataset_pred_j1 = test_eval['Baseline_Dataset']
if not dataset_pred_j1.isna().all():
    metrics = calculate_metrics(target_j1, dataset_pred_j1, 'Dataset Forecast')
    if metrics:
        metrics['SMAPE'] = calculate_smape(target_j1.values, dataset_pred_j1.values)
        comparison_results.append(metrics)

# XGBoost
xgb_pred_j1 = test_eval['Pred_XGB_j1']
if not xgb_pred_j1.isna().all():
    metrics = calculate_metrics(target_j1, xgb_pred_j1, 'XGBoost')
    if metrics:
        metrics['SMAPE'] = calculate_smape(target_j1.values, xgb_pred_j1.values)
        comparison_results.append(metrics)

comparison_j1_df = pd.DataFrame(comparison_results)

print("\nj+1 Horizon - All Models Comparison:")
print(comparison_j1_df[['Model', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'WAPE']].to_string(index=False))

# Calculate improvement
if len(comparison_j1_df) > 1:
    baseline_rmse = comparison_j1_df[comparison_j1_df['Model'] != 'XGBoost']['RMSE'].min()
    xgb_rmse = comparison_j1_df[comparison_j1_df['Model'] == 'XGBoost']['RMSE'].values[0]
    improvement_pct = ((baseline_rmse - xgb_rmse) / baseline_rmse) * 100

    print(f"\nüéØ XGBoost Performance:")
    print(f"  Best Baseline RMSE: {baseline_rmse:.2f}")
    print(f"  XGBoost RMSE: {xgb_rmse:.2f}")
    print(f"  Improvement: {improvement_pct:+.2f}%")

# ============================================================================
# 7. BUSINESS METRICS - SUPPLY CHAIN KPIs
# ============================================================================
print("\n" + "=" * 80)
print("[7] BUSINESS METRICS - SUPPLY CHAIN KPIs")
print("=" * 80)

def calculate_supply_chain_kpis(df_eval, forecast_col, scenario_name):
    """
    Calculate comprehensive supply chain KPIs

    Parameters:
    - df_eval: DataFrame with actual demand and inventory data
    - forecast_col: Column name with demand forecast (not used in calculation but for reference)
    - scenario_name: Name of the scenario for reporting

    Returns:
    - kpi_results: Dictionary with aggregated KPIs
    - sku_kpis: DataFrame with SKU-level KPIs
    """
    df_kpi = df_eval.copy()

    # --- a) Fill Rate ---
    df_kpi['served_demand'] = np.minimum(df_kpi['Units_Sold'], df_kpi['Inventory_Level'])
    df_kpi['total_demand'] = df_kpi['Units_Sold']

    # Overall fill rate
    total_served = df_kpi['served_demand'].sum()
    total_demand = df_kpi['total_demand'].sum()
    fill_rate = (total_served / (total_demand + 1e-10)) * 100

    # --- b) Stockout Rate ---
    df_kpi['days_with_stockout'] = (
        (df_kpi['Inventory_Level'] < df_kpi['Units_Sold']) |
        (df_kpi['Stockout_Flag'] == 1)
    ).astype(int)

    stockout_rate = (df_kpi['days_with_stockout'].sum() / len(df_kpi)) * 100

    # --- c) Holding Cost ---
    annual_storage_rate = 0.20  # 20% annual storage cost
    df_kpi['unit_storage_cost'] = df_kpi['Unit_Cost'] * (annual_storage_rate / 365)
    df_kpi['holding_cost_per_day'] = df_kpi['Inventory_Level'] * df_kpi['unit_storage_cost']
    total_holding_cost = df_kpi['holding_cost_per_day'].sum()

    # --- d) Stockout Cost ---
    penalty_factor = 1.5  # 150% of margin
    df_kpi['unit_margin'] = df_kpi['Unit_Price'] - df_kpi['Unit_Cost']
    df_kpi['unit_stockout_cost'] = df_kpi['unit_margin'] * penalty_factor
    df_kpi['unmet_demand'] = np.maximum(0, df_kpi['Units_Sold'] - df_kpi['Inventory_Level'])
    df_kpi['stockout_cost_per_day'] = df_kpi['unmet_demand'] * df_kpi['unit_stockout_cost']
    total_stockout_cost = df_kpi['stockout_cost_per_day'].sum()

    # --- e) Total Cost ---
    total_cost = total_holding_cost + total_stockout_cost

    # --- SKU-Level Aggregation ---
    sku_kpis = df_kpi.groupby('SKU_ID').agg({
        'served_demand': 'sum',
        'total_demand': 'sum',
        'days_with_stockout': 'sum',
        'holding_cost_per_day': 'sum',
        'stockout_cost_per_day': 'sum',
        'Units_Sold': 'sum'  # For demand weighting
    }).reset_index()

    # Calculate SKU-level rates
    sku_kpis['days_count'] = df_kpi.groupby('SKU_ID').size().values
    sku_kpis['fill_rate'] = (sku_kpis['served_demand'] / (sku_kpis['total_demand'] + 1e-10)) * 100
    sku_kpis['stockout_rate'] = (sku_kpis['days_with_stockout'] / sku_kpis['days_count']) * 100
    sku_kpis['total_cost'] = sku_kpis['holding_cost_per_day'] + sku_kpis['stockout_cost_per_day']

    # Weighted averages (by demand/Units_Sold)
    total_demand_weight = sku_kpis['Units_Sold'].sum()
    weighted_fill_rate = (sku_kpis['fill_rate'] * sku_kpis['Units_Sold']).sum() / (total_demand_weight + 1e-10)
    weighted_stockout_rate = (sku_kpis['stockout_rate'] * sku_kpis['Units_Sold']).sum() / (total_demand_weight + 1e-10)

    # Aggregate results
    kpi_results = {
        'Scenario': scenario_name,
        'Fill_Rate': fill_rate,
        'Weighted_Fill_Rate': weighted_fill_rate,
        'Stockout_Rate': stockout_rate,
        'Weighted_Stockout_Rate': weighted_stockout_rate,
        'Holding_Cost': total_holding_cost,
        'Stockout_Cost': total_stockout_cost,
        'Total_Cost': total_cost,
        'Total_Unmet_Demand': df_kpi['unmet_demand'].sum()
    }

    return kpi_results, sku_kpis

# Calculate KPIs for Baseline Scenario
print("\n[7.1] Calculating KPIs for Baseline Scenario...")
print("(Using current inventory levels and actual demand)")

baseline_kpis, baseline_sku_kpis = calculate_supply_chain_kpis(
    test_df_clean,
    forecast_col='Units_Sold',  # Actual demand as baseline
    scenario_name='Baseline (Current State)'
)

print(f"\nüìä Baseline Scenario Results:")
print(f"  Fill Rate: {baseline_kpis['Fill_Rate']:.2f}%")
print(f"  Weighted Fill Rate: {baseline_kpis['Weighted_Fill_Rate']:.2f}%")
print(f"  Stockout Rate: {baseline_kpis['Stockout_Rate']:.2f}%")
print(f"  Weighted Stockout Rate: {baseline_kpis['Weighted_Stockout_Rate']:.2f}%")
print(f"  Holding Cost: ‚Ç¨{baseline_kpis['Holding_Cost']:,.2f}")
print(f"  Stockout Cost: ‚Ç¨{baseline_kpis['Stockout_Cost']:,.2f}")
print(f"  Total Cost: ‚Ç¨{baseline_kpis['Total_Cost']:,.2f}")
print(f"  Total Unmet Demand: {baseline_kpis['Total_Unmet_Demand']:,.0f} units")

# Calculate KPIs for XGBoost Scenario
print("\n[7.2] Simulating KPIs with XGBoost Forecast...")
print("(Note: This is a simplified simulation using j+1 forecast)")

# For simplicity, we use the actual inventory in test data
# In a full simulation (Block D), we would adjust inventory based on forecast
xgb_kpis, xgb_sku_kpis = calculate_supply_chain_kpis(
    test_df_clean,
    forecast_col='Pred_XGB_j1',
    scenario_name='XGBoost Forecast'
)

print(f"\nüìä XGBoost Scenario Results:")
print(f"  Fill Rate: {xgb_kpis['Fill_Rate']:.2f}%")
print(f"  Weighted Fill Rate: {xgb_kpis['Weighted_Fill_Rate']:.2f}%")
print(f"  Stockout Rate: {xgb_kpis['Stockout_Rate']:.2f}%")
print(f"  Weighted Stockout Rate: {xgb_kpis['Weighted_Stockout_Rate']:.2f}%")
print(f"  Holding Cost: ‚Ç¨{xgb_kpis['Holding_Cost']:,.2f}")
print(f"  Stockout Cost: ‚Ç¨{xgb_kpis['Stockout_Cost']:,.2f}")
print(f"  Total Cost: ‚Ç¨{xgb_kpis['Total_Cost']:,.2f}")
print(f"  Total Unmet Demand: {xgb_kpis['Total_Unmet_Demand']:,.0f} units")

# ============================================================================
# 8. BUSINESS IMPACT SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("[8] BUSINESS IMPACT SUMMARY")
print("=" * 80)

# Calculate improvements
cost_savings = baseline_kpis['Total_Cost'] - xgb_kpis['Total_Cost']
cost_savings_pct = (cost_savings / baseline_kpis['Total_Cost']) * 100

holding_cost_change = baseline_kpis['Holding_Cost'] - xgb_kpis['Holding_Cost']
stockout_cost_change = baseline_kpis['Stockout_Cost'] - xgb_kpis['Stockout_Cost']

fill_rate_change = xgb_kpis['Weighted_Fill_Rate'] - baseline_kpis['Weighted_Fill_Rate']
stockout_rate_change = baseline_kpis['Weighted_Stockout_Rate'] - xgb_kpis['Weighted_Stockout_Rate']

unmet_demand_reduction = baseline_kpis['Total_Unmet_Demand'] - xgb_kpis['Total_Unmet_Demand']

print("\nüéØ IMPACT OF XGBOOST FORECASTING vs BASELINE:")
print("\nüí∞ Cost Impact:")
print(f"  Total Cost Savings: ‚Ç¨{cost_savings:,.2f} ({cost_savings_pct:+.2f}%)")
print(f"    Holding Cost Change: ‚Ç¨{holding_cost_change:,.2f}")
print(f"    Stockout Cost Change: ‚Ç¨{stockout_cost_change:,.2f}")

print("\nüìà Service Level Impact:")
print(f"  Fill Rate Change: {fill_rate_change:+.2f} percentage points")
print(f"  Stockout Rate Reduction: {stockout_rate_change:+.2f} percentage points")

print("\nüì¶ Operational Impact:")
print(f"  Unmet Demand Reduction: {unmet_demand_reduction:,.0f} units")

# Create comparison dataframe
impact_comparison = pd.DataFrame([baseline_kpis, xgb_kpis])
impact_comparison = impact_comparison[[
    'Scenario', 'Fill_Rate', 'Weighted_Fill_Rate', 'Stockout_Rate',
    'Weighted_Stockout_Rate', 'Holding_Cost', 'Stockout_Cost', 'Total_Cost'
]]

print("\nüìä Side-by-Side Comparison:")
print(impact_comparison.to_string(index=False))

# ============================================================================
# 9. FEATURE IMPORTANCE ANALYSIS
# ============================================================================
print("\n" + "=" * 80)
print("[9] FEATURE IMPORTANCE ANALYSIS")
print("=" * 80)

# Extract feature importance from j+1 model
model_j1 = models['j1']
feature_importance = pd.DataFrame({
    'Feature': all_model_features,
    'Importance': model_j1.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nüîç Top 20 Most Important Features (j+1 model):")
print(feature_importance.head(20).to_string(index=False))

# Group importance by feature type
feature_groups = {
    'Time': ['week_day', 'month', 'week_number', 'day_trend'],
    'Lag': ['lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14'],
    'Moving_Avg': ['ma_7j', 'ma_28j'],
    'Volatility': ['volatility_j7', 'volatility_j14'],
    'Inventory': ['days_of_stock', 'Inventory_Level', 'Reorder_Point'],
    'Promotion': ['promotion_of_the_day', 'j1_promotion', 'promotion_density'],
    'Lead_Time': ['average_lead_time', 'leadtime_variability', 'Supplier_Lead_Time_Days'],
    'Classification': ['ABC_Class_encoded', 'XYZ_Class_encoded', 'CV']
}

print("\nüìä Feature Importance by Group:")
for group_name, features in feature_groups.items():
    group_features = [f for f in features if f in feature_importance['Feature'].values]
    if group_features:
        group_importance = feature_importance[feature_importance['Feature'].isin(group_features)]['Importance'].sum()
        print(f"  {group_name}: {group_importance:.4f}")

# ============================================================================
# 10. SUMMARY AND OUTPUTS
# ============================================================================
print("\n" + "=" * 80)
print("BLOCK C COMPLETE ‚úì")
print("=" * 80)

print("\nüì¶ Key Outputs Created:")
print("  ‚úì models: Dictionary of XGBoost models {j1, j7, j14}")
print("  ‚úì predictions_test: Test predictions for each horizon")
print("  ‚úì test_df_clean: Test data with predictions and targets")
print("  ‚úì comparison_j1_df: Technical metrics comparison (all models)")
print("  ‚úì cv_results_df: Cross-validation results")
print("  ‚úì baseline_kpis: Business KPIs for baseline scenario")
print("  ‚úì xgb_kpis: Business KPIs for XGBoost scenario")
print("  ‚úì baseline_sku_kpis: SKU-level KPIs (baseline)")
print("  ‚úì xgb_sku_kpis: SKU-level KPIs (XGBoost)")
print("  ‚úì feature_importance: Feature ranking")
print("  ‚úì impact_comparison: Business impact summary")

print("\nüéØ Final Performance Summary:")
if 'improvement_pct' in locals():
    print(f"  Technical: {improvement_pct:+.2f}% RMSE improvement over best baseline")
print(f"  Business: ‚Ç¨{cost_savings:,.2f} total cost savings ({cost_savings_pct:+.2f}%)")
print(f"  Service: {fill_rate_change:+.2f}pp fill rate improvement")
print(f"  Operations: {unmet_demand_reduction:,.0f} units less unmet demand")

print("\n‚úÖ Ready for Block D: Inventory Simulation & Monte Carlo")