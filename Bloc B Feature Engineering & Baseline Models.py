# -*- coding: utf-8 -*-
"""Untitled65.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgjgYUdY6fMgFrC3XxZzrkBupiGkGu38
"""

"""
DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN
Block B: Feature Engineering & Baseline Models (REVISED)
"""

# ============================================================================
# IMPORTS (Additional for Block B)
# ============================================================================
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import LabelEncoder
import scipy.stats as stats

print("\n" + "=" * 80)
print("BLOCK B: FEATURE ENGINEERING & BASELINE MODELS")
print("=" * 80)

# ============================================================================
# 1. BASELINE MODELS - CREATE FORECASTS
# ============================================================================
print("\n[1] Building Baseline Forecasts...")

# Sort data by SKU and Date for proper lag calculations
df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)

# ---- Baseline 1: Naïve Forecast (lag_sell_j-1) ----
df['Baseline_Naive'] = df.groupby('SKU_ID')['Units_Sold'].shift(1)
print("✓ Baseline_Naive created (j-1 lag)")

# ---- Baseline 2: Moving Average (7 days) ----
df['Baseline_MA7'] = df.groupby('SKU_ID')['Units_Sold'].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean().shift(1)
)
print("✓ Baseline_MA7 created (7-day MA)")

# ---- Baseline 3: Dataset Demand_Forecast ----
df['Baseline_Dataset'] = df['Demand_Forecast']
print("✓ Baseline_Dataset created (from Demand_Forecast column)")

# Fill NaN values in baselines with 0
df['Baseline_Naive'] = df['Baseline_Naive'].fillna(0)
df['Baseline_MA7'] = df['Baseline_MA7'].fillna(0)

print("\n✓ Three baseline forecasts created and ready for evaluation")

# ============================================================================
# 2. BASELINE PERFORMANCE METRICS (ON TEST SET)
# ============================================================================
print("\n[2] Evaluating Baseline Performance on Test Set...")

def calculate_metrics(y_true, y_pred, model_name):
    """Calculate RMSE, MAE, MAPE, and WAPE"""
    # Remove NaN values
    mask = ~(pd.isna(y_true) | pd.isna(y_pred))
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]

    if len(y_true_clean) == 0:
        return None

    # RMSE
    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

    # MAE
    mae = mean_absolute_error(y_true_clean, y_pred_clean)

    # MAPE (avoid division by zero)
    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / (y_true_clean + 1e-10))) * 100

    # WAPE (Weighted Absolute Percentage Error)
    wape = np.sum(np.abs(y_true_clean - y_pred_clean)) / (np.sum(y_true_clean) + 1e-10) * 100

    return {
        'Model': model_name,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape,
        'WAPE': wape
    }

# Create test baseline dataset
test_baseline = df[df['Date'] >= split_date].copy()

print(f"Test baseline dataset: {len(test_baseline):,} rows")

# Calculate metrics for each baseline
baseline_results = []

# Naïve
metrics_naive = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_Naive'],
    'Naïve (j-1)'
)
if metrics_naive:
    baseline_results.append(metrics_naive)

# MA7
metrics_ma7 = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_MA7'],
    'Moving Avg (7d)'
)
if metrics_ma7:
    baseline_results.append(metrics_ma7)

# Dataset
metrics_dataset = calculate_metrics(
    test_baseline['Units_Sold'],
    test_baseline['Baseline_Dataset'],
    'Dataset Forecast'
)
if metrics_dataset:
    baseline_results.append(metrics_dataset)

# Create baseline metrics dataframe
baseline_metrics_df = pd.DataFrame(baseline_results)

print("\n✓ Baseline Performance (Test Set):")
print(baseline_metrics_df.to_string(index=False))

# ============================================================================
# 3. SUPPLY CHAIN ANALYSIS
# ============================================================================
print("\n" + "=" * 80)
print("[3] SUPPLY CHAIN ANALYSIS")
print("=" * 80)

# ---- 3.1 Descriptive Statistics ----
print("\n[3.1] Descriptive Statistics (Mean & Std Dev)...")

numeric_cols = ['Units_Sold', 'Inventory_Level', 'Supplier_Lead_Time_Days',
                'Reorder_Point', 'Order_Quantity', 'Unit_Cost', 'Unit_Price']

stats_summary = df[numeric_cols].agg(['mean', 'std', 'min', 'max'])
print("\nOverall Statistics:")
print(stats_summary.round(2))

# ---- 3.2 Coefficient of Variation by SKU ----
print("\n[3.2] Coefficient of Variation (CV) by SKU...")

sku_stats = df.groupby('SKU_ID')['Units_Sold'].agg(['mean', 'std']).reset_index()
sku_stats['CV'] = sku_stats['std'] / (sku_stats['mean'] + 1e-10)
sku_stats = sku_stats.sort_values('CV', ascending=False)

print(f"\nCV Statistics across all SKUs:")
print(f"  Mean CV: {sku_stats['CV'].mean():.3f}")
print(f"  Median CV: {sku_stats['CV'].median():.3f}")
print(f"  Min CV: {sku_stats['CV'].min():.3f}")
print(f"  Max CV: {sku_stats['CV'].max():.3f}")

print(f"\nTop 5 SKUs with highest variability:")
print(sku_stats.head()[['SKU_ID', 'mean', 'std', 'CV']].to_string(index=False))

# ---- 3.3 Seasonality Analysis ----
print("\n[3.3] Identifying Seasonality...")

# Weekly seasonality (day of week)
df['DayOfWeek'] = pd.to_datetime(df['Date']).dt.dayofweek
weekly_pattern = df.groupby('DayOfWeek')['Units_Sold'].mean()

print("\nAverage Units Sold by Day of Week:")
days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
for i, day in enumerate(days):
    if i in weekly_pattern.index:
        print(f"  {day}: {weekly_pattern.iloc[i]:.2f}")

# Monthly seasonality
df['Month'] = pd.to_datetime(df['Date']).dt.month
monthly_pattern = df.groupby('Month')['Units_Sold'].mean()

print("\nAverage Units Sold by Month:")
for month in range(1, 13):
    if month in monthly_pattern.index:
        print(f"  Month {month:2d}: {monthly_pattern.loc[month]:.2f}")

# ---- 3.4 Promotion Impact ----
print("\n[3.4] Impact of Promotion_Flag on Sales...")

promo_impact = df.groupby('Promotion_Flag')['Units_Sold'].agg(['mean', 'std', 'count'])
print("\nUnits Sold by Promotion Status:")
print(promo_impact)

if 1 in promo_impact.index and 0 in promo_impact.index:
    promo_uplift = (promo_impact.loc[1, 'mean'] / promo_impact.loc[0, 'mean'] - 1) * 100
    print(f"\n✓ Promotion Uplift Effect: {promo_uplift:.2f}%")
    print(f"  (Sales are {promo_uplift:.2f}% higher during promotions)")

# ---- 3.5 Lead Time vs Stockout Correlation ----
print("\n[3.5] Supplier Lead Time Impact on Stockouts...")

correlation = df[['Supplier_Lead_Time_Days', 'Stockout_Flag']].corr().iloc[0, 1]
print(f"\nCorrelation (Lead Time vs Stockout): {correlation:.3f}")

if correlation > 0.1:
    print("  → Longer lead times are associated with more stockouts")
elif correlation < -0.1:
    print("  → Longer lead times are associated with fewer stockouts")
else:
    print("  → Weak correlation between lead time and stockouts")

# Stockout rate by lead time
leadtime_stockout = df.groupby('Supplier_Lead_Time_Days')['Stockout_Flag'].agg(['mean', 'count'])
leadtime_stockout.columns = ['Stockout_Rate', 'Count']
leadtime_stockout = leadtime_stockout[leadtime_stockout['Count'] > 100].sort_values('Stockout_Rate', ascending=False)

print("\nTop 5 Lead Times with Highest Stockout Rates:")
print(leadtime_stockout.head())

# ============================================================================
# 4. ABC/XYZ CLASSIFICATION
# ============================================================================
print("\n" + "=" * 80)
print("[4] ABC/XYZ SEGMENTATION")
print("=" * 80)

# ---- 4.1 ABC Classification (Revenue-based) ----
print("\n[4.1] ABC Classification (by Revenue)...")

# Calculate total revenue per SKU
sku_revenue = df.groupby('SKU_ID').agg({
    'Units_Sold': 'sum',
    'Unit_Price': 'mean'
}).reset_index()
sku_revenue['Revenue'] = sku_revenue['Units_Sold'] * sku_revenue['Unit_Price']

# Sort by revenue descending and calculate cumulative percentage
sku_revenue = sku_revenue.sort_values('Revenue', ascending=False).reset_index(drop=True)
sku_revenue['Cumulative_Revenue'] = sku_revenue['Revenue'].cumsum()
total_revenue = sku_revenue['Revenue'].sum()
sku_revenue['Cumulative_Pct'] = (sku_revenue['Cumulative_Revenue'] / total_revenue) * 100

# Classify ABC based on cumulative revenue
def classify_abc(cum_pct):
    if cum_pct <= 80:
        return 'A'
    elif cum_pct <= 95:
        return 'B'
    else:
        return 'C'

sku_revenue['ABC_Class'] = sku_revenue['Cumulative_Pct'].apply(classify_abc)

print("\nABC Classification Distribution:")
abc_summary = sku_revenue.groupby('ABC_Class').agg({
    'SKU_ID': 'count',
    'Revenue': 'sum'
}).reset_index()
abc_summary.columns = ['ABC_Class', 'SKU_Count', 'Total_Revenue']
abc_summary['SKU_Pct'] = (abc_summary['SKU_Count'] / len(sku_revenue)) * 100
abc_summary['Revenue_Pct'] = (abc_summary['Total_Revenue'] / total_revenue) * 100

for _, row in abc_summary.iterrows():
    print(f"  Class {row['ABC_Class']}: {row['SKU_Count']:4d} SKUs ({row['SKU_Pct']:5.1f}%) → {row['Revenue_Pct']:5.1f}% of revenue")

# ---- 4.2 XYZ Classification (Demand Variability) ----
print("\n[4.2] XYZ Classification (by Coefficient of Variation)...")

# Merge CV data
sku_revenue = sku_revenue.merge(sku_stats[['SKU_ID', 'CV']], on='SKU_ID', how='left')

# Classify XYZ based on CV
def classify_xyz(cv):
    if cv <= 0.5:
        return 'X'
    elif cv <= 1.0:
        return 'Y'
    else:
        return 'Z'

sku_revenue['XYZ_Class'] = sku_revenue['CV'].apply(classify_xyz)

print("\nXYZ Classification Distribution:")
xyz_summary = sku_revenue.groupby('XYZ_Class').agg({
    'SKU_ID': 'count'
}).reset_index()
xyz_summary.columns = ['XYZ_Class', 'SKU_Count']
xyz_summary['SKU_Pct'] = (xyz_summary['SKU_Count'] / len(sku_revenue)) * 100

for _, row in xyz_summary.iterrows():
    print(f"  Class {row['XYZ_Class']}: {row['SKU_Count']:4d} SKUs ({row['SKU_Pct']:5.1f}%)")

# ---- 4.3 Combined ABC/XYZ ----
sku_revenue['ABC_XYZ_Class'] = sku_revenue['ABC_Class'] + sku_revenue['XYZ_Class']

print("\n[4.3] Combined ABC/XYZ Matrix:")
abcxyz_matrix = pd.crosstab(sku_revenue['ABC_Class'], sku_revenue['XYZ_Class'], margins=True)
print(abcxyz_matrix)

print("\n✓ ABC/XYZ classification complete")
print("  Class interpretation:")
print("    - AX: High value, low variability (predictable best-sellers)")
print("    - CZ: Low value, high variability (difficult to forecast, low priority)")

# Merge classification back to main dataframe
df = df.merge(
    sku_revenue[['SKU_ID', 'ABC_Class', 'XYZ_Class', 'ABC_XYZ_Class', 'CV']],
    on='SKU_ID',
    how='left'
)

print("\n✓ ABC/XYZ classes added to main dataframe")

# ============================================================================
# 5. FEATURE ENGINEERING
# ============================================================================
print("\n" + "=" * 80)
print("[5] FEATURE ENGINEERING")
print("=" * 80)

# Ensure sorted by SKU and Date
df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)

# ---- 5.1 Time Features ----
print("\n[5.1] Creating Time Features from Date...")

df['Date_dt'] = pd.to_datetime(df['Date'])
df['week_day'] = df['Date_dt'].dt.dayofweek
df['month'] = df['Date_dt'].dt.month
df['week_number'] = df['Date_dt'].dt.isocalendar().week.astype(int)
df['day_trend'] = (df['Date_dt'] - df['Date_dt'].min()).dt.days

print("✓ Time features: week_day, month, week_number, day_trend")

# ---- 5.2 Sales Lag Features ----
print("\n[5.2] Creating Sales Lag Features...")

df['lag_sell_j1'] = df.groupby('SKU_ID')['Units_Sold'].shift(1)
df['lag_sell_j7'] = df.groupby('SKU_ID')['Units_Sold'].shift(7)
df['lag_sell_j14'] = df.groupby('SKU_ID')['Units_Sold'].shift(14)

print("✓ Lag features: lag_sell_j1, lag_sell_j7, lag_sell_j14")

# ---- 5.3 Moving Average Features ----
print("\n[5.3] Creating Moving Average Features...")

df['ma_7j'] = df.groupby('SKU_ID')['Units_Sold'].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean().shift(1)
)
df['ma_28j'] = df.groupby('SKU_ID')['Units_Sold'].transform(
    lambda x: x.rolling(window=28, min_periods=1).mean().shift(1)
)

print("✓ Moving averages: ma_7j, ma_28j")

# ---- 5.4 Volatility Features (Standard Deviation) ----
print("\n[5.4] Creating Volatility Features...")

df['volatility_j7'] = df.groupby('SKU_ID')['Units_Sold'].transform(
    lambda x: x.rolling(window=7, min_periods=1).std().shift(1)
)
df['volatility_j14'] = df.groupby('SKU_ID')['Units_Sold'].transform(
    lambda x: x.rolling(window=14, min_periods=1).std().shift(1)
)

print("✓ Volatility features: volatility_j7, volatility_j14")

# ---- 5.5 Inventory Features ----
print("\n[5.5] Creating Inventory Features...")

# Days of stock = Inventory / Average daily demand
df['days_of_stock'] = df['Inventory_Level'] / (df['ma_7j'] + 1e-10)
df['days_of_stock'] = df['days_of_stock'].clip(upper=365)  # Cap at 1 year

print("✓ Inventory feature: days_of_stock")

# ---- 5.6 Promotion Features ----
print("\n[5.6] Creating Promotion Features...")

df['promotion_of_the_day'] = df['Promotion_Flag'].astype(int)
df['j1_promotion'] = df.groupby('SKU_ID')['Promotion_Flag'].shift(1).fillna(0).astype(int)

# Promotion density over last 30 days
df['promotion_density'] = df.groupby('SKU_ID')['Promotion_Flag'].transform(
    lambda x: x.rolling(window=30, min_periods=1).mean().shift(1)
)

print("✓ Promotion features: promotion_of_the_day, j1_promotion, promotion_density")

# ---- 5.7 Lead Time Features ----
print("\n[5.7] Creating Lead Time & Supplier Features...")

# Average lead time per supplier
df['average_lead_time'] = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].transform('mean')

# Lead time variability per supplier
df['leadtime_variability'] = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].transform('std')
df['leadtime_variability'] = df['leadtime_variability'].fillna(0)

print("✓ Lead time features: average_lead_time, leadtime_variability")

# ============================================================================
# 6. CATEGORICAL ENCODING
# ============================================================================
print("\n" + "=" * 80)
print("[6] CATEGORICAL ENCODING")
print("=" * 80)

# ---- 6.1 Label Encoding for High Cardinality Features ----
print("\n[6.1] Label Encoding for SKU_ID and Supplier_ID...")

# SKU_ID
le_sku = LabelEncoder()
df['SKU_ID_encoded'] = le_sku.fit_transform(df['SKU_ID'].astype(str))
print(f"✓ SKU_ID encoded: {df['SKU_ID'].nunique()} unique values → 0 to {df['SKU_ID_encoded'].max()}")

# Supplier_ID
le_supplier = LabelEncoder()
df['Supplier_ID_encoded'] = le_supplier.fit_transform(df['Supplier_ID'].astype(str))
print(f"✓ Supplier_ID encoded: {df['Supplier_ID'].nunique()} unique values → 0 to {df['Supplier_ID_encoded'].max()}")

# ---- 6.2 Encoding for Warehouse_ID and Region ----
print("\n[6.2] Encoding Warehouse_ID and Region...")

# Check cardinality
warehouse_card = df['Warehouse_ID'].nunique()
region_card = df['Region'].nunique()

print(f"Warehouse_ID cardinality: {warehouse_card}")
print(f"Region cardinality: {region_card}")

# Warehouse_ID
if warehouse_card < 20:
    # One-hot encoding
    warehouse_dummies = pd.get_dummies(df['Warehouse_ID'], prefix='Warehouse', drop_first=True, dtype=int)
    df = pd.concat([df, warehouse_dummies], axis=1)
    print(f"✓ Warehouse_ID one-hot encoded: {warehouse_dummies.shape[1]} dummy features")
else:
    # Label encoding
    le_warehouse = LabelEncoder()
    df['Warehouse_ID_encoded'] = le_warehouse.fit_transform(df['Warehouse_ID'].astype(str))
    print(f"✓ Warehouse_ID label encoded")

# Region
if region_card < 20:
    # One-hot encoding
    region_dummies = pd.get_dummies(df['Region'], prefix='Region', drop_first=True, dtype=int)
    df = pd.concat([df, region_dummies], axis=1)
    print(f"✓ Region one-hot encoded: {region_dummies.shape[1]} dummy features")
else:
    # Label encoding
    le_region = LabelEncoder()
    df['Region_encoded'] = le_region.fit_transform(df['Region'].astype(str))
    print(f"✓ Region label encoded")

# ---- 6.3 Encode ABC/XYZ Classes ----
print("\n[6.3] Encoding ABC/XYZ Classes...")

abc_mapping = {'A': 3, 'B': 2, 'C': 1}
xyz_mapping = {'X': 3, 'Y': 2, 'Z': 1}

df['ABC_Class_encoded'] = df['ABC_Class'].map(abc_mapping).fillna(0).astype(int)
df['XYZ_Class_encoded'] = df['XYZ_Class'].map(xyz_mapping).fillna(0).astype(int)

print("✓ ABC/XYZ classes encoded (A=3, B=2, C=1; X=3, Y=2, Z=1)")

# ============================================================================
# 7. FILL MISSING VALUES IN ENGINEERED FEATURES
# ============================================================================
print("\n[7] Handling Missing Values in Engineered Features...")

# List of features that may have NaN due to lag/rolling operations
feature_cols = [
    'lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14',
    'ma_7j', 'ma_28j',
    'volatility_j7', 'volatility_j14',
    'days_of_stock',
    'j1_promotion', 'promotion_density',
    'average_lead_time', 'leadtime_variability'
]

for col in feature_cols:
    if col in df.columns:
        missing_count = df[col].isna().sum()
        if missing_count > 0:
            df[col] = df[col].fillna(0)
            print(f"  {col}: {missing_count:,} NaN values filled with 0")

print("\n✓ All engineered features have no missing values")

# ============================================================================
# 8. VERIFY ALL FEATURES ARE NUMERIC
# ============================================================================
print("\n[8] Verifying All Features Are Numeric...")

# Get all numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

print(f"\nNumeric columns: {len(numeric_cols)}")
print(f"Non-numeric columns: {len(non_numeric_cols)}")

if non_numeric_cols:
    print(f"\nNon-numeric columns (will not be used in modeling):")
    print(f"  {non_numeric_cols}")

# ============================================================================
# 9. DEFINE FEATURE LIST FOR MODELING
# ============================================================================
print("\n[9] Defining Feature List for ML Modeling...")

# Time features
time_features = ['week_day', 'month', 'week_number', 'day_trend']

# Lag features
lag_features = ['lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14']

# Moving average features
ma_features = ['ma_7j', 'ma_28j']

# Volatility features
volatility_features = ['volatility_j7', 'volatility_j14']

# Inventory features
inventory_features = ['days_of_stock', 'Inventory_Level', 'Reorder_Point']

# Promotion features
promotion_features = ['promotion_of_the_day', 'j1_promotion', 'promotion_density']

# Lead time features
leadtime_features = ['average_lead_time', 'leadtime_variability', 'Supplier_Lead_Time_Days']

# Classification features
classification_features = ['ABC_Class_encoded', 'XYZ_Class_encoded', 'CV']

# ID features (encoded)
id_features = ['SKU_ID_encoded', 'Supplier_ID_encoded']

# Warehouse and Region features
warehouse_features = [col for col in df.columns if col.startswith('Warehouse_')]
region_features = [col for col in df.columns if col.startswith('Region_')]

# If no one-hot encoded features, use label encoded
if not warehouse_features and 'Warehouse_ID_encoded' in df.columns:
    warehouse_features = ['Warehouse_ID_encoded']
if not region_features and 'Region_encoded' in df.columns:
    region_features = ['Region_encoded']

# Combine all features
all_model_features = (
    time_features +
    lag_features +
    ma_features +
    volatility_features +
    inventory_features +
    promotion_features +
    leadtime_features +
    classification_features +
    id_features +
    warehouse_features +
    region_features
)

# Verify all features exist in dataframe
all_model_features = [f for f in all_model_features if f in df.columns]

print(f"\n✓ Total features for modeling: {len(all_model_features)}")
print("\nFeature groups:")
print(f"  Time: {len(time_features)}")
print(f"  Lag: {len(lag_features)}")
print(f"  Moving Average: {len(ma_features)}")
print(f"  Volatility: {len(volatility_features)}")
print(f"  Inventory: {len(inventory_features)}")
print(f"  Promotion: {len(promotion_features)}")
print(f"  Lead Time: {len(leadtime_features)}")
print(f"  Classification: {len(classification_features)}")
print(f"  IDs: {len(id_features)}")
print(f"  Warehouse: {len(warehouse_features)}")
print(f"  Region: {len(region_features)}")

# Verify all features are numeric
non_numeric_features = [f for f in all_model_features if f not in numeric_cols]
if non_numeric_features:
    print(f"\n⚠️ WARNING: Non-numeric features found: {non_numeric_features}")
    all_model_features = [f for f in all_model_features if f in numeric_cols]
else:
    print(f"\n✓ All {len(all_model_features)} features are numeric")

# ============================================================================
# 10. UPDATE TRAIN-TEST SPLIT WITH NEW FEATURES
# ============================================================================
print("\n[10] Updating Train-Test Split...")

# Update splits
train_df = df[df['Date'] < split_date].copy()
test_df = df[df['Date'] >= split_date].copy()

print(f"✓ Training set: {len(train_df):,} rows")
print(f"  Date range: {train_df['Date'].min()} to {train_df['Date'].max()}")
print(f"✓ Test set: {len(test_df):,} rows")
print(f"  Date range: {test_df['Date'].min()} to {test_df['Date'].max()}")

# Update test_baseline with all features
test_baseline = test_df.copy()
print(f"✓ test_baseline updated with {len(test_baseline):,} rows and all engineered features")

# ============================================================================
# 11. FINAL SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("BLOCK B COMPLETE ✓")
print("=" * 80)

print("\nKey outputs created:")
print("  ✓ baseline_metrics_df: Performance metrics for 3 baselines")
print("  ✓ sku_revenue: ABC/XYZ classification per SKU")
print("  ✓ df: Full dataset with all features")
print("  ✓ train_df: Training set with features")
print("  ✓ test_df: Test set with features")
print("  ✓ test_baseline: Test set with baseline forecasts")
print("  ✓ all_model_features: List of numeric features for ML modeling")

print("\nBaseline Performance Summary:")
print(baseline_metrics_df[['Model', 'RMSE', 'MAPE', 'WAPE']].to_string(index=False))

print(f"\nTotal features ready for modeling: {len(all_model_features)}")
print(f"All features are numeric: {len([f for f in all_model_features if f in numeric_cols]) == len(all_model_features)}")

print("\n✅ Ready for Block C: Forecast Model & Metrics")