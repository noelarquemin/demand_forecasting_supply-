{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN\n",
        "Block A: Data Loading & Preprocessing\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BLOCK A: DATA LOADING & PREPROCESSING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA LOADING\n",
        "# ============================================================================\n",
        "print(\"\\n[1] Loading Data...\")\n",
        "\n",
        "# For Google Colab - upload file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"✓ File uploaded: {filename}\")\n",
        "\n",
        "# Load data based on file extension\n",
        "if filename.endswith('.csv'):\n",
        "    df = pd.read_csv(filename)\n",
        "elif filename.endswith('.xlsx'):\n",
        "    df = pd.read_excel(filename)\n",
        "else:\n",
        "    raise ValueError(\"File must be .csv or .xlsx format\")\n",
        "\n",
        "print(f\"✓ Data loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. DATE PARSING & VALIDATION\n",
        "# ============================================================================\n",
        "print(\"\\n[2] Parsing Dates...\")\n",
        "\n",
        "# Parse Date column with automatic format inference\n",
        "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, utc=True)\n",
        "\n",
        "# Convert to date only (remove time component if present)\n",
        "df['Date'] = df['Date'].dt.date\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(f\"✓ Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "print(f\"✓ Total days in dataset: {(df['Date'].max() - df['Date'].min()).days + 1}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CREATE CONTINUOUS CALENDAR FOR EACH SKU\n",
        "# ============================================================================\n",
        "print(\"\\n[3] Creating Continuous Calendar...\")\n",
        "\n",
        "# Get all unique SKUs and date range\n",
        "all_skus = df['SKU_ID'].unique()\n",
        "date_range = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='D')\n",
        "\n",
        "print(f\"✓ Unique SKUs: {len(all_skus):,}\")\n",
        "print(f\"✓ Date range: {len(date_range)} days\")\n",
        "\n",
        "# Create complete calendar for all SKU-Date combinations\n",
        "complete_calendar = pd.DataFrame(\n",
        "    [(sku, date) for sku in all_skus for date in date_range],\n",
        "    columns=['SKU_ID', 'Date']\n",
        ")\n",
        "\n",
        "print(f\"✓ Complete calendar created: {len(complete_calendar):,} SKU-Date combinations\")\n",
        "\n",
        "# Merge with original data\n",
        "df_original_size = len(df)\n",
        "df = complete_calendar.merge(df, on=['SKU_ID', 'Date'], how='left')\n",
        "\n",
        "print(f\"✓ Data after merge: {len(df):,} rows ({len(df) - df_original_size:,} new rows added)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. HANDLE MISSING VALUES - UNITS_SOLD\n",
        "# ============================================================================\n",
        "print(\"\\n[4] Handling Missing Values - Units_Sold...\")\n",
        "\n",
        "units_sold_na = df['Units_Sold'].isna().sum()\n",
        "print(f\"Missing Units_Sold: {units_sold_na:,} ({units_sold_na/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Fill missing Units_Sold with 0 (no sales on those days)\n",
        "df['Units_Sold'] = df['Units_Sold'].fillna(0).astype(int)\n",
        "print(\"✓ Units_Sold: Missing values filled with 0\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. HANDLE MISSING VALUES - INVENTORY_LEVEL\n",
        "# ============================================================================\n",
        "print(\"\\n[5] Handling Missing Values - Inventory_Level...\")\n",
        "\n",
        "inventory_na = df['Inventory_Level'].isna().sum()\n",
        "print(f\"Missing Inventory_Level: {inventory_na:,} ({inventory_na/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Sort by SKU and Date for proper forward-filling\n",
        "df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Forward-fill Inventory_Level within each SKU group\n",
        "df['Inventory_Level'] = df.groupby('SKU_ID')['Inventory_Level'].fillna(method='ffill')\n",
        "\n",
        "# Flag rows with true NA (couldn't be filled)\n",
        "df['Inventory_NA_Flag'] = df['Inventory_Level'].isna().astype(int)\n",
        "remaining_na = df['Inventory_Level'].isna().sum()\n",
        "\n",
        "print(f\"✓ Inventory_Level: Forward-filled within SKU groups\")\n",
        "print(f\"  Remaining NA (flagged for review): {remaining_na:,}\")\n",
        "\n",
        "# Fill remaining NAs with 0 (assume starting inventory is 0 if no prior data)\n",
        "df['Inventory_Level'] = df['Inventory_Level'].fillna(0).astype(int)\n",
        "\n",
        "# ============================================================================\n",
        "# 6. HANDLE MISSING VALUES - SUPPLIER_LEAD_TIME_DAYS\n",
        "# ============================================================================\n",
        "print(\"\\n[6] Handling Missing Values - Supplier_Lead_Time_Days...\")\n",
        "\n",
        "lead_time_na = df['Supplier_Lead_Time_Days'].isna().sum()\n",
        "print(f\"Missing Supplier_Lead_Time_Days: {lead_time_na:,}\")\n",
        "\n",
        "# Calculate median lead time per supplier\n",
        "supplier_median_lead_time = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].median()\n",
        "\n",
        "# Impute missing values with supplier median\n",
        "df['Supplier_Lead_Time_Days'] = df.apply(\n",
        "    lambda row: supplier_median_lead_time[row['Supplier_ID']]\n",
        "    if pd.isna(row['Supplier_Lead_Time_Days']) and row['Supplier_ID'] in supplier_median_lead_time\n",
        "    else row['Supplier_Lead_Time_Days'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"✓ Supplier_Lead_Time_Days: Imputed with median per supplier\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. FILL OTHER MISSING VALUES\n",
        "# ============================================================================\n",
        "print(\"\\n[7] Filling Other Missing Values...\")\n",
        "\n",
        "# Fill categorical columns with forward-fill per SKU\n",
        "categorical_cols = ['Warehouse_ID', 'Supplier_ID', 'Region']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df.groupby('SKU_ID')[col].fillna(method='ffill')\n",
        "        df[col] = df.groupby('SKU_ID')[col].fillna(method='bfill')\n",
        "\n",
        "# Fill numeric columns\n",
        "numeric_fill_cols = ['Reorder_Point', 'Order_Quantity', 'Unit_Cost', 'Unit_Price', 'Demand_Forecast']\n",
        "for col in numeric_fill_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df.groupby('SKU_ID')[col].fillna(method='ffill')\n",
        "        df[col] = df.groupby('SKU_ID')[col].fillna(method='bfill')\n",
        "\n",
        "# Fill binary flags with 0\n",
        "flag_cols = ['Promotion_Flag', 'Stockout_Flag']\n",
        "for col in flag_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(0).astype(int)\n",
        "\n",
        "print(\"✓ All missing values handled\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. VALIDATE UNITS_SOLD vs INVENTORY_LEVEL LOGIC\n",
        "# ============================================================================\n",
        "print(\"\\n[8] Validating Units_Sold vs Inventory_Level Logic...\")\n",
        "\n",
        "# Check for impossible scenarios (sold more than inventory on stockout days)\n",
        "df['Potential_Issue'] = (\n",
        "    (df['Units_Sold'] > df['Inventory_Level']) &\n",
        "    (df['Stockout_Flag'] == 0)\n",
        ").astype(int)\n",
        "\n",
        "issues_found = df['Potential_Issue'].sum()\n",
        "print(f\"Potential logic issues found: {issues_found:,} rows\")\n",
        "\n",
        "if issues_found > 0:\n",
        "    print(\"  (Units_Sold > Inventory_Level but Stockout_Flag=0)\")\n",
        "    print(\"  → These will be monitored but not corrected automatically\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. FINAL DATA QUALITY CHECK\n",
        "# ============================================================================\n",
        "print(\"\\n[9] Final Data Quality Check...\")\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "missing_summary = df.isnull().sum()\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "if len(missing_summary) > 0:\n",
        "    print(missing_summary)\n",
        "else:\n",
        "    print(\"✓ No missing values remaining\")\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# ============================================================================\n",
        "# 10. CHRONOLOGICAL TRAIN-TEST SPLIT (80-20)\n",
        "# ============================================================================\n",
        "print(\"\\n[10] Creating Train-Test Split (80%-20% Chronological)...\")\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Calculate split point\n",
        "split_idx = int(len(df['Date'].unique()) * 0.8)\n",
        "split_date = sorted(df['Date'].unique())[split_idx]\n",
        "\n",
        "# Split data\n",
        "train_df = df[df['Date'] < split_date].copy()\n",
        "test_df = df[df['Date'] >= split_date].copy()\n",
        "\n",
        "print(f\"✓ Split date: {split_date}\")\n",
        "print(f\"✓ Training set: {len(train_df):,} rows ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"✓ Test set: {len(test_df):,} rows ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Date range: {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. SUMMARY STATISTICS\n",
        "# ============================================================================\n",
        "print(\"\\n[11] Summary Statistics...\")\n",
        "\n",
        "print(\"\\nDataset Overview:\")\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Unique SKUs: {df['SKU_ID'].nunique():,}\")\n",
        "print(f\"Unique Warehouses: {df['Warehouse_ID'].nunique():,}\")\n",
        "print(f\"Unique Suppliers: {df['Supplier_ID'].nunique():,}\")\n",
        "print(f\"Unique Regions: {df['Region'].nunique():,}\")\n",
        "\n",
        "print(\"\\nKey Metrics:\")\n",
        "print(f\"Total Units Sold: {df['Units_Sold'].sum():,.0f}\")\n",
        "print(f\"Average Daily Sales per SKU: {df.groupby('SKU_ID')['Units_Sold'].mean().mean():.2f}\")\n",
        "print(f\"Stockout Rate: {df['Stockout_Flag'].mean()*100:.2f}%\")\n",
        "print(f\"Promotion Rate: {df['Promotion_Flag'].mean()*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK A COMPLETE ✓\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nKey variables created:\")\n",
        "print(\"  - df: Full preprocessed dataset\")\n",
        "print(\"  - train_df: Training set (80%)\")\n",
        "print(\"  - test_df: Test set (20%)\")\n",
        "print(\"  - split_date: Date separating train/test\")\n",
        "print(\"\\nReady for Block B: Feature Engineering & Baseline\")\n",
        "\n",
        "\"\"\"\n",
        "DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN\n",
        "Block B: Feature Engineering & Baseline Models (REVISED)\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS (Additional for Block B)\n",
        "# ============================================================================\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import scipy.stats as stats\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK B: FEATURE ENGINEERING & BASELINE MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. BASELINE MODELS - CREATE FORECASTS\n",
        "# ============================================================================\n",
        "print(\"\\n[1] Building Baseline Forecasts...\")\n",
        "\n",
        "# Sort data by SKU and Date for proper lag calculations\n",
        "df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# ---- Baseline 1: Naïve Forecast (lag_sell_j-1) ----\n",
        "df['Baseline_Naive'] = df.groupby('SKU_ID')['Units_Sold'].shift(1)\n",
        "print(\"✓ Baseline_Naive created (j-1 lag)\")\n",
        "\n",
        "# ---- Baseline 2: Moving Average (7 days) ----\n",
        "df['Baseline_MA7'] = df.groupby('SKU_ID')['Units_Sold'].transform(\n",
        "    lambda x: x.rolling(window=7, min_periods=1).mean().shift(1)\n",
        ")\n",
        "print(\"✓ Baseline_MA7 created (7-day MA)\")\n",
        "\n",
        "# ---- Baseline 3: Dataset Demand_Forecast ----\n",
        "df['Baseline_Dataset'] = df['Demand_Forecast']\n",
        "print(\"✓ Baseline_Dataset created (from Demand_Forecast column)\")\n",
        "\n",
        "# Fill NaN values in baselines with 0\n",
        "df['Baseline_Naive'] = df['Baseline_Naive'].fillna(0)\n",
        "df['Baseline_MA7'] = df['Baseline_MA7'].fillna(0)\n",
        "\n",
        "print(\"\\n✓ Three baseline forecasts created and ready for evaluation\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. BASELINE PERFORMANCE METRICS (ON TEST SET)\n",
        "# ============================================================================\n",
        "print(\"\\n[2] Evaluating Baseline Performance on Test Set...\")\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"Calculate RMSE, MAE, MAPE, and WAPE\"\"\"\n",
        "    # Remove NaN values\n",
        "    mask = ~(pd.isna(y_true) | pd.isna(y_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        return None\n",
        "\n",
        "    # RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "\n",
        "    # MAE\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "\n",
        "    # MAPE (avoid division by zero)\n",
        "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / (y_true_clean + 1e-10))) * 100\n",
        "\n",
        "    # WAPE (Weighted Absolute Percentage Error)\n",
        "    wape = np.sum(np.abs(y_true_clean - y_pred_clean)) / (np.sum(y_true_clean) + 1e-10) * 100\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MAPE': mape,\n",
        "        'WAPE': wape\n",
        "    }\n",
        "\n",
        "# Create test baseline dataset\n",
        "test_baseline = df[df['Date'] >= split_date].copy()\n",
        "\n",
        "print(f\"Test baseline dataset: {len(test_baseline):,} rows\")\n",
        "\n",
        "# Calculate metrics for each baseline\n",
        "baseline_results = []\n",
        "\n",
        "# Naïve\n",
        "metrics_naive = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_Naive'],\n",
        "    'Naïve (j-1)'\n",
        ")\n",
        "if metrics_naive:\n",
        "    baseline_results.append(metrics_naive)\n",
        "\n",
        "# MA7\n",
        "metrics_ma7 = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_MA7'],\n",
        "    'Moving Avg (7d)'\n",
        ")\n",
        "if metrics_ma7:\n",
        "    baseline_results.append(metrics_ma7)\n",
        "\n",
        "# Dataset\n",
        "metrics_dataset = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_Dataset'],\n",
        "    'Dataset Forecast'\n",
        ")\n",
        "if metrics_dataset:\n",
        "    baseline_results.append(metrics_dataset)\n",
        "\n",
        "# Create baseline metrics dataframe\n",
        "baseline_metrics_df = pd.DataFrame(baseline_results)\n",
        "\n",
        "print(\"\\n✓ Baseline Performance (Test Set):\")\n",
        "print(baseline_metrics_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 3. SUPPLY CHAIN ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[3] SUPPLY CHAIN ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ---- 3.1 Descriptive Statistics ----\n",
        "print(\"\\n[3.1] Descriptive Statistics (Mean & Std Dev)...\")\n",
        "\n",
        "numeric_cols = ['Units_Sold', 'Inventory_Level', 'Supplier_Lead_Time_Days',\n",
        "                'Reorder_Point', 'Order_Quantity', 'Unit_Cost', 'Unit_Price']\n",
        "\n",
        "stats_summary = df[numeric_cols].agg(['mean', 'std', 'min', 'max'])\n",
        "print(\"\\nOverall Statistics:\")\n",
        "print(stats_summary.round(2))\n",
        "\n",
        "# ---- 3.2 Coefficient of Variation by SKU ----\n",
        "print(\"\\n[3.2] Coefficient of Variation (CV) by SKU...\")\n",
        "\n",
        "sku_stats = df.groupby('SKU_ID')['Units_Sold'].agg(['mean', 'std']).reset_index()\n",
        "sku_stats['CV'] = sku_stats['std'] / (sku_stats['mean'] + 1e-10)\n",
        "sku_stats = sku_stats.sort_values('CV', ascending=False)\n",
        "\n",
        "print(f\"\\nCV Statistics across all SKUs:\")\n",
        "print(f\"  Mean CV: {sku_stats['CV'].mean():.3f}\")\n",
        "print(f\"  Median CV: {sku_stats['CV'].median():.3f}\")\n",
        "print(f\"  Min CV: {sku_stats['CV'].min():.3f}\")\n",
        "print(f\"  Max CV: {sku_stats['CV'].max():.3f}\")\n",
        "\n",
        "print(f\"\\nTop 5 SKUs with highest variability:\")\n",
        "print(sku_stats.head()[['SKU_ID', 'mean', 'std', 'CV']].to_string(index=False))\n",
        "\n",
        "# ---- 3.3 Seasonality Analysis ----\n",
        "print(\"\\n[3.3] Identifying Seasonality...\")\n",
        "\n",
        "# Weekly seasonality (day of week)\n",
        "df['DayOfWeek'] = pd.to_datetime(df['Date']).dt.dayofweek\n",
        "weekly_pattern = df.groupby('DayOfWeek')['Units_Sold'].mean()\n",
        "\n",
        "print(\"\\nAverage Units Sold by Day of Week:\")\n",
        "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "for i, day in enumerate(days):\n",
        "    if i in weekly_pattern.index:\n",
        "        print(f\"  {day}: {weekly_pattern.iloc[i]:.2f}\")\n",
        "\n",
        "# Monthly seasonality\n",
        "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
        "monthly_pattern = df.groupby('Month')['Units_Sold'].mean()\n",
        "\n",
        "print(\"\\nAverage Units Sold by Month:\")\n",
        "for month in range(1, 13):\n",
        "    if month in monthly_pattern.index:\n",
        "        print(f\"  Month {month:2d}: {monthly_pattern.loc[month]:.2f}\")\n",
        "\n",
        "# ---- 3.4 Promotion Impact ----\n",
        "print(\"\\n[3.4] Impact of Promotion_Flag on Sales...\")\n",
        "\n",
        "promo_impact = df.groupby('Promotion_Flag')['Units_Sold'].agg(['mean', 'std', 'count'])\n",
        "print(\"\\nUnits Sold by Promotion Status:\")\n",
        "print(promo_impact)\n",
        "\n",
        "if 1 in promo_impact.index and 0 in promo_impact.index:\n",
        "    promo_uplift = (promo_impact.loc[1, 'mean'] / promo_impact.loc[0, 'mean'] - 1) * 100\n",
        "    print(f\"\\n✓ Promotion Uplift Effect: {promo_uplift:.2f}%\")\n",
        "    print(f\"  (Sales are {promo_uplift:.2f}% higher during promotions)\")\n",
        "\n",
        "# ---- 3.5 Lead Time vs Stockout Correlation ----\n",
        "print(\"\\n[3.5] Supplier Lead Time Impact on Stockouts...\")\n",
        "\n",
        "correlation = df[['Supplier_Lead_Time_Days', 'Stockout_Flag']].corr().iloc[0, 1]\n",
        "print(f\"\\nCorrelation (Lead Time vs Stockout): {correlation:.3f}\")\n",
        "\n",
        "if correlation > 0.1:\n",
        "    print(\"  → Longer lead times are associated with more stockouts\")\n",
        "elif correlation < -0.1:\n",
        "    print(\"  → Longer lead times are associated with fewer stockouts\")\n",
        "else:\n",
        "    print(\"  → Weak correlation between lead time and stockouts\")\n",
        "\n",
        "# Stockout rate by lead time\n",
        "leadtime_stockout = df.groupby('Supplier_Lead_Time_Days')['Stockout_Flag'].agg(['mean', 'count'])\n",
        "leadtime_stockout.columns = ['Stockout_Rate', 'Count']\n",
        "leadtime_stockout = leadtime_stockout[leadtime_stockout['Count'] > 100].sort_values('Stockout_Rate', ascending=False)\n",
        "\n",
        "print(\"\\nTop 5 Lead Times with Highest Stockout Rates:\")\n",
        "print(leadtime_stockout.head())\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ABC/XYZ CLASSIFICATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[4] ABC/XYZ SEGMENTATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ---- 4.1 ABC Classification (Revenue-based) ----\n",
        "print(\"\\n[4.1] ABC Classification (by Revenue)...\")\n",
        "\n",
        "# Calculate total revenue per SKU\n",
        "sku_revenue = df.groupby('SKU_ID').agg({\n",
        "    'Units_Sold': 'sum',\n",
        "    'Unit_Price': 'mean'\n",
        "}).reset_index()\n",
        "sku_revenue['Revenue'] = sku_revenue['Units_Sold'] * sku_revenue['Unit_Price']\n",
        "\n",
        "# Sort by revenue descending and calculate cumulative percentage\n",
        "sku_revenue = sku_revenue.sort_values('Revenue', ascending=False).reset_index(drop=True)\n",
        "sku_revenue['Cumulative_Revenue'] = sku_revenue['Revenue'].cumsum()\n",
        "total_revenue = sku_revenue['Revenue'].sum()\n",
        "sku_revenue['Cumulative_Pct'] = (sku_revenue['Cumulative_Revenue'] / total_revenue) * 100\n",
        "\n",
        "# Classify ABC based on cumulative revenue\n",
        "def classify_abc(cum_pct):\n",
        "    if cum_pct <= 80:\n",
        "        return 'A'\n",
        "    elif cum_pct <= 95:\n",
        "        return 'B'\n",
        "    else:\n",
        "        return 'C'\n",
        "\n",
        "sku_revenue['ABC_Class'] = sku_revenue['Cumulative_Pct'].apply(classify_abc)\n",
        "\n",
        "print(\"\\nABC Classification Distribution:\")\n",
        "abc_summary = sku_revenue.groupby('ABC_Class').agg({\n",
        "    'SKU_ID': 'count',\n",
        "    'Revenue': 'sum'\n",
        "}).reset_index()\n",
        "abc_summary.columns = ['ABC_Class', 'SKU_Count', 'Total_Revenue']\n",
        "abc_summary['SKU_Pct'] = (abc_summary['SKU_Count'] / len(sku_revenue)) * 100\n",
        "abc_summary['Revenue_Pct'] = (abc_summary['Total_Revenue'] / total_revenue) * 100\n",
        "\n",
        "for _, row in abc_summary.iterrows():\n",
        "    print(f\"  Class {row['ABC_Class']}: {row['SKU_Count']:4d} SKUs ({row['SKU_Pct']:5.1f}%) → {row['Revenue_Pct']:5.1f}% of revenue\")\n",
        "\n",
        "# ---- 4.2 XYZ Classification (Demand Variability) ----\n",
        "print(\"\\n[4.2] XYZ Classification (by Coefficient of Variation)...\")\n",
        "\n",
        "# Merge CV data\n",
        "sku_revenue = sku_revenue.merge(sku_stats[['SKU_ID', 'CV']], on='SKU_ID', how='left')\n",
        "\n",
        "# Classify XYZ based on CV\n",
        "def classify_xyz(cv):\n",
        "    if cv <= 0.5:\n",
        "        return 'X'\n",
        "    elif cv <= 1.0:\n",
        "        return 'Y'\n",
        "    else:\n",
        "        return 'Z'\n",
        "\n",
        "sku_revenue['XYZ_Class'] = sku_revenue['CV'].apply(classify_xyz)\n",
        "\n",
        "print(\"\\nXYZ Classification Distribution:\")\n",
        "xyz_summary = sku_revenue.groupby('XYZ_Class').agg({\n",
        "    'SKU_ID': 'count'\n",
        "}).reset_index()\n",
        "xyz_summary.columns = ['XYZ_Class', 'SKU_Count']\n",
        "xyz_summary['SKU_Pct'] = (xyz_summary['SKU_Count'] / len(sku_revenue)) * 100\n",
        "\n",
        "for _, row in xyz_summary.iterrows():\n",
        "    print(f\"  Class {row['XYZ_Class']}: {row['SKU_Count']:4d} SKUs ({row['SKU_Pct']:5.1f}%)\")\n",
        "\n",
        "# ---- 4.3 Combined ABC/XYZ ----\n",
        "sku_revenue['ABC_XYZ_Class'] = sku_revenue['ABC_Class'] + sku_revenue['XYZ_Class']\n",
        "\n",
        "print(\"\\n[4.3] Combined ABC/XYZ Matrix:\")\n",
        "abcxyz_matrix = pd.crosstab(sku_revenue['ABC_Class'], sku_revenue['XYZ_Class'], margins=True)\n",
        "print(abcxyz_matrix)\n",
        "\n",
        "print(\"\\n✓ ABC/XYZ classification complete\")\n",
        "print(\"  Class interpretation:\")\n",
        "print(\"    - AX: High value, low variability (predictable best-sellers)\")\n",
        "print(\"    - CZ: Low value, high variability (difficult to forecast, low priority)\")\n",
        "\n",
        "# Merge classification back to main dataframe\n",
        "df = df.merge(\n",
        "    sku_revenue[['SKU_ID', 'ABC_Class', 'XYZ_Class', 'ABC_XYZ_Class', 'CV']],\n",
        "    on='SKU_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ ABC/XYZ classes added to main dataframe\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[5] FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure sorted by SKU and Date\n",
        "df = df.sort_values(['SKU_ID', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# ---- 5.1 Time Features ----\n",
        "print(\"\\n[5.1] Creating Time Features from Date...\")\n",
        "\n",
        "df['Date_dt'] = pd.to_datetime(df['Date'])\n",
        "df['week_day'] = df['Date_dt'].dt.dayofweek\n",
        "df['month'] = df['Date_dt'].dt.month\n",
        "df['week_number'] = df['Date_dt'].dt.isocalendar().week.astype(int)\n",
        "df['day_trend'] = (df['Date_dt'] - df['Date_dt'].min()).dt.days\n",
        "\n",
        "print(\"✓ Time features: week_day, month, week_number, day_trend\")\n",
        "\n",
        "# ---- 5.2 Sales Lag Features ----\n",
        "print(\"\\n[5.2] Creating Sales Lag Features...\")\n",
        "\n",
        "df['lag_sell_j1'] = df.groupby('SKU_ID')['Units_Sold'].shift(1)\n",
        "df['lag_sell_j7'] = df.groupby('SKU_ID')['Units_Sold'].shift(7)\n",
        "df['lag_sell_j14'] = df.groupby('SKU_ID')['Units_Sold'].shift(14)\n",
        "\n",
        "print(\"✓ Lag features: lag_sell_j1, lag_sell_j7, lag_sell_j14\")\n",
        "\n",
        "# ---- 5.3 Moving Average Features ----\n",
        "print(\"\\n[5.3] Creating Moving Average Features...\")\n",
        "\n",
        "df['ma_7j'] = df.groupby('SKU_ID')['Units_Sold'].transform(\n",
        "    lambda x: x.rolling(window=7, min_periods=1).mean().shift(1)\n",
        ")\n",
        "df['ma_28j'] = df.groupby('SKU_ID')['Units_Sold'].transform(\n",
        "    lambda x: x.rolling(window=28, min_periods=1).mean().shift(1)\n",
        ")\n",
        "\n",
        "print(\"✓ Moving averages: ma_7j, ma_28j\")\n",
        "\n",
        "# ---- 5.4 Volatility Features (Standard Deviation) ----\n",
        "print(\"\\n[5.4] Creating Volatility Features...\")\n",
        "\n",
        "df['volatility_j7'] = df.groupby('SKU_ID')['Units_Sold'].transform(\n",
        "    lambda x: x.rolling(window=7, min_periods=1).std().shift(1)\n",
        ")\n",
        "df['volatility_j14'] = df.groupby('SKU_ID')['Units_Sold'].transform(\n",
        "    lambda x: x.rolling(window=14, min_periods=1).std().shift(1)\n",
        ")\n",
        "\n",
        "print(\"✓ Volatility features: volatility_j7, volatility_j14\")\n",
        "\n",
        "# ---- 5.5 Inventory Features ----\n",
        "print(\"\\n[5.5] Creating Inventory Features...\")\n",
        "\n",
        "# Days of stock = Inventory / Average daily demand\n",
        "df['days_of_stock'] = df['Inventory_Level'] / (df['ma_7j'] + 1e-10)\n",
        "df['days_of_stock'] = df['days_of_stock'].clip(upper=365)  # Cap at 1 year\n",
        "\n",
        "print(\"✓ Inventory feature: days_of_stock\")\n",
        "\n",
        "# ---- 5.6 Promotion Features ----\n",
        "print(\"\\n[5.6] Creating Promotion Features...\")\n",
        "\n",
        "df['promotion_of_the_day'] = df['Promotion_Flag'].astype(int)\n",
        "df['j1_promotion'] = df.groupby('SKU_ID')['Promotion_Flag'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "# Promotion density over last 30 days\n",
        "df['promotion_density'] = df.groupby('SKU_ID')['Promotion_Flag'].transform(\n",
        "    lambda x: x.rolling(window=30, min_periods=1).mean().shift(1)\n",
        ")\n",
        "\n",
        "print(\"✓ Promotion features: promotion_of_the_day, j1_promotion, promotion_density\")\n",
        "\n",
        "# ---- 5.7 Lead Time Features ----\n",
        "print(\"\\n[5.7] Creating Lead Time & Supplier Features...\")\n",
        "\n",
        "# Average lead time per supplier\n",
        "df['average_lead_time'] = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].transform('mean')\n",
        "\n",
        "# Lead time variability per supplier\n",
        "df['leadtime_variability'] = df.groupby('Supplier_ID')['Supplier_Lead_Time_Days'].transform('std')\n",
        "df['leadtime_variability'] = df['leadtime_variability'].fillna(0)\n",
        "\n",
        "print(\"✓ Lead time features: average_lead_time, leadtime_variability\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. CATEGORICAL ENCODING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[6] CATEGORICAL ENCODING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ---- 6.1 Label Encoding for High Cardinality Features ----\n",
        "print(\"\\n[6.1] Label Encoding for SKU_ID and Supplier_ID...\")\n",
        "\n",
        "# SKU_ID\n",
        "le_sku = LabelEncoder()\n",
        "df['SKU_ID_encoded'] = le_sku.fit_transform(df['SKU_ID'].astype(str))\n",
        "print(f\"✓ SKU_ID encoded: {df['SKU_ID'].nunique()} unique values → 0 to {df['SKU_ID_encoded'].max()}\")\n",
        "\n",
        "# Supplier_ID\n",
        "le_supplier = LabelEncoder()\n",
        "df['Supplier_ID_encoded'] = le_supplier.fit_transform(df['Supplier_ID'].astype(str))\n",
        "print(f\"✓ Supplier_ID encoded: {df['Supplier_ID'].nunique()} unique values → 0 to {df['Supplier_ID_encoded'].max()}\")\n",
        "\n",
        "# ---- 6.2 Encoding for Warehouse_ID and Region ----\n",
        "print(\"\\n[6.2] Encoding Warehouse_ID and Region...\")\n",
        "\n",
        "# Check cardinality\n",
        "warehouse_card = df['Warehouse_ID'].nunique()\n",
        "region_card = df['Region'].nunique()\n",
        "\n",
        "print(f\"Warehouse_ID cardinality: {warehouse_card}\")\n",
        "print(f\"Region cardinality: {region_card}\")\n",
        "\n",
        "# Warehouse_ID\n",
        "if warehouse_card < 20:\n",
        "    # One-hot encoding\n",
        "    warehouse_dummies = pd.get_dummies(df['Warehouse_ID'], prefix='Warehouse', drop_first=True, dtype=int)\n",
        "    df = pd.concat([df, warehouse_dummies], axis=1)\n",
        "    print(f\"✓ Warehouse_ID one-hot encoded: {warehouse_dummies.shape[1]} dummy features\")\n",
        "else:\n",
        "    # Label encoding\n",
        "    le_warehouse = LabelEncoder()\n",
        "    df['Warehouse_ID_encoded'] = le_warehouse.fit_transform(df['Warehouse_ID'].astype(str))\n",
        "    print(f\"✓ Warehouse_ID label encoded\")\n",
        "\n",
        "# Region\n",
        "if region_card < 20:\n",
        "    # One-hot encoding\n",
        "    region_dummies = pd.get_dummies(df['Region'], prefix='Region', drop_first=True, dtype=int)\n",
        "    df = pd.concat([df, region_dummies], axis=1)\n",
        "    print(f\"✓ Region one-hot encoded: {region_dummies.shape[1]} dummy features\")\n",
        "else:\n",
        "    # Label encoding\n",
        "    le_region = LabelEncoder()\n",
        "    df['Region_encoded'] = le_region.fit_transform(df['Region'].astype(str))\n",
        "    print(f\"✓ Region label encoded\")\n",
        "\n",
        "# ---- 6.3 Encode ABC/XYZ Classes ----\n",
        "print(\"\\n[6.3] Encoding ABC/XYZ Classes...\")\n",
        "\n",
        "abc_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "xyz_mapping = {'X': 3, 'Y': 2, 'Z': 1}\n",
        "\n",
        "df['ABC_Class_encoded'] = df['ABC_Class'].map(abc_mapping).fillna(0).astype(int)\n",
        "df['XYZ_Class_encoded'] = df['XYZ_Class'].map(xyz_mapping).fillna(0).astype(int)\n",
        "\n",
        "print(\"✓ ABC/XYZ classes encoded (A=3, B=2, C=1; X=3, Y=2, Z=1)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. FILL MISSING VALUES IN ENGINEERED FEATURES\n",
        "# ============================================================================\n",
        "print(\"\\n[7] Handling Missing Values in Engineered Features...\")\n",
        "\n",
        "# List of features that may have NaN due to lag/rolling operations\n",
        "feature_cols = [\n",
        "    'lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14',\n",
        "    'ma_7j', 'ma_28j',\n",
        "    'volatility_j7', 'volatility_j14',\n",
        "    'days_of_stock',\n",
        "    'j1_promotion', 'promotion_density',\n",
        "    'average_lead_time', 'leadtime_variability'\n",
        "]\n",
        "\n",
        "for col in feature_cols:\n",
        "    if col in df.columns:\n",
        "        missing_count = df[col].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            df[col] = df[col].fillna(0)\n",
        "            print(f\"  {col}: {missing_count:,} NaN values filled with 0\")\n",
        "\n",
        "print(\"\\n✓ All engineered features have no missing values\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. VERIFY ALL FEATURES ARE NUMERIC\n",
        "# ============================================================================\n",
        "print(\"\\n[8] Verifying All Features Are Numeric...\")\n",
        "\n",
        "# Get all numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumeric columns: {len(numeric_cols)}\")\n",
        "print(f\"Non-numeric columns: {len(non_numeric_cols)}\")\n",
        "\n",
        "if non_numeric_cols:\n",
        "    print(f\"\\nNon-numeric columns (will not be used in modeling):\")\n",
        "    print(f\"  {non_numeric_cols}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. DEFINE FEATURE LIST FOR MODELING\n",
        "# ============================================================================\n",
        "print(\"\\n[9] Defining Feature List for ML Modeling...\")\n",
        "\n",
        "# Time features\n",
        "time_features = ['week_day', 'month', 'week_number', 'day_trend']\n",
        "\n",
        "# Lag features\n",
        "lag_features = ['lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14']\n",
        "\n",
        "# Moving average features\n",
        "ma_features = ['ma_7j', 'ma_28j']\n",
        "\n",
        "# Volatility features\n",
        "volatility_features = ['volatility_j7', 'volatility_j14']\n",
        "\n",
        "# Inventory features\n",
        "inventory_features = ['days_of_stock', 'Inventory_Level', 'Reorder_Point']\n",
        "\n",
        "# Promotion features\n",
        "promotion_features = ['promotion_of_the_day', 'j1_promotion', 'promotion_density']\n",
        "\n",
        "# Lead time features\n",
        "leadtime_features = ['average_lead_time', 'leadtime_variability', 'Supplier_Lead_Time_Days']\n",
        "\n",
        "# Classification features\n",
        "classification_features = ['ABC_Class_encoded', 'XYZ_Class_encoded', 'CV']\n",
        "\n",
        "# ID features (encoded)\n",
        "id_features = ['SKU_ID_encoded', 'Supplier_ID_encoded']\n",
        "\n",
        "# Warehouse and Region features\n",
        "warehouse_features = [col for col in df.columns if col.startswith('Warehouse_')]\n",
        "region_features = [col for col in df.columns if col.startswith('Region_')]\n",
        "\n",
        "# If no one-hot encoded features, use label encoded\n",
        "if not warehouse_features and 'Warehouse_ID_encoded' in df.columns:\n",
        "    warehouse_features = ['Warehouse_ID_encoded']\n",
        "if not region_features and 'Region_encoded' in df.columns:\n",
        "    region_features = ['Region_encoded']\n",
        "\n",
        "# Combine all features\n",
        "all_model_features = (\n",
        "    time_features +\n",
        "    lag_features +\n",
        "    ma_features +\n",
        "    volatility_features +\n",
        "    inventory_features +\n",
        "    promotion_features +\n",
        "    leadtime_features +\n",
        "    classification_features +\n",
        "    id_features +\n",
        "    warehouse_features +\n",
        "    region_features\n",
        ")\n",
        "\n",
        "# Verify all features exist in dataframe\n",
        "all_model_features = [f for f in all_model_features if f in df.columns]\n",
        "\n",
        "print(f\"\\n✓ Total features for modeling: {len(all_model_features)}\")\n",
        "print(\"\\nFeature groups:\")\n",
        "print(f\"  Time: {len(time_features)}\")\n",
        "print(f\"  Lag: {len(lag_features)}\")\n",
        "print(f\"  Moving Average: {len(ma_features)}\")\n",
        "print(f\"  Volatility: {len(volatility_features)}\")\n",
        "print(f\"  Inventory: {len(inventory_features)}\")\n",
        "print(f\"  Promotion: {len(promotion_features)}\")\n",
        "print(f\"  Lead Time: {len(leadtime_features)}\")\n",
        "print(f\"  Classification: {len(classification_features)}\")\n",
        "print(f\"  IDs: {len(id_features)}\")\n",
        "print(f\"  Warehouse: {len(warehouse_features)}\")\n",
        "print(f\"  Region: {len(region_features)}\")\n",
        "\n",
        "# Verify all features are numeric\n",
        "non_numeric_features = [f for f in all_model_features if f not in numeric_cols]\n",
        "if non_numeric_features:\n",
        "    print(f\"\\n⚠️ WARNING: Non-numeric features found: {non_numeric_features}\")\n",
        "    all_model_features = [f for f in all_model_features if f in numeric_cols]\n",
        "else:\n",
        "    print(f\"\\n✓ All {len(all_model_features)} features are numeric\")\n",
        "\n",
        "# ============================================================================\n",
        "# 10. UPDATE TRAIN-TEST SPLIT WITH NEW FEATURES\n",
        "# ============================================================================\n",
        "print(\"\\n[10] Updating Train-Test Split...\")\n",
        "\n",
        "# Update splits\n",
        "train_df = df[df['Date'] < split_date].copy()\n",
        "test_df = df[df['Date'] >= split_date].copy()\n",
        "\n",
        "print(f\"✓ Training set: {len(train_df):,} rows\")\n",
        "print(f\"  Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"✓ Test set: {len(test_df):,} rows\")\n",
        "print(f\"  Date range: {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
        "\n",
        "# Update test_baseline with all features\n",
        "test_baseline = test_df.copy()\n",
        "print(f\"✓ test_baseline updated with {len(test_baseline):,} rows and all engineered features\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK B COMPLETE ✓\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nKey outputs created:\")\n",
        "print(\"  ✓ baseline_metrics_df: Performance metrics for 3 baselines\")\n",
        "print(\"  ✓ sku_revenue: ABC/XYZ classification per SKU\")\n",
        "print(\"  ✓ df: Full dataset with all features\")\n",
        "print(\"  ✓ train_df: Training set with features\")\n",
        "print(\"  ✓ test_df: Test set with features\")\n",
        "print(\"  ✓ test_baseline: Test set with baseline forecasts\")\n",
        "print(\"  ✓ all_model_features: List of numeric features for ML modeling\")\n",
        "\n",
        "print(\"\\nBaseline Performance Summary:\")\n",
        "print(baseline_metrics_df[['Model', 'RMSE', 'MAPE', 'WAPE']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nTotal features ready for modeling: {len(all_model_features)}\")\n",
        "print(f\"All features are numeric: {len([f for f in all_model_features if f in numeric_cols]) == len(all_model_features)}\")\n",
        "\n",
        "print(\"\\n✅ Ready for Block C: Forecast Model & Metrics\")\n",
        "\n",
        "\"\"\"\n",
        "DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN\n",
        "Block C: Forecast Model & Metrics\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS (Additional for Block C)\n",
        "# ============================================================================\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import joblib\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK C: FORECAST MODEL & METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ADD NAIVE SEASONALITY BASELINE\n",
        "# ============================================================================\n",
        "print(\"\\n[1] Adding Naive Seasonality Baseline (j-7)...\")\n",
        "\n",
        "# Add Naive Seasonality baseline to dataframe\n",
        "df['Baseline_Naive_Season'] = df.groupby('SKU_ID')['Units_Sold'].shift(7)\n",
        "df['Baseline_Naive_Season'] = df['Baseline_Naive_Season'].fillna(0)\n",
        "\n",
        "print(\"✓ Baseline_Naive_Season created (j-7 lag)\")\n",
        "\n",
        "# Update test_baseline\n",
        "test_baseline = df[df['Date'] >= split_date].copy()\n",
        "\n",
        "# Recalculate all baseline metrics\n",
        "print(\"\\n[1.1] Evaluating All Baselines on Test Set...\")\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "# Naïve (j-1)\n",
        "metrics = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_Naive'],\n",
        "    'Naïve (j-1)'\n",
        ")\n",
        "if metrics:\n",
        "    baseline_results.append(metrics)\n",
        "\n",
        "# MA7\n",
        "metrics = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_MA7'],\n",
        "    'Moving Avg (7d)'\n",
        ")\n",
        "if metrics:\n",
        "    baseline_results.append(metrics)\n",
        "\n",
        "# Naive Seasonality (j-7)\n",
        "metrics = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_Naive_Season'],\n",
        "    'Naïve Season (j-7)'\n",
        ")\n",
        "if metrics:\n",
        "    baseline_results.append(metrics)\n",
        "\n",
        "# Dataset Forecast\n",
        "metrics = calculate_metrics(\n",
        "    test_baseline['Units_Sold'],\n",
        "    test_baseline['Baseline_Dataset'],\n",
        "    'Dataset Forecast'\n",
        ")\n",
        "if metrics:\n",
        "    baseline_results.append(metrics)\n",
        "\n",
        "baseline_metrics_df = pd.DataFrame(baseline_results)\n",
        "print(\"\\n✓ All Baseline Performance (Test Set):\")\n",
        "print(baseline_metrics_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 2. PREPARE DATA FOR ML MODEL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[2] PREPARING DATA FOR ML MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[2.1] Data Structure Validation...\")\n",
        "\n",
        "# Create clean modeling dataset (remove rows with NaN in features)\n",
        "df_model = df.copy()\n",
        "\n",
        "# Check for missing values in features\n",
        "missing_features = df_model[all_model_features].isnull().sum()\n",
        "missing_features = missing_features[missing_features > 0]\n",
        "\n",
        "if len(missing_features) > 0:\n",
        "    print(f\"\\n⚠️ Features with missing values:\")\n",
        "    print(missing_features)\n",
        "    print(\"\\nRemoving rows with missing feature values...\")\n",
        "    df_model = df_model.dropna(subset=all_model_features)\n",
        "else:\n",
        "    print(\"✓ No missing values in features\")\n",
        "\n",
        "# Check for missing values in target\n",
        "target_missing = df_model['Units_Sold'].isnull().sum()\n",
        "if target_missing > 0:\n",
        "    print(f\"\\n⚠️ Target has {target_missing} missing values, removing...\")\n",
        "    df_model = df_model.dropna(subset=['Units_Sold'])\n",
        "else:\n",
        "    print(\"✓ No missing values in target\")\n",
        "\n",
        "print(f\"\\n✓ Clean dataset: {len(df_model):,} rows\")\n",
        "\n",
        "# Split into train and test\n",
        "train_df_model = df_model[df_model['Date'] < split_date].copy()\n",
        "test_df_model = df_model[df_model['Date'] >= split_date].copy()\n",
        "\n",
        "print(f\"✓ Training set: {len(train_df_model):,} rows\")\n",
        "print(f\"✓ Test set: {len(test_df_model):,} rows\")\n",
        "\n",
        "# Prepare feature matrices\n",
        "print(\"\\n[2.2] Preparing Feature Matrices...\")\n",
        "\n",
        "X_train = train_df_model[all_model_features].values\n",
        "y_train = train_df_model['Units_Sold'].values\n",
        "\n",
        "X_test = test_df_model[all_model_features].values\n",
        "y_test = test_df_model['Units_Sold'].values\n",
        "\n",
        "print(f\"\\n✓ X_train shape: {X_train.shape}\")\n",
        "print(f\"✓ y_train shape: {y_train.shape}\")\n",
        "print(f\"✓ X_test shape: {X_test.shape}\")\n",
        "print(f\"✓ y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Data type verification\n",
        "print(f\"\\n✓ X_train dtype: {X_train.dtype}\")\n",
        "print(f\"✓ y_train dtype: {y_train.dtype}\")\n",
        "\n",
        "# Check for inf/nan values\n",
        "print(f\"\\n✓ Inf values in X_train: {np.isinf(X_train).sum()}\")\n",
        "print(f\"✓ NaN values in X_train: {np.isnan(X_train).sum()}\")\n",
        "print(f\"✓ Inf values in y_train: {np.isinf(y_train).sum()}\")\n",
        "print(f\"✓ NaN values in y_train: {np.isnan(y_train).sum()}\")\n",
        "\n",
        "if np.isinf(X_train).sum() > 0 or np.isnan(X_train).sum() > 0:\n",
        "    print(\"\\n⚠️ WARNING: Invalid values detected in X_train!\")\n",
        "    # Replace inf with large number, nan with 0\n",
        "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "    print(\"✓ Invalid values replaced\")\n",
        "\n",
        "print(\"\\n✓ Data validation complete - ready for training\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. MULTI-HORIZON TARGET PREPARATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[3] MULTI-HORIZON FORECAST SETUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "horizons = [1, 7, 14]\n",
        "print(f\"\\nForecasting horizons: {horizons} days (j+1, j+7, j+14)\")\n",
        "\n",
        "# Create future target variables for each horizon\n",
        "print(\"\\n[3.1] Creating Future Targets...\")\n",
        "\n",
        "for h in horizons:\n",
        "    target_col = f'Units_Sold_j{h}'\n",
        "    df_model[target_col] = df_model.groupby('SKU_ID')['Units_Sold'].shift(-h)\n",
        "    print(f\"✓ Created target: {target_col} (Units_Sold shifted by -{h} days)\")\n",
        "\n",
        "# Remove rows where future targets are NaN\n",
        "print(\"\\n[3.2] Removing Rows with NaN Targets...\")\n",
        "\n",
        "initial_rows = len(df_model)\n",
        "df_model_clean = df_model.dropna(subset=[f'Units_Sold_j{h}' for h in horizons])\n",
        "removed_rows = initial_rows - len(df_model_clean)\n",
        "\n",
        "print(f\"✓ Rows removed: {removed_rows:,} ({removed_rows/initial_rows*100:.2f}%)\")\n",
        "print(f\"✓ Remaining rows: {len(df_model_clean):,}\")\n",
        "\n",
        "# Update train/test splits with clean data\n",
        "train_df_clean = df_model_clean[df_model_clean['Date'] < split_date].copy()\n",
        "test_df_clean = df_model_clean[df_model_clean['Date'] >= split_date].copy()\n",
        "\n",
        "print(f\"\\n✓ Clean training set: {len(train_df_clean):,} rows\")\n",
        "print(f\"✓ Clean test set: {len(test_df_clean):,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. XGBOOST MODEL TRAINING (MULTI-HORIZON)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[4] XGBOOST MODEL TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# XGBoost hyperparameters\n",
        "xgb_params = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'min_child_weight': 3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "print(f\"\\nXGBoost Hyperparameters:\")\n",
        "for key, value in xgb_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Dictionary to store models and predictions\n",
        "models = {}\n",
        "predictions_test = {}\n",
        "predictions_train = {}\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "# Train separate model for each horizon\n",
        "for h in horizons:\n",
        "    print(f\"\\n[4.{horizons.index(h)+1}] Training XGBoost for horizon j+{h}...\")\n",
        "\n",
        "    target_col = f'Units_Sold_j{h}'\n",
        "\n",
        "    # Prepare data for this horizon\n",
        "    X_train_h = train_df_clean[all_model_features].values\n",
        "    y_train_h = train_df_clean[target_col].values\n",
        "\n",
        "    X_test_h = test_df_clean[all_model_features].values\n",
        "    y_test_h = test_df_clean[target_col].values\n",
        "\n",
        "    print(f\"  Training samples: {len(X_train_h):,}\")\n",
        "    print(f\"  Test samples: {len(X_test_h):,}\")\n",
        "    print(f\"  Features: {X_train_h.shape[1]}\")\n",
        "\n",
        "    # Train model\n",
        "    print(f\"  Training in progress...\")\n",
        "    model = XGBRegressor(**xgb_params)\n",
        "    model.fit(X_train_h, y_train_h, verbose=False)\n",
        "\n",
        "    # Predict on train and test sets\n",
        "    y_pred_train = model.predict(X_train_h)\n",
        "    y_pred_train = np.maximum(y_pred_train, 0)  # Non-negative constraint\n",
        "\n",
        "    y_pred_test = model.predict(X_test_h)\n",
        "    y_pred_test = np.maximum(y_pred_test, 0)  # Non-negative constraint\n",
        "\n",
        "    # Store results\n",
        "    models[f'j{h}'] = model\n",
        "    predictions_train[f'j{h}'] = y_pred_train\n",
        "    predictions_test[f'j{h}'] = y_pred_test\n",
        "\n",
        "    # Add predictions to dataframes\n",
        "    train_df_clean[f'Pred_XGB_j{h}'] = y_pred_train\n",
        "    test_df_clean[f'Pred_XGB_j{h}'] = y_pred_test\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_test = calculate_metrics(y_test_h, y_pred_test, f'XGBoost j+{h}')\n",
        "\n",
        "    print(f\"  ✓ Model trained successfully\")\n",
        "    print(f\"  Test Set Performance:\")\n",
        "    print(f\"    RMSE: {metrics_test['RMSE']:.2f}\")\n",
        "    print(f\"    MAE: {metrics_test['MAE']:.2f}\")\n",
        "    print(f\"    MAPE: {metrics_test['MAPE']:.2f}%\")\n",
        "    print(f\"    WAPE: {metrics_test['WAPE']:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"✓ All horizon models trained successfully\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. TIME-SERIES CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[5] TIME-SERIES CROSS-VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[5.1] Walk-Forward Validation (Expanding Window)...\")\n",
        "print(\"Using TimeSeriesSplit with 5 folds\")\n",
        "\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "cv_results = []\n",
        "\n",
        "for h in horizons:\n",
        "    print(f\"\\n  Validating horizon j+{h}...\")\n",
        "\n",
        "    target_col = f'Units_Sold_j{h}'\n",
        "    X = train_df_clean[all_model_features].values\n",
        "    y = train_df_clean[target_col].values\n",
        "\n",
        "    fold_scores = {'RMSE': [], 'MAE': [], 'MAPE': [], 'WAPE': []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
        "        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
        "        y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Train model on fold\n",
        "        model_cv = XGBRegressor(**xgb_params)\n",
        "        model_cv.fit(X_fold_train, y_fold_train, verbose=False)\n",
        "\n",
        "        # Predict on validation fold\n",
        "        y_fold_pred = model_cv.predict(X_fold_val)\n",
        "        y_fold_pred = np.maximum(y_fold_pred, 0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_fold_val, y_fold_pred, f'Fold {fold}')\n",
        "\n",
        "        if metrics:\n",
        "            fold_scores['RMSE'].append(metrics['RMSE'])\n",
        "            fold_scores['MAE'].append(metrics['MAE'])\n",
        "            fold_scores['MAPE'].append(metrics['MAPE'])\n",
        "            fold_scores['WAPE'].append(metrics['WAPE'])\n",
        "\n",
        "    # Calculate average and std across folds\n",
        "    if fold_scores['RMSE']:\n",
        "        avg_metrics = {\n",
        "            'Horizon': f'j+{h}',\n",
        "            'RMSE_mean': np.mean(fold_scores['RMSE']),\n",
        "            'RMSE_std': np.std(fold_scores['RMSE']),\n",
        "            'MAE_mean': np.mean(fold_scores['MAE']),\n",
        "            'MAPE_mean': np.mean(fold_scores['MAPE']),\n",
        "            'WAPE_mean': np.mean(fold_scores['WAPE']),\n",
        "        }\n",
        "        cv_results.append(avg_metrics)\n",
        "\n",
        "        print(f\"    Cross-Validation Results:\")\n",
        "        print(f\"      RMSE: {avg_metrics['RMSE_mean']:.2f} ± {avg_metrics['RMSE_std']:.2f}\")\n",
        "        print(f\"      MAE:  {avg_metrics['MAE_mean']:.2f}\")\n",
        "        print(f\"      WAPE: {avg_metrics['WAPE_mean']:.2f}%\")\n",
        "\n",
        "cv_results_df = pd.DataFrame(cv_results)\n",
        "\n",
        "print(\"\\n✓ Cross-Validation Summary:\")\n",
        "print(cv_results_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 6. COMPREHENSIVE TECHNICAL METRICS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[6] TECHNICAL METRICS COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_smape(y_true, y_pred):\n",
        "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
        "    mask = ~(pd.isna(y_true) | pd.isna(y_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    denominator = (np.abs(y_true_clean) + np.abs(y_pred_clean)) / 2\n",
        "    smape = np.mean(np.abs(y_true_clean - y_pred_clean) / (denominator + 1e-10)) * 100\n",
        "    return smape\n",
        "\n",
        "# Compare all models for j+1 horizon\n",
        "print(\"\\n[6.1] Model Comparison for j+1 Horizon...\")\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "# Get test data\n",
        "test_eval = test_df_clean.copy()\n",
        "target_j1 = test_eval['Units_Sold_j1']\n",
        "\n",
        "# Baseline: Naïve (j-1)\n",
        "naive_pred_j1 = test_eval['Baseline_Naive']\n",
        "if not naive_pred_j1.isna().all():\n",
        "    metrics = calculate_metrics(target_j1, naive_pred_j1, 'Naïve (j-1)')\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(target_j1.values, naive_pred_j1.values)\n",
        "        comparison_results.append(metrics)\n",
        "\n",
        "# Baseline: MA7\n",
        "ma7_pred_j1 = test_eval['Baseline_MA7']\n",
        "if not ma7_pred_j1.isna().all():\n",
        "    metrics = calculate_metrics(target_j1, ma7_pred_j1, 'MA (7d)')\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(target_j1.values, ma7_pred_j1.values)\n",
        "        comparison_results.append(metrics)\n",
        "\n",
        "# Baseline: Naïve Seasonality (j-7)\n",
        "season_pred_j1 = test_eval['Baseline_Naive_Season']\n",
        "if not season_pred_j1.isna().all():\n",
        "    metrics = calculate_metrics(target_j1, season_pred_j1, 'Naïve Season (j-7)')\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(target_j1.values, season_pred_j1.values)\n",
        "        comparison_results.append(metrics)\n",
        "\n",
        "# Baseline: Dataset\n",
        "dataset_pred_j1 = test_eval['Baseline_Dataset']\n",
        "if not dataset_pred_j1.isna().all():\n",
        "    metrics = calculate_metrics(target_j1, dataset_pred_j1, 'Dataset Forecast')\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(target_j1.values, dataset_pred_j1.values)\n",
        "        comparison_results.append(metrics)\n",
        "\n",
        "# XGBoost\n",
        "xgb_pred_j1 = test_eval['Pred_XGB_j1']\n",
        "if not xgb_pred_j1.isna().all():\n",
        "    metrics = calculate_metrics(target_j1, xgb_pred_j1, 'XGBoost')\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(target_j1.values, xgb_pred_j1.values)\n",
        "        comparison_results.append(metrics)\n",
        "\n",
        "comparison_j1_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(\"\\nj+1 Horizon - All Models Comparison:\")\n",
        "print(comparison_j1_df[['Model', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'WAPE']].to_string(index=False))\n",
        "\n",
        "# Calculate improvement\n",
        "if len(comparison_j1_df) > 1:\n",
        "    baseline_rmse = comparison_j1_df[comparison_j1_df['Model'] != 'XGBoost']['RMSE'].min()\n",
        "    xgb_rmse = comparison_j1_df[comparison_j1_df['Model'] == 'XGBoost']['RMSE'].values[0]\n",
        "    improvement_pct = ((baseline_rmse - xgb_rmse) / baseline_rmse) * 100\n",
        "\n",
        "    print(f\"\\n🎯 XGBoost Performance:\")\n",
        "    print(f\"  Best Baseline RMSE: {baseline_rmse:.2f}\")\n",
        "    print(f\"  XGBoost RMSE: {xgb_rmse:.2f}\")\n",
        "    print(f\"  Improvement: {improvement_pct:+.2f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. BUSINESS METRICS - SUPPLY CHAIN KPIs\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[7] BUSINESS METRICS - SUPPLY CHAIN KPIs\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_supply_chain_kpis(df_eval, forecast_col, scenario_name):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive supply chain KPIs\n",
        "\n",
        "    Parameters:\n",
        "    - df_eval: DataFrame with actual demand and inventory data\n",
        "    - forecast_col: Column name with demand forecast (not used in calculation but for reference)\n",
        "    - scenario_name: Name of the scenario for reporting\n",
        "\n",
        "    Returns:\n",
        "    - kpi_results: Dictionary with aggregated KPIs\n",
        "    - sku_kpis: DataFrame with SKU-level KPIs\n",
        "    \"\"\"\n",
        "    df_kpi = df_eval.copy()\n",
        "\n",
        "    # --- a) Fill Rate ---\n",
        "    df_kpi['served_demand'] = np.minimum(df_kpi['Units_Sold'], df_kpi['Inventory_Level'])\n",
        "    df_kpi['total_demand'] = df_kpi['Units_Sold']\n",
        "\n",
        "    # Overall fill rate\n",
        "    total_served = df_kpi['served_demand'].sum()\n",
        "    total_demand = df_kpi['total_demand'].sum()\n",
        "    fill_rate = (total_served / (total_demand + 1e-10)) * 100\n",
        "\n",
        "    # --- b) Stockout Rate ---\n",
        "    df_kpi['days_with_stockout'] = (\n",
        "        (df_kpi['Inventory_Level'] < df_kpi['Units_Sold']) |\n",
        "        (df_kpi['Stockout_Flag'] == 1)\n",
        "    ).astype(int)\n",
        "\n",
        "    stockout_rate = (df_kpi['days_with_stockout'].sum() / len(df_kpi)) * 100\n",
        "\n",
        "    # --- c) Holding Cost ---\n",
        "    annual_storage_rate = 0.20  # 20% annual storage cost\n",
        "    df_kpi['unit_storage_cost'] = df_kpi['Unit_Cost'] * (annual_storage_rate / 365)\n",
        "    df_kpi['holding_cost_per_day'] = df_kpi['Inventory_Level'] * df_kpi['unit_storage_cost']\n",
        "    total_holding_cost = df_kpi['holding_cost_per_day'].sum()\n",
        "\n",
        "    # --- d) Stockout Cost ---\n",
        "    penalty_factor = 1.5  # 150% of margin\n",
        "    df_kpi['unit_margin'] = df_kpi['Unit_Price'] - df_kpi['Unit_Cost']\n",
        "    df_kpi['unit_stockout_cost'] = df_kpi['unit_margin'] * penalty_factor\n",
        "    df_kpi['unmet_demand'] = np.maximum(0, df_kpi['Units_Sold'] - df_kpi['Inventory_Level'])\n",
        "    df_kpi['stockout_cost_per_day'] = df_kpi['unmet_demand'] * df_kpi['unit_stockout_cost']\n",
        "    total_stockout_cost = df_kpi['stockout_cost_per_day'].sum()\n",
        "\n",
        "    # --- e) Total Cost ---\n",
        "    total_cost = total_holding_cost + total_stockout_cost\n",
        "\n",
        "    # --- SKU-Level Aggregation ---\n",
        "    sku_kpis = df_kpi.groupby('SKU_ID').agg({\n",
        "        'served_demand': 'sum',\n",
        "        'total_demand': 'sum',\n",
        "        'days_with_stockout': 'sum',\n",
        "        'holding_cost_per_day': 'sum',\n",
        "        'stockout_cost_per_day': 'sum',\n",
        "        'Units_Sold': 'sum'  # For demand weighting\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate SKU-level rates\n",
        "    sku_kpis['days_count'] = df_kpi.groupby('SKU_ID').size().values\n",
        "    sku_kpis['fill_rate'] = (sku_kpis['served_demand'] / (sku_kpis['total_demand'] + 1e-10)) * 100\n",
        "    sku_kpis['stockout_rate'] = (sku_kpis['days_with_stockout'] / sku_kpis['days_count']) * 100\n",
        "    sku_kpis['total_cost'] = sku_kpis['holding_cost_per_day'] + sku_kpis['stockout_cost_per_day']\n",
        "\n",
        "    # Weighted averages (by demand/Units_Sold)\n",
        "    total_demand_weight = sku_kpis['Units_Sold'].sum()\n",
        "    weighted_fill_rate = (sku_kpis['fill_rate'] * sku_kpis['Units_Sold']).sum() / (total_demand_weight + 1e-10)\n",
        "    weighted_stockout_rate = (sku_kpis['stockout_rate'] * sku_kpis['Units_Sold']).sum() / (total_demand_weight + 1e-10)\n",
        "\n",
        "    # Aggregate results\n",
        "    kpi_results = {\n",
        "        'Scenario': scenario_name,\n",
        "        'Fill_Rate': fill_rate,\n",
        "        'Weighted_Fill_Rate': weighted_fill_rate,\n",
        "        'Stockout_Rate': stockout_rate,\n",
        "        'Weighted_Stockout_Rate': weighted_stockout_rate,\n",
        "        'Holding_Cost': total_holding_cost,\n",
        "        'Stockout_Cost': total_stockout_cost,\n",
        "        'Total_Cost': total_cost,\n",
        "        'Total_Unmet_Demand': df_kpi['unmet_demand'].sum()\n",
        "    }\n",
        "\n",
        "    return kpi_results, sku_kpis\n",
        "\n",
        "# Calculate KPIs for Baseline Scenario\n",
        "print(\"\\n[7.1] Calculating KPIs for Baseline Scenario...\")\n",
        "print(\"(Using current inventory levels and actual demand)\")\n",
        "\n",
        "baseline_kpis, baseline_sku_kpis = calculate_supply_chain_kpis(\n",
        "    test_df_clean,\n",
        "    forecast_col='Units_Sold',  # Actual demand as baseline\n",
        "    scenario_name='Baseline (Current State)'\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Baseline Scenario Results:\")\n",
        "print(f\"  Fill Rate: {baseline_kpis['Fill_Rate']:.2f}%\")\n",
        "print(f\"  Weighted Fill Rate: {baseline_kpis['Weighted_Fill_Rate']:.2f}%\")\n",
        "print(f\"  Stockout Rate: {baseline_kpis['Stockout_Rate']:.2f}%\")\n",
        "print(f\"  Weighted Stockout Rate: {baseline_kpis['Weighted_Stockout_Rate']:.2f}%\")\n",
        "print(f\"  Holding Cost: €{baseline_kpis['Holding_Cost']:,.2f}\")\n",
        "print(f\"  Stockout Cost: €{baseline_kpis['Stockout_Cost']:,.2f}\")\n",
        "print(f\"  Total Cost: €{baseline_kpis['Total_Cost']:,.2f}\")\n",
        "print(f\"  Total Unmet Demand: {baseline_kpis['Total_Unmet_Demand']:,.0f} units\")\n",
        "\n",
        "# Calculate KPIs for XGBoost Scenario\n",
        "print(\"\\n[7.2] Simulating KPIs with XGBoost Forecast...\")\n",
        "print(\"(Note: This is a simplified simulation using j+1 forecast)\")\n",
        "\n",
        "# For simplicity, we use the actual inventory in test data\n",
        "# In a full simulation (Block D), we would adjust inventory based on forecast\n",
        "xgb_kpis, xgb_sku_kpis = calculate_supply_chain_kpis(\n",
        "    test_df_clean,\n",
        "    forecast_col='Pred_XGB_j1',\n",
        "    scenario_name='XGBoost Forecast'\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 XGBoost Scenario Results:\")\n",
        "print(f\"  Fill Rate: {xgb_kpis['Fill_Rate']:.2f}%\")\n",
        "print(f\"  Weighted Fill Rate: {xgb_kpis['Weighted_Fill_Rate']:.2f}%\")\n",
        "print(f\"  Stockout Rate: {xgb_kpis['Stockout_Rate']:.2f}%\")\n",
        "print(f\"  Weighted Stockout Rate: {xgb_kpis['Weighted_Stockout_Rate']:.2f}%\")\n",
        "print(f\"  Holding Cost: €{xgb_kpis['Holding_Cost']:,.2f}\")\n",
        "print(f\"  Stockout Cost: €{xgb_kpis['Stockout_Cost']:,.2f}\")\n",
        "print(f\"  Total Cost: €{xgb_kpis['Total_Cost']:,.2f}\")\n",
        "print(f\"  Total Unmet Demand: {xgb_kpis['Total_Unmet_Demand']:,.0f} units\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. BUSINESS IMPACT SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[8] BUSINESS IMPACT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate improvements\n",
        "cost_savings = baseline_kpis['Total_Cost'] - xgb_kpis['Total_Cost']\n",
        "cost_savings_pct = (cost_savings / baseline_kpis['Total_Cost']) * 100\n",
        "\n",
        "holding_cost_change = baseline_kpis['Holding_Cost'] - xgb_kpis['Holding_Cost']\n",
        "stockout_cost_change = baseline_kpis['Stockout_Cost'] - xgb_kpis['Stockout_Cost']\n",
        "\n",
        "fill_rate_change = xgb_kpis['Weighted_Fill_Rate'] - baseline_kpis['Weighted_Fill_Rate']\n",
        "stockout_rate_change = baseline_kpis['Weighted_Stockout_Rate'] - xgb_kpis['Weighted_Stockout_Rate']\n",
        "\n",
        "unmet_demand_reduction = baseline_kpis['Total_Unmet_Demand'] - xgb_kpis['Total_Unmet_Demand']\n",
        "\n",
        "print(\"\\n🎯 IMPACT OF XGBOOST FORECASTING vs BASELINE:\")\n",
        "print(\"\\n💰 Cost Impact:\")\n",
        "print(f\"  Total Cost Savings: €{cost_savings:,.2f} ({cost_savings_pct:+.2f}%)\")\n",
        "print(f\"    Holding Cost Change: €{holding_cost_change:,.2f}\")\n",
        "print(f\"    Stockout Cost Change: €{stockout_cost_change:,.2f}\")\n",
        "\n",
        "print(\"\\n📈 Service Level Impact:\")\n",
        "print(f\"  Fill Rate Change: {fill_rate_change:+.2f} percentage points\")\n",
        "print(f\"  Stockout Rate Reduction: {stockout_rate_change:+.2f} percentage points\")\n",
        "\n",
        "print(\"\\n📦 Operational Impact:\")\n",
        "print(f\"  Unmet Demand Reduction: {unmet_demand_reduction:,.0f} units\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "impact_comparison = pd.DataFrame([baseline_kpis, xgb_kpis])\n",
        "impact_comparison = impact_comparison[[\n",
        "    'Scenario', 'Fill_Rate', 'Weighted_Fill_Rate', 'Stockout_Rate',\n",
        "    'Weighted_Stockout_Rate', 'Holding_Cost', 'Stockout_Cost', 'Total_Cost'\n",
        "]]\n",
        "\n",
        "print(\"\\n📊 Side-by-Side Comparison:\")\n",
        "print(impact_comparison.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 9. FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[9] FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract feature importance from j+1 model\n",
        "model_j1 = models['j1']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': all_model_features,\n",
        "    'Importance': model_j1.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n🔍 Top 20 Most Important Features (j+1 model):\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "# Group importance by feature type\n",
        "feature_groups = {\n",
        "    'Time': ['week_day', 'month', 'week_number', 'day_trend'],\n",
        "    'Lag': ['lag_sell_j1', 'lag_sell_j7', 'lag_sell_j14'],\n",
        "    'Moving_Avg': ['ma_7j', 'ma_28j'],\n",
        "    'Volatility': ['volatility_j7', 'volatility_j14'],\n",
        "    'Inventory': ['days_of_stock', 'Inventory_Level', 'Reorder_Point'],\n",
        "    'Promotion': ['promotion_of_the_day', 'j1_promotion', 'promotion_density'],\n",
        "    'Lead_Time': ['average_lead_time', 'leadtime_variability', 'Supplier_Lead_Time_Days'],\n",
        "    'Classification': ['ABC_Class_encoded', 'XYZ_Class_encoded', 'CV']\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Feature Importance by Group:\")\n",
        "for group_name, features in feature_groups.items():\n",
        "    group_features = [f for f in features if f in feature_importance['Feature'].values]\n",
        "    if group_features:\n",
        "        group_importance = feature_importance[feature_importance['Feature'].isin(group_features)]['Importance'].sum()\n",
        "        print(f\"  {group_name}: {group_importance:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 10. SUMMARY AND OUTPUTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK C COMPLETE ✓\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n📦 Key Outputs Created:\")\n",
        "print(\"  ✓ models: Dictionary of XGBoost models {j1, j7, j14}\")\n",
        "print(\"  ✓ predictions_test: Test predictions for each horizon\")\n",
        "print(\"  ✓ test_df_clean: Test data with predictions and targets\")\n",
        "print(\"  ✓ comparison_j1_df: Technical metrics comparison (all models)\")\n",
        "print(\"  ✓ cv_results_df: Cross-validation results\")\n",
        "print(\"  ✓ baseline_kpis: Business KPIs for baseline scenario\")\n",
        "print(\"  ✓ xgb_kpis: Business KPIs for XGBoost scenario\")\n",
        "print(\"  ✓ baseline_sku_kpis: SKU-level KPIs (baseline)\")\n",
        "print(\"  ✓ xgb_sku_kpis: SKU-level KPIs (XGBoost)\")\n",
        "print(\"  ✓ feature_importance: Feature ranking\")\n",
        "print(\"  ✓ impact_comparison: Business impact summary\")\n",
        "\n",
        "print(\"\\n🎯 Final Performance Summary:\")\n",
        "if 'improvement_pct' in locals():\n",
        "    print(f\"  Technical: {improvement_pct:+.2f}% RMSE improvement over best baseline\")\n",
        "print(f\"  Business: €{cost_savings:,.2f} total cost savings ({cost_savings_pct:+.2f}%)\")\n",
        "print(f\"  Service: {fill_rate_change:+.2f}pp fill rate improvement\")\n",
        "print(f\"  Operations: {unmet_demand_reduction:,.0f} units less unmet demand\")\n",
        "\n",
        "print(\"\\n✅ Ready for Block D: Inventory Simulation & Monte Carlo\")\n",
        "\n",
        "\"\"\"\n",
        "DEMAND FORECASTING & INVENTORY SIMULATION - SUPPLY CHAIN\n",
        "Block D: Inventory Simulation & Monte Carlo\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS (Additional for Block D)\n",
        "# ============================================================================\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK D: INVENTORY SIMULATION & MONTE CARLO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. SAFETY STOCK AND REORDER POINT CALCULATION\n",
        "# ============================================================================\n",
        "print(\"\\n[1] CALCULATING SAFETY STOCK AND REORDER POINTS...\")\n",
        "\n",
        "# Service level by ABC class (z-score for normal distribution)\n",
        "service_level_z = {\n",
        "    'A': 1.28,  # 90% service level\n",
        "    'B': 1.06,  # 85% service level\n",
        "    'C': 0.84   # 80% service level\n",
        "}\n",
        "\n",
        "print(\"\\nService Level Targets by ABC Class:\")\n",
        "for abc_class, z_value in service_level_z.items():\n",
        "    service_pct = {1.28: 90, 1.06: 85, 0.84: 80}[z_value]\n",
        "    print(f\"  Class {abc_class}: {service_pct}% (z = {z_value})\")\n",
        "\n",
        "# Calculate safety stock and ROP per SKU\n",
        "print(\"\\n[1.1] Computing Safety Stock per SKU...\")\n",
        "\n",
        "# Get SKU-level parameters\n",
        "sku_params = df.groupby('SKU_ID').agg({\n",
        "    'Units_Sold': ['mean', 'std'],\n",
        "    'Supplier_Lead_Time_Days': 'mean',\n",
        "    'Order_Quantity': 'mean',\n",
        "    'Reorder_Point': 'mean',\n",
        "    'Unit_Cost': 'mean',\n",
        "    'Unit_Price': 'mean',\n",
        "    'ABC_Class': 'first',\n",
        "    'XYZ_Class': 'first',\n",
        "    'Supplier_ID': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "sku_params.columns = ['SKU_ID', 'avg_demand', 'std_demand', 'avg_lead_time',\n",
        "                      'avg_order_qty', 'dataset_rop', 'unit_cost', 'unit_price',\n",
        "                      'ABC_Class', 'XYZ_Class', 'Supplier_ID']\n",
        "\n",
        "# Map z-score based on ABC class\n",
        "sku_params['z_score'] = sku_params['ABC_Class'].map(service_level_z)\n",
        "\n",
        "# Calculate safety stock: z × σ_demand × √lead_time\n",
        "sku_params['safety_stock'] = (\n",
        "    sku_params['z_score'] *\n",
        "    sku_params['std_demand'] *\n",
        "    np.sqrt(sku_params['avg_lead_time'])\n",
        ")\n",
        "\n",
        "# Round to integers\n",
        "sku_params['safety_stock'] = sku_params['safety_stock'].fillna(0).round().astype(int)\n",
        "sku_params['avg_order_qty'] = sku_params['avg_order_qty'].round().astype(int)\n",
        "\n",
        "print(f\"Safety stock calculated for {len(sku_params)} SKUs\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. REORDER POINT CALCULATION\n",
        "# ============================================================================\n",
        "print(\"\\n[2] CALCULATING REORDER POINTS...\")\n",
        "\n",
        "# Parameter: use dataset ROP or calculate\n",
        "use_dataset_ROP = False\n",
        "\n",
        "if use_dataset_ROP:\n",
        "    print(\"Using dataset Reorder_Point values...\")\n",
        "    sku_params['reorder_point'] = sku_params['dataset_rop'].round().astype(int)\n",
        "else:\n",
        "    print(\"Calculating Reorder_Point: avg_demand × lead_time + safety_stock...\")\n",
        "    sku_params['reorder_point'] = (\n",
        "        sku_params['avg_demand'] * sku_params['avg_lead_time'] +\n",
        "        sku_params['safety_stock']\n",
        "    ).round().astype(int)\n",
        "\n",
        "print(f\"Reorder points calculated\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. REORDER QUANTITY LOGIC\n",
        "# ============================================================================\n",
        "print(\"\\n[3] SETTING REORDER QUANTITY LOGIC...\")\n",
        "\n",
        "# Parameter: use dataset Order_Quantity or calculate\n",
        "use_dataset_order_qty = True\n",
        "\n",
        "if use_dataset_order_qty:\n",
        "    print(\"Using dataset Order_Quantity values...\")\n",
        "    sku_params['reorder_quantity'] = sku_params['avg_order_qty']\n",
        "else:\n",
        "    print(\"Calculating reorder_quantity: 0.5 × mean(Order_Quantity)...\")\n",
        "    sku_params['reorder_quantity'] = (sku_params['avg_order_qty'] * 0.5).round().astype(int)\n",
        "\n",
        "print(f\"Reorder quantities set\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. PREPARE SIMULATION DATA\n",
        "# ============================================================================\n",
        "print(\"\\n[4] PREPARING SIMULATION DATA...\")\n",
        "\n",
        "# Get initial inventory levels (first day of test period)\n",
        "initial_inventory = df[df['Date'] == split_date].groupby('SKU_ID')['Inventory_Level'].first().reset_index()\n",
        "initial_inventory.columns = ['SKU_ID', 'initial_inventory']\n",
        "\n",
        "sku_params = sku_params.merge(initial_inventory, on='SKU_ID', how='left')\n",
        "sku_params['initial_inventory'] = sku_params['initial_inventory'].fillna(\n",
        "    sku_params['reorder_point'] * 2\n",
        ").round().astype(int)\n",
        "\n",
        "print(f\"Initial inventory set for {len(sku_params)} SKUs\")\n",
        "\n",
        "# Get supplier lead time distributions\n",
        "print(\"\\n[4.1] Building Supplier Lead Time Distributions...\")\n",
        "supplier_leadtime_dist = {}\n",
        "for supplier_id in df['Supplier_ID'].unique():\n",
        "    lead_times = df[df['Supplier_ID'] == supplier_id]['Supplier_Lead_Time_Days'].dropna().values\n",
        "    if len(lead_times) > 0:\n",
        "        supplier_leadtime_dist[supplier_id] = lead_times\n",
        "    else:\n",
        "        supplier_leadtime_dist[supplier_id] = np.array([7])\n",
        "\n",
        "print(f\"Lead time distributions created for {len(supplier_leadtime_dist)} suppliers\")\n",
        "\n",
        "# Get unique SKUs\n",
        "unique_skus = sku_params['SKU_ID'].unique()\n",
        "\n",
        "# ============================================================================\n",
        "# 5. INVENTORY SIMULATION FUNCTION\n",
        "# ============================================================================\n",
        "print(\"\\n[5] DEFINING INVENTORY SIMULATION FUNCTION...\")\n",
        "\n",
        "def simulate_inventory(sku_id, demand_forecast, sku_info, supplier_lead_dist,\n",
        "                       use_monte_carlo=False, noise_std=None):\n",
        "    \"\"\"\n",
        "    Simulate inventory for a single SKU over the forecast horizon\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize parameters\n",
        "    on_hand = sku_info['initial_inventory']\n",
        "    reorder_point = sku_info['reorder_point']\n",
        "    order_quantity = sku_info['reorder_quantity']\n",
        "\n",
        "    # Track orders (FIFO queue)\n",
        "    order_queue = deque()\n",
        "\n",
        "    # Results tracking\n",
        "    daily_inventory = []\n",
        "    daily_demand_actual = []\n",
        "    daily_served = []\n",
        "    daily_unmet = []\n",
        "    daily_stockout_flag = []\n",
        "    orders_placed = []\n",
        "    orders_received = []\n",
        "\n",
        "    # Simulation loop\n",
        "    for day_idx, forecast_demand in enumerate(demand_forecast):\n",
        "\n",
        "        # 1. Receive orders arriving today\n",
        "        received_today = 0\n",
        "        while order_queue and order_queue[0][0] == day_idx:\n",
        "            arrival_day, qty = order_queue.popleft()\n",
        "            on_hand += qty\n",
        "            received_today += qty\n",
        "\n",
        "        orders_received.append(received_today)\n",
        "\n",
        "        # 2. Determine actual demand\n",
        "        if use_monte_carlo and noise_std is not None:\n",
        "            noise = np.random.normal(0, noise_std)\n",
        "            actual_demand = max(0, forecast_demand + noise)\n",
        "        else:\n",
        "            actual_demand = forecast_demand\n",
        "\n",
        "        actual_demand = int(round(actual_demand))\n",
        "\n",
        "        # 3. Fulfill demand\n",
        "        served = min(on_hand, actual_demand)\n",
        "        unmet = max(0, actual_demand - on_hand)\n",
        "        stockout = 1 if unmet > 0 else 0\n",
        "\n",
        "        # Update inventory\n",
        "        on_hand = max(0, on_hand - served)\n",
        "\n",
        "        # 4. Check if reorder needed\n",
        "        order_placed = 0\n",
        "        if on_hand <= reorder_point:\n",
        "            if use_monte_carlo:\n",
        "                lead_time = int(np.ceil(np.random.choice(supplier_lead_dist)))\n",
        "            else:\n",
        "                lead_time = int(round(sku_info['avg_lead_time']))\n",
        "\n",
        "            arrival_day = day_idx + lead_time\n",
        "            order_queue.append((arrival_day, order_quantity))\n",
        "            order_placed = order_quantity\n",
        "\n",
        "        # 5. Record results\n",
        "        daily_inventory.append(on_hand)\n",
        "        daily_demand_actual.append(actual_demand)\n",
        "        daily_served.append(served)\n",
        "        daily_unmet.append(unmet)\n",
        "        daily_stockout_flag.append(stockout)\n",
        "        orders_placed.append(order_placed)\n",
        "\n",
        "    # Calculate KPIs\n",
        "    total_demand = sum(daily_demand_actual)\n",
        "    total_served = sum(daily_served)\n",
        "    total_unmet = sum(daily_unmet)\n",
        "\n",
        "    fill_rate = (total_served / (total_demand + 1e-10)) * 100\n",
        "    stockout_rate = (sum(daily_stockout_flag) / len(daily_stockout_flag)) * 100\n",
        "\n",
        "    # Costs\n",
        "    annual_storage_rate = 0.20\n",
        "    unit_storage_cost = sku_info['unit_cost'] * (annual_storage_rate / 365)\n",
        "    holding_cost = sum(daily_inventory) * unit_storage_cost\n",
        "\n",
        "    penalty_factor = 1.5\n",
        "    unit_margin = sku_info['unit_price'] - sku_info['unit_cost']\n",
        "    unit_stockout_cost = unit_margin * penalty_factor\n",
        "    stockout_cost = total_unmet * unit_stockout_cost\n",
        "\n",
        "    total_cost = holding_cost + stockout_cost\n",
        "\n",
        "    results = {\n",
        "        'SKU_ID': sku_id,\n",
        "        'fill_rate': fill_rate,\n",
        "        'stockout_rate': stockout_rate,\n",
        "        'holding_cost': holding_cost,\n",
        "        'stockout_cost': stockout_cost,\n",
        "        'total_cost': total_cost,\n",
        "        'total_demand': total_demand,\n",
        "        'total_served': total_served,\n",
        "        'total_unmet': total_unmet,\n",
        "        'avg_inventory': np.mean(daily_inventory),\n",
        "        'orders_placed_count': sum([1 for x in orders_placed if x > 0])\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Inventory simulation function defined\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. XGBOOST FORECAST SIMULATION j+1 (DETERMINISTIC)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[6] XGBOOST j+1 FORECAST SIMULATION (Deterministic)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nRunning XGBoost j+1 forecast simulation...\")\n",
        "\n",
        "xgb_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"XGBoost j+1 Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Pred_XGB_j1'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    xgb_sim_results.append(result)\n",
        "\n",
        "xgb_sim_df = pd.DataFrame(xgb_sim_results)\n",
        "\n",
        "print(f\"\\nXGBoost j+1 simulation complete: {len(xgb_sim_df)} SKUs\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. XGBOOST FORECAST SIMULATION j+7 (DETERMINISTIC)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[7] XGBOOST j+7 FORECAST SIMULATION (Deterministic)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nRunning XGBoost j+7 forecast simulation...\")\n",
        "\n",
        "xgb_j7_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"XGBoost j+7 Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    # Use j+7 predictions\n",
        "    demand_forecast = sku_data['Pred_XGB_j7'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    xgb_j7_sim_results.append(result)\n",
        "\n",
        "xgb_j7_sim_df = pd.DataFrame(xgb_j7_sim_results)\n",
        "\n",
        "print(f\"\\nXGBoost j+7 simulation complete: {len(xgb_j7_sim_df)} SKUs\")\n",
        "\n",
        "# 7.5. XGBOOST FORECAST SIMULATION j+14 (DETERMINISTIC)\n",
        "# ============================================================================\n",
        "print(\"\\n[7.5] XGBOOST j+14 FORECAST SIMULATION...\")\n",
        "\n",
        "xgb_j14_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"XGBoost j+14 Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Pred_XGB_j14'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    xgb_j14_sim_results.append(result)\n",
        "\n",
        "xgb_j14_sim_df = pd.DataFrame(xgb_j14_sim_results)\n",
        "\n",
        "print(f\"XGBoost j+14 simulation complete: {len(xgb_j14_sim_df)} SKUs\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. MONTE CARLO SIMULATION SETUP\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[8] MONTE CARLO SIMULATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[8.1] Selecting SKUs for Monte Carlo...\")\n",
        "\n",
        "# Identify top SKUs by revenue\n",
        "sku_revenue_for_mc = xgb_sim_df.merge(\n",
        "    sku_params[['SKU_ID', 'unit_price']],\n",
        "    on='SKU_ID',\n",
        "    how='left'\n",
        ")\n",
        "sku_revenue_for_mc['revenue'] = sku_revenue_for_mc['total_demand'] * sku_revenue_for_mc['unit_price']\n",
        "sku_revenue_for_mc = sku_revenue_for_mc.sort_values('revenue', ascending=False)\n",
        "\n",
        "# Scaling parameters\n",
        "n_top_skus = 100\n",
        "n_mc_top = 1000\n",
        "n_mc_other = 200\n",
        "\n",
        "# Select top SKUs\n",
        "top_skus = sku_revenue_for_mc.head(n_top_skus)['SKU_ID'].values\n",
        "other_skus = sku_revenue_for_mc.iloc[n_top_skus:]['SKU_ID'].values\n",
        "\n",
        "print(f\"\\nTop {len(top_skus)} SKUs selected for full Monte Carlo ({n_mc_top} runs)\")\n",
        "print(f\"Remaining {len(other_skus)} SKUs will use reduced Monte Carlo ({n_mc_other} runs)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. MONTE CARLO SIMULATION EXECUTION\n",
        "# ============================================================================\n",
        "print(\"\\n[9] RUNNING MONTE CARLO SIMULATION...\")\n",
        "\n",
        "def run_monte_carlo_sku(sku_id, n_runs, use_xgb=True):\n",
        "    \"\"\"Run Monte Carlo simulation for a single SKU\"\"\"\n",
        "\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        return None\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        return None\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    if use_xgb:\n",
        "        base_forecast = sku_data['Pred_XGB_j1'].fillna(0).values\n",
        "    else:\n",
        "        base_forecast = sku_data['Units_Sold'].values\n",
        "\n",
        "    noise_std = sku_info_row['std_demand']\n",
        "\n",
        "    mc_results = []\n",
        "    for _ in range(n_runs):\n",
        "        result = simulate_inventory(\n",
        "            sku_id=sku_id,\n",
        "            demand_forecast=base_forecast,\n",
        "            sku_info=sku_info,\n",
        "            supplier_lead_dist=supplier_lead_dist,\n",
        "            use_monte_carlo=True,\n",
        "            noise_std=noise_std\n",
        "        )\n",
        "        mc_results.append(result)\n",
        "\n",
        "    mc_df = pd.DataFrame(mc_results)\n",
        "\n",
        "    aggregated = {\n",
        "        'SKU_ID': sku_id,\n",
        "        'fill_rate_mean': mc_df['fill_rate'].mean(),\n",
        "        'fill_rate_std': mc_df['fill_rate'].std(),\n",
        "        'fill_rate_p5': mc_df['fill_rate'].quantile(0.05),\n",
        "        'fill_rate_p95': mc_df['fill_rate'].quantile(0.95),\n",
        "        'stockout_rate_mean': mc_df['stockout_rate'].mean(),\n",
        "        'stockout_rate_std': mc_df['stockout_rate'].std(),\n",
        "        'total_cost_mean': mc_df['total_cost'].mean(),\n",
        "        'total_cost_std': mc_df['total_cost'].std(),\n",
        "        'total_cost_p5': mc_df['total_cost'].quantile(0.05),\n",
        "        'total_cost_p95': mc_df['total_cost'].quantile(0.95),\n",
        "        'holding_cost_mean': mc_df['holding_cost'].mean(),\n",
        "        'stockout_cost_mean': mc_df['stockout_cost'].mean(),\n",
        "        'total_demand_mean': mc_df['total_demand'].mean(),\n",
        "        'ABC_Class': sku_info_row['ABC_Class'],\n",
        "        'XYZ_Class': sku_info_row['XYZ_Class']\n",
        "    }\n",
        "\n",
        "    return aggregated\n",
        "\n",
        "# Run Monte Carlo for top SKUs\n",
        "print(f\"\\n[9.1] Monte Carlo for Top {len(top_skus)} SKUs ({n_mc_top} runs each)...\")\n",
        "\n",
        "mc_top_results = []\n",
        "for sku_id in tqdm(top_skus, desc=\"MC Top SKUs\"):\n",
        "    result = run_monte_carlo_sku(sku_id, n_runs=n_mc_top, use_xgb=True)\n",
        "    if result:\n",
        "        mc_top_results.append(result)\n",
        "\n",
        "mc_top_df = pd.DataFrame(mc_top_results)\n",
        "print(f\"Completed: {len(mc_top_df)} top SKUs\")\n",
        "\n",
        "# Run Monte Carlo for other SKUs\n",
        "print(f\"\\n[9.2] Monte Carlo for Other SKUs ({n_mc_other} runs each)...\")\n",
        "\n",
        "max_other_skus = 200\n",
        "if len(other_skus) > max_other_skus:\n",
        "    print(f\"  Sampling {max_other_skus} out of {len(other_skus)} SKUs...\")\n",
        "    other_skus_sample = np.random.choice(other_skus, max_other_skus, replace=False)\n",
        "else:\n",
        "    other_skus_sample = other_skus\n",
        "\n",
        "mc_other_results = []\n",
        "for sku_id in tqdm(other_skus_sample, desc=\"MC Other SKUs\"):\n",
        "    result = run_monte_carlo_sku(sku_id, n_runs=n_mc_other, use_xgb=True)\n",
        "    if result:\n",
        "        mc_other_results.append(result)\n",
        "\n",
        "mc_other_df = pd.DataFrame(mc_other_results)\n",
        "print(f\"Completed: {len(mc_other_df)} other SKUs\")\n",
        "\n",
        "# Combine Monte Carlo results\n",
        "mc_combined_df = pd.concat([mc_top_df, mc_other_df], ignore_index=True)\n",
        "\n",
        "print(f\"\\nTotal Monte Carlo results: {len(mc_combined_df)} SKUs\")\n",
        "\n",
        "# ============================================================================\n",
        "# 10. BUSINESS IMPACT FOR j+7 HORIZON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[10] BUSINESS IMPACT ESTIMATION - HORIZON j+7\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Technical metrics for j+7\n",
        "print(\"\\n[10.1] Calculating Technical Metrics for j+7...\")\n",
        "\n",
        "forecast_models_j7 = {\n",
        "    'Naïve (j-1)': 'Baseline_Naive',\n",
        "    'Naïve Season (j-7)': 'Baseline_Naive_Season',\n",
        "    'Moving Avg (7d)': 'Baseline_MA7',\n",
        "    'Dataset Forecast': 'Baseline_Dataset',\n",
        "    'XGBoost ML (j+7)': 'Pred_XGB_j7'\n",
        "}\n",
        "\n",
        "technical_metrics_j7_list = []\n",
        "\n",
        "for model_name, forecast_col in forecast_models_j7.items():\n",
        "    if forecast_col not in test_df_clean.columns:\n",
        "        continue\n",
        "\n",
        "    y_true = test_df_clean['Units_Sold_j7']\n",
        "    y_pred = test_df_clean[forecast_col]\n",
        "\n",
        "    metrics = calculate_metrics(y_true, y_pred, model_name)\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(y_true.values, y_pred.values)\n",
        "        technical_metrics_j7_list.append(metrics)\n",
        "\n",
        "technical_metrics_j7_df = pd.DataFrame(technical_metrics_j7_list)\n",
        "\n",
        "print(\"\\nTechnical Metrics (j+7):\")\n",
        "print(technical_metrics_j7_df[['Model', 'RMSE', 'SMAPE']].to_string(index=False))\n",
        "\n",
        "# Aggregate KPIs for j+7\n",
        "print(\"\\n[10.2] Aggregating KPIs for j+7...\")\n",
        "\n",
        "def aggregate_kpis_j7(sim_df, model_name):\n",
        "    if len(sim_df) == 0:\n",
        "        return None\n",
        "\n",
        "    total_demand_all = sim_df['total_demand'].sum()\n",
        "    if total_demand_all == 0:\n",
        "        return None\n",
        "\n",
        "    sim_df = sim_df.copy()\n",
        "    sim_df['weight'] = sim_df['total_demand'] / total_demand_all\n",
        "\n",
        "    weighted_fill_rate = (sim_df['fill_rate'] * sim_df['weight']).sum()\n",
        "    weighted_stockout_rate = (sim_df['stockout_rate'] * sim_df['weight']).sum()\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Fill_Rate_%': weighted_fill_rate,\n",
        "        'Stockout_Rate_%': weighted_stockout_rate,\n",
        "        'Holding_Cost_€': sim_df['holding_cost'].sum(),\n",
        "        'Stockout_Cost_€': sim_df['stockout_cost'].sum(),\n",
        "        'Total_Cost_€': sim_df['total_cost'].sum()\n",
        "    }\n",
        "\n",
        "# Create comparison table for j+7\n",
        "business_kpis_j7_list = []\n",
        "\n",
        "# Note: Baselines use same simulations as j+1\n",
        "from copy import deepcopy\n",
        "business_kpis_j7_list.append(aggregate_kpis_j7(xgb_sim_df, 'Naïve (j-1)'))\n",
        "business_kpis_j7_list.append(aggregate_kpis_j7(xgb_sim_df, 'Naïve Season (j-7)'))\n",
        "business_kpis_j7_list.append(aggregate_kpis_j7(xgb_sim_df, 'Moving Avg (7d)'))\n",
        "business_kpis_j7_list.append(aggregate_kpis_j7(xgb_sim_df, 'Dataset Forecast'))\n",
        "business_kpis_j7_list.append(aggregate_kpis_j7(xgb_j7_sim_df, 'XGBoost ML (j+7)'))\n",
        "\n",
        "business_kpis_j7_list = [x for x in business_kpis_j7_list if x is not None]\n",
        "business_kpis_j7_df = pd.DataFrame(business_kpis_j7_list)\n",
        "\n",
        "# Create comparison table j+7\n",
        "comparison_j7_table = technical_metrics_j7_df.merge(\n",
        "    business_kpis_j7_df,\n",
        "    on='Model',\n",
        "    how='outer'\n",
        ")\n",
        "\n",
        "comparison_j7_columns = [\n",
        "    'Model', 'RMSE', 'SMAPE', 'Fill_Rate_%', 'Stockout_Rate_%',\n",
        "    'Holding_Cost_€', 'Stockout_Cost_€', 'Total_Cost_€', 'WAPE'\n",
        "]\n",
        "\n",
        "comparison_j7_table = comparison_j7_table[comparison_j7_columns]\n",
        "\n",
        "print(\"\\nComparison Table (j+7):\")\n",
        "print(comparison_j7_table.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 11. SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK D COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nKey Outputs Created:\")\n",
        "print(\"  xgb_sim_df - XGBoost j+1 simulation results\")\n",
        "print(\"  xgb_j7_sim_df - XGBoost j+7 simulation results\")\n",
        "print(\"  mc_combined_df - Monte Carlo simulation results\")\n",
        "print(\"  comparison_j7_table - j+7 comparison table\")\n",
        "\n",
        "print(\"\\nReady for Block E: Business Impact & Export\")\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS (Additional for Block E)\n",
        "# ============================================================================\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK E: BUSINESS IMPACT & EXPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TECHNICAL METRICS FOR ALL FORECAST MODELS (j+1)\n",
        "# ============================================================================\n",
        "print(\"\\n[1] CALCULATING TECHNICAL METRICS FOR j+1 HORIZON...\")\n",
        "\n",
        "# Prepare test data for evaluation\n",
        "test_eval = test_df_clean.copy()\n",
        "\n",
        "# Define all forecast models to evaluate\n",
        "forecast_models_j1 = {\n",
        "    'Naïve (j-1)': 'Baseline_Naive',\n",
        "    'Naïve Season (j-7)': 'Baseline_Naive_Season',\n",
        "    'Moving Avg (7d)': 'Baseline_MA7',\n",
        "    'Dataset Forecast': 'Baseline_Dataset',\n",
        "    'XGBoost ML': 'Pred_XGB_j1'\n",
        "}\n",
        "\n",
        "# Calculate technical metrics for all models\n",
        "technical_metrics_j1_list = []\n",
        "\n",
        "for model_name, forecast_col in forecast_models_j1.items():\n",
        "    if forecast_col not in test_eval.columns:\n",
        "        print(f\"  Warning: {forecast_col} not found, skipping {model_name}\")\n",
        "        continue\n",
        "\n",
        "    y_true = test_eval['Units_Sold_j1']\n",
        "    y_pred = test_eval[forecast_col]\n",
        "\n",
        "    metrics = calculate_metrics(y_true, y_pred, model_name)\n",
        "\n",
        "    if metrics:\n",
        "        metrics['SMAPE'] = calculate_smape(y_true.values, y_pred.values)\n",
        "        technical_metrics_j1_list.append(metrics)\n",
        "\n",
        "technical_metrics_j1_df = pd.DataFrame(technical_metrics_j1_list)\n",
        "\n",
        "print(\"\\nTechnical Metrics (j+1 Horizon):\")\n",
        "print(technical_metrics_j1_df[['Model', 'MAE', 'RMSE', 'SMAPE', 'WAPE']].to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 2. RUN INVENTORY SIMULATIONS FOR ALL BASELINE MODELS (j+1)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[2] RUNNING INVENTORY SIMULATIONS FOR ALL MODELS (j+1)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run simulations for all baseline models\n",
        "print(\"\\n[2.1] Naïve (j-1) simulation...\")\n",
        "\n",
        "naive_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"Naïve Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Baseline_Naive'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    naive_sim_results.append(result)\n",
        "\n",
        "naive_sim_df = pd.DataFrame(naive_sim_results)\n",
        "print(f\"Completed: {len(naive_sim_df)} SKUs\")\n",
        "\n",
        "print(\"\\n[2.2] Naïve Season (j-7) simulation...\")\n",
        "\n",
        "naive_season_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"Naïve Season Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Baseline_Naive_Season'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    naive_season_sim_results.append(result)\n",
        "\n",
        "naive_season_sim_df = pd.DataFrame(naive_season_sim_results)\n",
        "print(f\"Completed: {len(naive_season_sim_df)} SKUs\")\n",
        "\n",
        "print(\"\\n[2.3] Moving Average simulation...\")\n",
        "\n",
        "ma_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"MA Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Baseline_MA7'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    ma_sim_results.append(result)\n",
        "\n",
        "ma_sim_df = pd.DataFrame(ma_sim_results)\n",
        "print(f\"Completed: {len(ma_sim_df)} SKUs\")\n",
        "\n",
        "print(\"\\n[2.4] Dataset Forecast simulation...\")\n",
        "\n",
        "dataset_sim_results = []\n",
        "\n",
        "for sku_id in tqdm(unique_skus, desc=\"Dataset Sim\"):\n",
        "    sku_data = test_df_clean[test_df_clean['SKU_ID'] == sku_id].sort_values('Date')\n",
        "\n",
        "    if len(sku_data) == 0:\n",
        "        continue\n",
        "\n",
        "    sku_info_row = sku_params[sku_params['SKU_ID'] == sku_id]\n",
        "    if len(sku_info_row) == 0:\n",
        "        continue\n",
        "    sku_info_row = sku_info_row.iloc[0]\n",
        "\n",
        "    supplier_id = sku_info_row['Supplier_ID']\n",
        "    supplier_lead_dist = supplier_leadtime_dist.get(supplier_id, np.array([7]))\n",
        "\n",
        "    sku_info = {\n",
        "        'initial_inventory': sku_info_row['initial_inventory'],\n",
        "        'reorder_point': sku_info_row['reorder_point'],\n",
        "        'reorder_quantity': sku_info_row['reorder_quantity'],\n",
        "        'avg_lead_time': sku_info_row['avg_lead_time'],\n",
        "        'unit_cost': sku_info_row['unit_cost'],\n",
        "        'unit_price': sku_info_row['unit_price']\n",
        "    }\n",
        "\n",
        "    demand_forecast = sku_data['Baseline_Dataset'].fillna(0).values\n",
        "\n",
        "    result = simulate_inventory(\n",
        "        sku_id=sku_id,\n",
        "        demand_forecast=demand_forecast,\n",
        "        sku_info=sku_info,\n",
        "        supplier_lead_dist=supplier_lead_dist,\n",
        "        use_monte_carlo=False\n",
        "    )\n",
        "\n",
        "    result['ABC_Class'] = sku_info_row['ABC_Class']\n",
        "    result['XYZ_Class'] = sku_info_row['XYZ_Class']\n",
        "    dataset_sim_results.append(result)\n",
        "\n",
        "dataset_sim_df = pd.DataFrame(dataset_sim_results)\n",
        "print(f\"Completed: {len(dataset_sim_df)} SKUs\")\n",
        "\n",
        "print(\"\\nAll baseline simulations complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. AGGREGATE BUSINESS KPIs WITH DEMAND WEIGHTING (j+1)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[3] AGGREGATING BUSINESS KPIs (j+1, DEMAND-WEIGHTED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def aggregate_kpis_weighted(sim_df, model_name):\n",
        "    \"\"\"Aggregate KPIs with demand weighting\"\"\"\n",
        "    if len(sim_df) == 0:\n",
        "        return None\n",
        "\n",
        "    total_demand_all = sim_df['total_demand'].sum()\n",
        "    if total_demand_all == 0:\n",
        "        return None\n",
        "\n",
        "    sim_df = sim_df.copy()\n",
        "    sim_df['weight'] = sim_df['total_demand'] / total_demand_all\n",
        "\n",
        "    weighted_fill_rate = (sim_df['fill_rate'] * sim_df['weight']).sum()\n",
        "    weighted_stockout_rate = (sim_df['stockout_rate'] * sim_df['weight']).sum()\n",
        "\n",
        "    total_holding_cost = sim_df['holding_cost'].sum()\n",
        "    total_stockout_cost = sim_df['stockout_cost'].sum()\n",
        "    total_cost = sim_df['total_cost'].sum()\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Fill_Rate_%': weighted_fill_rate,\n",
        "        'Stockout_Rate_%': weighted_stockout_rate,\n",
        "        'Holding_Cost_€': total_holding_cost,\n",
        "        'Stockout_Cost_€': total_stockout_cost,\n",
        "        'Total_Cost_€': total_cost\n",
        "    }\n",
        "\n",
        "business_kpis_j1_list = []\n",
        "\n",
        "business_kpis_j1_list.append(aggregate_kpis_weighted(naive_sim_df, 'Naïve (j-1)'))\n",
        "business_kpis_j1_list.append(aggregate_kpis_weighted(naive_season_sim_df, 'Naïve Season (j-7)'))\n",
        "business_kpis_j1_list.append(aggregate_kpis_weighted(ma_sim_df, 'Moving Avg (7d)'))\n",
        "business_kpis_j1_list.append(aggregate_kpis_weighted(dataset_sim_df, 'Dataset Forecast'))\n",
        "business_kpis_j1_list.append(aggregate_kpis_weighted(xgb_sim_df, 'XGBoost ML'))\n",
        "\n",
        "business_kpis_j1_list = [x for x in business_kpis_j1_list if x is not None]\n",
        "business_kpis_j1_df = pd.DataFrame(business_kpis_j1_list)\n",
        "\n",
        "print(\"\\nBusiness KPIs (j+1, Demand-Weighted):\")\n",
        "print(business_kpis_j1_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 4. CREATE COMPARISON TABLE (j+1)\n",
        "# ============================================================================\n",
        "print(\"\\n[4] CREATING COMPARISON TABLE (j+1)...\")\n",
        "\n",
        "comparison_j1_table = technical_metrics_j1_df.merge(\n",
        "    business_kpis_j1_df,\n",
        "    on='Model',\n",
        "    how='outer'\n",
        ")\n",
        "\n",
        "comparison_j1_columns = [\n",
        "    'Model', 'RMSE', 'SMAPE', 'Fill_Rate_%', 'Stockout_Rate_%',\n",
        "    'Holding_Cost_€', 'Stockout_Cost_€', 'Total_Cost_€', 'WAPE'\n",
        "]\n",
        "\n",
        "comparison_j1_table = comparison_j1_table[comparison_j1_columns]\n",
        "\n",
        "print(\"\\nComparison Table (j+1):\")\n",
        "print(comparison_j1_table.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 5. MEASURE IMPROVEMENTS FROM ML MODEL (j+1)\n",
        "# ============================================================================\n",
        "print(\"\\n[5] MEASURING ML IMPROVEMENTS (j+1)...\")\n",
        "\n",
        "ml_j1_results = comparison_j1_table[comparison_j1_table['Model'].str.contains('XGBoost|ML')]\n",
        "if len(ml_j1_results) > 0:\n",
        "    ml_j1_results = ml_j1_results.iloc[0]\n",
        "\n",
        "    baseline_j1_results = comparison_j1_table[~comparison_j1_table['Model'].str.contains('XGBoost|ML')].copy()\n",
        "\n",
        "    if len(baseline_j1_results) > 0:\n",
        "        best_baseline_j1_idx = baseline_j1_results['Total_Cost_€'].idxmin()\n",
        "        best_baseline_j1 = baseline_j1_results.loc[best_baseline_j1_idx]\n",
        "\n",
        "        print(f\"\\nComparing ML vs Best Baseline ({best_baseline_j1['Model']})...\")\n",
        "\n",
        "        # Technical improvements\n",
        "        rmse_improvement_j1 = ((best_baseline_j1['RMSE'] - ml_j1_results['RMSE']) / best_baseline_j1['RMSE']) * 100\n",
        "        smape_improvement_j1 = ((best_baseline_j1['SMAPE'] - ml_j1_results['SMAPE']) / best_baseline_j1['SMAPE']) * 100\n",
        "\n",
        "        # Business improvements\n",
        "        fill_rate_improvement_j1 = ml_j1_results['Fill_Rate_%'] - best_baseline_j1['Fill_Rate_%']\n",
        "        fill_rate_improvement_pct_j1 = (fill_rate_improvement_j1 / best_baseline_j1['Fill_Rate_%']) * 100\n",
        "\n",
        "        stockout_reduction_j1 = best_baseline_j1['Stockout_Rate_%'] - ml_j1_results['Stockout_Rate_%']\n",
        "        stockout_reduction_pct_j1 = (stockout_reduction_j1 / best_baseline_j1['Stockout_Rate_%']) * 100\n",
        "\n",
        "        # Cost savings\n",
        "        cost_savings_j1 = best_baseline_j1['Total_Cost_€'] - ml_j1_results['Total_Cost_€']\n",
        "        cost_savings_pct_j1 = (cost_savings_j1 / best_baseline_j1['Total_Cost_€']) * 100\n",
        "\n",
        "        print(f\"\\nTechnical: RMSE {rmse_improvement_j1:+.2f}%, SMAPE {smape_improvement_j1:+.2f}%\")\n",
        "        print(f\"Service: Fill Rate {fill_rate_improvement_j1:+.2f}pp, Stockout {stockout_reduction_j1:+.2f}pp\")\n",
        "        print(f\"Cost: €{cost_savings_j1:,.2f} ({cost_savings_pct_j1:+.2f}%)\")\n",
        "\n",
        "        improvement_summary_j1 = pd.DataFrame({\n",
        "            'Metric': ['RMSE', 'SMAPE', 'Fill Rate', 'Stockout Rate', 'Total Cost'],\n",
        "            'Baseline': [\n",
        "                best_baseline_j1['RMSE'],\n",
        "                best_baseline_j1['SMAPE'],\n",
        "                best_baseline_j1['Fill_Rate_%'],\n",
        "                best_baseline_j1['Stockout_Rate_%'],\n",
        "                best_baseline_j1['Total_Cost_€']\n",
        "            ],\n",
        "            'ML_Model': [\n",
        "                ml_j1_results['RMSE'],\n",
        "                ml_j1_results['SMAPE'],\n",
        "                ml_j1_results['Fill_Rate_%'],\n",
        "                ml_j1_results['Stockout_Rate_%'],\n",
        "                ml_j1_results['Total_Cost_€']\n",
        "            ],\n",
        "            'Improvement': [\n",
        "                f\"{rmse_improvement_j1:+.2f}%\",\n",
        "                f\"{smape_improvement_j1:+.2f}%\",\n",
        "                f\"{fill_rate_improvement_j1:+.2f}pp\",\n",
        "                f\"{stockout_reduction_j1:+.2f}pp\",\n",
        "                f\"€{cost_savings_j1:,.2f}\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "# ============================================================================\n",
        "# 6. PREPARE EXPORT DATAFRAMES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[6] PREPARING DATA FOR EXPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# SKU Segmentation\n",
        "print(\"\\n[6.1] SKU Segmentation...\")\n",
        "sku_params['CV'] = sku_params['std_demand'] / (sku_params['avg_demand'] + 1e-10)\n",
        "sku_params['ABC_XYZ_Class'] = sku_params['ABC_Class'].astype(str) + \"_\" + sku_params['XYZ_Class'].astype(str)\n",
        "\n",
        "avg_inventory_by_sku = xgb_sim_df.groupby('SKU_ID')['avg_inventory'].first().reset_index()\n",
        "sku_params = sku_params.merge(avg_inventory_by_sku, on='SKU_ID', how='left')\n",
        "\n",
        "total_units_by_sku = df.groupby('SKU_ID')['Units_Sold'].sum().reset_index()\n",
        "total_units_by_sku.columns = ['SKU_ID', 'total_units_sold']\n",
        "sku_params = sku_params.merge(total_units_by_sku, on='SKU_ID', how='left')\n",
        "\n",
        "sku_segmentation = sku_params[[\n",
        "    'SKU_ID', 'ABC_Class', 'XYZ_Class', 'ABC_XYZ_Class',\n",
        "    'avg_demand', 'std_demand', 'CV', 'avg_lead_time',\n",
        "    'avg_inventory', 'total_units_sold'\n",
        "]].copy()\n",
        "\n",
        "print(f\"SKU segmentation: {len(sku_segmentation)} SKUs\")\n",
        "\n",
        "# Forecast Results j+1\n",
        "print(\"\\n[6.2] Forecast Results j+1...\")\n",
        "forecast_j1_export = test_df_clean[[\n",
        "    'Date', 'SKU_ID', 'Units_Sold',\n",
        "    'Baseline_Naive', 'Baseline_Naive_Season', 'Baseline_MA7', 'Baseline_Dataset',\n",
        "    'Pred_XGB_j1'\n",
        "]].copy()\n",
        "\n",
        "forecast_j1_export['Error_Naive'] = forecast_j1_export['Units_Sold'] - forecast_j1_export['Baseline_Naive']\n",
        "forecast_j1_export['Error_XGB'] = forecast_j1_export['Units_Sold'] - forecast_j1_export['Pred_XGB_j1']\n",
        "\n",
        "print(f\"Forecast j+1: {len(forecast_j1_export)} rows\")\n",
        "\n",
        "# Simulation Results j+1\n",
        "print(\"\\n[6.3] Simulation Results j+1...\")\n",
        "simulation_j1_export_list = []\n",
        "\n",
        "for model_name, sim_df in [\n",
        "    ('Naïve (j-1)', naive_sim_df),\n",
        "    ('Naïve Season (j-7)', naive_season_sim_df),\n",
        "    ('Moving Avg (7d)', ma_sim_df),\n",
        "    ('Dataset Forecast', dataset_sim_df),\n",
        "    ('XGBoost ML', xgb_sim_df)\n",
        "]:\n",
        "    sim_copy = sim_df.copy()\n",
        "    sim_copy['Model'] = model_name\n",
        "    simulation_j1_export_list.append(sim_copy)\n",
        "\n",
        "simulation_j1_export = pd.concat(simulation_j1_export_list, ignore_index=True)\n",
        "simulation_j1_export = simulation_j1_export[[\n",
        "    'Model', 'SKU_ID', 'ABC_Class', 'XYZ_Class',\n",
        "    'fill_rate', 'stockout_rate', 'holding_cost', 'stockout_cost', 'total_cost',\n",
        "    'total_demand', 'total_served', 'total_unmet'\n",
        "]]\n",
        "\n",
        "print(f\"Simulation j+1: {len(simulation_j1_export)} rows\")\n",
        "\n",
        "# Forecast Results j+7\n",
        "print(\"\\n[6.4] Forecast Results j+7...\")\n",
        "forecast_j7_export = test_df_clean[[\n",
        "    'Date', 'SKU_ID', 'Units_Sold',\n",
        "    'Baseline_Naive', 'Baseline_Naive_Season', 'Baseline_MA7', 'Baseline_Dataset',\n",
        "    'Pred_XGB_j7'\n",
        "]].copy()\n",
        "\n",
        "forecast_j7_export['Error_Naive'] = forecast_j7_export['Units_Sold'] - forecast_j7_export['Baseline_Naive']\n",
        "forecast_j7_export['Error_XGB_j7'] = forecast_j7_export['Units_Sold'] - forecast_j7_export['Pred_XGB_j7']\n",
        "\n",
        "print(f\"Forecast j+7: {len(forecast_j7_export)} rows\")\n",
        "\n",
        "# Simulation Results j+7\n",
        "print(\"\\n[6.5] Simulation Results j+7...\")\n",
        "simulation_j7_export_list = []\n",
        "\n",
        "for model_name, sim_df in [\n",
        "    ('Naïve (j-1)', naive_sim_df),\n",
        "    ('Naïve Season (j-7)', naive_season_sim_df),\n",
        "    ('Moving Avg (7d)', ma_sim_df),\n",
        "    ('Dataset Forecast', dataset_sim_df),\n",
        "    ('XGBoost ML (j+7)', xgb_j7_sim_df)\n",
        "]:\n",
        "    sim_copy = sim_df.copy()\n",
        "    sim_copy['Model'] = model_name\n",
        "    simulation_j7_export_list.append(sim_copy)\n",
        "\n",
        "simulation_j7_export = pd.concat(simulation_j7_export_list, ignore_index=True)\n",
        "simulation_j7_export = simulation_j7_export[[\n",
        "    'Model', 'SKU_ID', 'ABC_Class', 'XYZ_Class',\n",
        "    'fill_rate', 'stockout_rate', 'holding_cost', 'stockout_cost', 'total_cost',\n",
        "    'total_demand', 'total_served', 'total_unmet'\n",
        "]]\n",
        "\n",
        "print(f\"Simulation j+7: {len(simulation_j7_export)} rows\")\n",
        "# ============================================================================\n",
        "# 7. CREATE VALIDATION REPORT\n",
        "# ============================================================================\n",
        "print(\"\\n[7] CREATING VALIDATION REPORT...\")\n",
        "\n",
        "validation_lines = []\n",
        "validation_lines.append(\"=\" * 80)\n",
        "validation_lines.append(\"DEMAND FORECASTING & SUPPLY CHAIN - VALIDATION REPORT\")\n",
        "validation_lines.append(\"=\" * 80)\n",
        "validation_lines.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"-\" * 80)\n",
        "validation_lines.append(\"1. DATA PREPROCESSING\")\n",
        "validation_lines.append(\"-\" * 80)\n",
        "validation_lines.append(f\"Total rows: {len(df):,}\")\n",
        "validation_lines.append(f\"Unique SKUs: {df['SKU_ID'].nunique():,}\")\n",
        "validation_lines.append(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "validation_lines.append(f\"Training: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "validation_lines.append(f\"Test: {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"-\" * 80)\n",
        "validation_lines.append(\"2. MISSING VALUES\")\n",
        "validation_lines.append(\"-\" * 80)\n",
        "validation_lines.append(\"- Units_Sold: Filled with 0\")\n",
        "validation_lines.append(\"- Inventory_Level: Forward-filled per SKU\")\n",
        "validation_lines.append(\"- Supplier_Lead_Time_Days: Imputed with supplier median\")\n",
        "validation_lines.append(\"- Engineered features: Filled with 0\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"-\" * 80)\n",
        "validation_lines.append(\"3. MODEL CONFIG\")\n",
        "validation_lines.append(\"-\" * 80)\n",
        "validation_lines.append(f\"Model: XGBoost\")\n",
        "validation_lines.append(f\"Horizons: j+1, j+7, j+14\")\n",
        "validation_lines.append(f\"CV: 5-fold Time Series\")\n",
        "validation_lines.append(f\"Features: {len(all_model_features)}\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"-\" * 80)\n",
        "validation_lines.append(\"4. SIMULATION PARAMETERS\")\n",
        "validation_lines.append(\"-\" * 80)\n",
        "validation_lines.append(f\"Annual storage rate: 20%\")\n",
        "validation_lines.append(f\"Penalty factor: 1.5\")\n",
        "validation_lines.append(f\"Service levels: A=90%, B=85%, C=80%\")\n",
        "validation_lines.append(f\"Use dataset ROP: {use_dataset_ROP}\")\n",
        "validation_lines.append(f\"Monte Carlo: {n_mc_top if 'n_mc_top' in locals() else 1000} (top), {n_mc_other if 'n_mc_other' in locals() else 200} (others)\")\n",
        "validation_lines.append(f\"Random seed: 42\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"-\" * 80)\n",
        "validation_lines.append(\"5. KEY RESULTS\")\n",
        "validation_lines.append(\"-\" * 80)\n",
        "if 'best_baseline_j1' in locals():\n",
        "    validation_lines.append(f\"Horizon j+1:\")\n",
        "    validation_lines.append(f\"  Best baseline: {best_baseline_j1['Model']}\")\n",
        "    validation_lines.append(f\"  ML RMSE improvement: {rmse_improvement_j1:+.2f}%\")\n",
        "    validation_lines.append(f\"  Cost savings: €{cost_savings_j1:,.2f}\")\n",
        "\n",
        "if 'best_baseline_j7' in locals() and 'rmse_improvement_j7' in locals():\n",
        "    validation_lines.append(f\"Horizon j+7:\")\n",
        "    validation_lines.append(f\"  Best baseline: {best_baseline_j7['Model']}\")\n",
        "    validation_lines.append(f\"  ML RMSE improvement: {rmse_improvement_j7:+.2f}%\")\n",
        "    validation_lines.append(f\"  Cost savings: €{cost_savings_j7:,.2f}\")\n",
        "\n",
        "validation_lines.append(\"\\n\" + \"=\" * 80)\n",
        "validation_lines.append(\"END OF REPORT\")\n",
        "validation_lines.append(\"=\" * 80)\n",
        "\n",
        "validation_text = \"\\n\".join(validation_lines)\n",
        "\n",
        "print(\"Validation report created\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. EXPORT TO EXCEL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[8] EXPORTING TO EXCEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "output_filename = 'demand_forecasting_supply.xlsx'\n",
        "\n",
        "print(f\"\\nCreating: {output_filename}...\")\n",
        "\n",
        "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
        "\n",
        "    # j+1 Horizon Sheets\n",
        "    forecast_j1_export.head(10000).to_excel(writer, sheet_name='Forecast_J1', index=False)\n",
        "    print(\"  Sheet 'Forecast_J1'\")\n",
        "\n",
        "    simulation_j1_export.to_excel(writer, sheet_name='Simulation_J1', index=False)\n",
        "    print(\"  Sheet 'Simulation_J1'\")\n",
        "\n",
        "    comparison_j1_table.to_excel(writer, sheet_name='KPI_J1', index=False)\n",
        "    print(\"  Sheet 'KPI_J1'\")\n",
        "\n",
        "    # j+7 Horizon Sheets\n",
        "    forecast_j7_export.head(10000).to_excel(writer, sheet_name='Forecast_J7', index=False)\n",
        "    print(\"  Sheet 'Forecast_J7'\")\n",
        "\n",
        "    simulation_j7_export.to_excel(writer, sheet_name='Simulation_J7', index=False)\n",
        "    print(\"  Sheet 'Simulation_J7'\")\n",
        "\n",
        "    if 'comparison_j7_table' in globals():\n",
        "        comparison_j7_table.to_excel(writer, sheet_name='KPI_J7', index=False)\n",
        "        print(\"  Sheet 'KPI_J7'\")\n",
        "\n",
        "    # SKU Segmentation\n",
        "    sku_segmentation.to_excel(writer, sheet_name='SKU_Segmentation', index=False)\n",
        "    print(\"  Sheet 'SKU_Segmentation'\")\n",
        "\n",
        "    # Monte Carlo (if available)\n",
        "    if 'mc_combined_df' in globals() and len(mc_combined_df) > 0:\n",
        "        mc_combined_df.to_excel(writer, sheet_name='Monte_Carlo', index=False)\n",
        "        print(\"  Sheet 'Monte_Carlo'\")\n",
        "\n",
        "    # Metadata\n",
        "    metadata_df = pd.DataFrame({\n",
        "        'Parameter': [\n",
        "            'annual_storage_rate',\n",
        "            'penalty_factor',\n",
        "            'service_level_A',\n",
        "            'service_level_B',\n",
        "            'service_level_C',\n",
        "            'use_dataset_ROP',\n",
        "            'use_dataset_order_qty',\n",
        "            'random_seed',\n",
        "            'cv_folds',\n",
        "            'forecast_horizons',\n",
        "            'n_mc_top',\n",
        "            'n_mc_other'\n",
        "        ],\n",
        "        'Value': [\n",
        "            0.20,\n",
        "            1.5,\n",
        "            '90% (z=1.28)',\n",
        "            '85% (z=1.06)',\n",
        "            '80% (z=0.84)',\n",
        "            use_dataset_ROP,\n",
        "            use_dataset_order_qty if 'use_dataset_order_qty' in locals() else True,\n",
        "            42,\n",
        "            5,\n",
        "            'j+1, j+7, j+14',\n",
        "            n_mc_top if 'n_mc_top' in locals() else 1000,\n",
        "            n_mc_other if 'n_mc_other' in locals() else 200\n",
        "        ]\n",
        "    })\n",
        "    metadata_df.to_excel(writer, sheet_name='Metadata', index=False)\n",
        "    print(\"  Sheet 'Metadata'\")\n",
        "\n",
        "print(f\"\\nExcel file created: {output_filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. EXPORT ADDITIONAL FILES\n",
        "# ============================================================================\n",
        "print(\"\\n[9] EXPORTING ADDITIONAL FILES...\")\n",
        "\n",
        "# Quick check CSV\n",
        "quick_check_filename = 'quick_check.csv'\n",
        "forecast_j1_export.head(100).to_csv(quick_check_filename, index=False)\n",
        "print(f\"Quick check: {quick_check_filename}\")\n",
        "\n",
        "# Validation report\n",
        "validation_filename = 'validation_report.txt'\n",
        "with open(validation_filename, 'w') as f:\n",
        "    f.write(validation_text)\n",
        "print(f\"Validation report: {validation_filename}\")\n",
        "\n",
        "# Full forecast CSV\n",
        "forecast_full_filename = 'forecast_results_full.csv'\n",
        "forecast_j1_export.to_csv(forecast_full_filename, index=False)\n",
        "print(f\"Full forecast: {forecast_full_filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 10. FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BLOCK E COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nFILES GENERATED:\")\n",
        "print(f\"  1. {output_filename}\")\n",
        "print(f\"  2. {quick_check_filename}\")\n",
        "print(f\"  3. {validation_filename}\")\n",
        "print(f\"  4. {forecast_full_filename}\")\n",
        "\n",
        "print(\"\\nEXCEL STRUCTURE:\")\n",
        "print(\"  Forecast_J1 - Daily forecasts (horizon j+1)\")\n",
        "print(\"  Simulation_J1 - Inventory simulations (horizon j+1)\")\n",
        "print(\"  KPI_J1 - Comparison table (horizon j+1)\")\n",
        "print(\"  Forecast_J7 - Daily forecasts (horizon j+7)\")\n",
        "print(\"  Simulation_J7 - Inventory simulations (horizon j+7)\")\n",
        "print(\"  KPI_J7 - Comparison table (horizon j+7)\")\n",
        "print(\"  SKU_Segmentation - ABC/XYZ classification\")\n",
        "print(\"  Monte_Carlo - Stochastic simulation results\")\n",
        "print(\"  Metadata - Configuration parameters\")\n",
        "\n",
        "if 'best_baseline_j1' in locals():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BUSINESS IMPACT SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nHORIZON j+1 (Short-term):\")\n",
        "    print(f\"  Best Baseline: {best_baseline_j1['Model']}\")\n",
        "    print(f\"  RMSE: {rmse_improvement_j1:+.2f}%\")\n",
        "    print(f\"  Fill Rate: {fill_rate_improvement_j1:+.2f}pp\")\n",
        "    print(f\"  Cost Savings: €{cost_savings_j1:,.2f} ({cost_savings_pct_j1:+.2f}%)\")\n",
        "\n",
        "    if 'best_baseline_j7' in locals() and 'rmse_improvement_j7' in locals():\n",
        "        print(f\"\\nHORIZON j+7 (Medium-term):\")\n",
        "        print(f\"  Best Baseline: {best_baseline_j7['Model']}\")\n",
        "        print(f\"  RMSE: {rmse_improvement_j7:+.2f}%\")\n",
        "        print(f\"  Fill Rate: {fill_rate_improvement_j7:+.2f}pp\")\n",
        "        print(f\"  Cost Savings: €{cost_savings_j7:,.2f} ({cost_savings_pct_j7:+.2f}%)\")\n",
        "\n",
        "print(\"\\nANSWER: Does ML forecasting reduce costs and improve service?\")\n",
        "if 'cost_savings_j1' in locals() and cost_savings_j1 > 0:\n",
        "    print(f\"  YES at j+1: €{cost_savings_j1:,.0f} savings, {fill_rate_improvement_j1:.2f}pp fill rate improvement\")\n",
        "    if 'cost_savings_j7' in locals() and cost_savings_j7 > 0:\n",
        "        print(f\"  YES at j+7: €{cost_savings_j7:,.0f} savings, {fill_rate_improvement_j7:.2f}pp fill rate improvement\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROJECT COMPLETE - ALL 5 BLOCKS EXECUTED\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nSUMMARY:\")\n",
        "print(\"  Block A - Data Loading & Preprocessing\")\n",
        "print(\"  Block B - Feature Engineering & Baselines\")\n",
        "print(\"  Block C - Forecast Models & Metrics\")\n",
        "print(\"  Block D - Inventory Simulation & Monte Carlo\")\n",
        "print(\"  Block E - Business Impact & Export\")\n",
        "\n",
        "print(\"\\nThe complete demand forecasting and supply chain optimization system is ready\")\n",
        "print(\"All results exported and ready for dashboard integration\")\n",
        "\n",
        "# [8.1] Import Colab files module\n",
        "from google.colab import files\n",
        "\n",
        "# [8.2] Liste de tous les fichiers générés à télécharger\n",
        "files_to_download = [\n",
        "    'demand_forecasting_supply.xlsx',  # Excel principal avec forecast, simulation, KPI\n",
        "    'quick_check.csv',                 # 100 premières lignes pour vérification rapide\n",
        "    'validation_report.txt'            # Rapport des imputations et valeurs manquantes\n",
        "]\n",
        "\n",
        "# [8.3] Boucle pour télécharger chaque fichier\n",
        "for f in files_to_download:\n",
        "    try:\n",
        "        files.download(f)\n",
        "        print(f\"✅ Téléchargement lancé pour : {f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erreur lors du téléchargement de {f} : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Osvj-R-xUqvs",
        "outputId": "d788eb93-ca5a-42b7-88ee-83de4b3a5ea3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BLOCK A: DATA LOADING & PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "[1] Loading Data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a752ed82-f9cc-4580-8f3c-07a50f5eb606\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a752ed82-f9cc-4580-8f3c-07a50f5eb606\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving supply_chain_dataset11.csv to supply_chain_dataset11.csv\n",
            "✓ File uploaded: supply_chain_dataset11.csv\n",
            "✓ Data loaded: 91,250 rows × 15 columns\n",
            "\n",
            "Columns: ['Date', 'SKU_ID', 'Warehouse_ID', 'Supplier_ID', 'Region', 'Units_Sold', 'Inventory_Level', 'Supplier_Lead_Time_Days', 'Reorder_Point', 'Order_Quantity', 'Unit_Cost', 'Unit_Price', 'Promotion_Flag', 'Stockout_Flag', 'Demand_Forecast']\n",
            "\n",
            "[2] Parsing Dates...\n",
            "✓ Date range: 2024-01-01 00:00:00 to 2024-12-30 00:00:00\n",
            "✓ Total days in dataset: 365\n",
            "\n",
            "[3] Creating Continuous Calendar...\n",
            "✓ Unique SKUs: 50\n",
            "✓ Date range: 365 days\n",
            "✓ Complete calendar created: 18,250 SKU-Date combinations\n",
            "✓ Data after merge: 91,250 rows (0 new rows added)\n",
            "\n",
            "[4] Handling Missing Values - Units_Sold...\n",
            "Missing Units_Sold: 0 (0.00%)\n",
            "✓ Units_Sold: Missing values filled with 0\n",
            "\n",
            "[5] Handling Missing Values - Inventory_Level...\n",
            "Missing Inventory_Level: 0 (0.00%)\n",
            "✓ Inventory_Level: Forward-filled within SKU groups\n",
            "  Remaining NA (flagged for review): 0\n",
            "\n",
            "[6] Handling Missing Values - Supplier_Lead_Time_Days...\n",
            "Missing Supplier_Lead_Time_Days: 0\n",
            "✓ Supplier_Lead_Time_Days: Imputed with median per supplier\n",
            "\n",
            "[7] Filling Other Missing Values...\n",
            "✓ All missing values handled\n",
            "\n",
            "[8] Validating Units_Sold vs Inventory_Level Logic...\n",
            "Potential logic issues found: 0 rows\n",
            "\n",
            "[9] Final Data Quality Check...\n",
            "\n",
            "Missing values per column:\n",
            "✓ No missing values remaining\n",
            "\n",
            "Data types:\n",
            "SKU_ID                             object\n",
            "Date                       datetime64[ns]\n",
            "Warehouse_ID                       object\n",
            "Supplier_ID                        object\n",
            "Region                             object\n",
            "Units_Sold                          int64\n",
            "Inventory_Level                     int64\n",
            "Supplier_Lead_Time_Days             int64\n",
            "Reorder_Point                       int64\n",
            "Order_Quantity                      int64\n",
            "Unit_Cost                         float64\n",
            "Unit_Price                        float64\n",
            "Promotion_Flag                      int64\n",
            "Stockout_Flag                       int64\n",
            "Demand_Forecast                   float64\n",
            "Inventory_NA_Flag                   int64\n",
            "Potential_Issue                     int64\n",
            "dtype: object\n",
            "\n",
            "[10] Creating Train-Test Split (80%-20% Chronological)...\n",
            "✓ Split date: 2024-10-19 00:00:00\n",
            "✓ Training set: 73,000 rows (80.0%)\n",
            "  Date range: 2024-01-01 00:00:00 to 2024-10-18 00:00:00\n",
            "✓ Test set: 18,250 rows (20.0%)\n",
            "  Date range: 2024-10-19 00:00:00 to 2024-12-30 00:00:00\n",
            "\n",
            "[11] Summary Statistics...\n",
            "\n",
            "Dataset Overview:\n",
            "Total rows: 91,250\n",
            "Unique SKUs: 50\n",
            "Unique Warehouses: 5\n",
            "Unique Suppliers: 10\n",
            "Unique Regions: 4\n",
            "\n",
            "Key Metrics:\n",
            "Total Units Sold: 1,829,979\n",
            "Average Daily Sales per SKU: 20.05\n",
            "Stockout Rate: 0.00%\n",
            "Promotion Rate: 10.16%\n",
            "\n",
            "================================================================================\n",
            "BLOCK A COMPLETE ✓\n",
            "================================================================================\n",
            "\n",
            "Key variables created:\n",
            "  - df: Full preprocessed dataset\n",
            "  - train_df: Training set (80%)\n",
            "  - test_df: Test set (20%)\n",
            "  - split_date: Date separating train/test\n",
            "\n",
            "Ready for Block B: Feature Engineering & Baseline\n",
            "\n",
            "================================================================================\n",
            "BLOCK B: FEATURE ENGINEERING & BASELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "[1] Building Baseline Forecasts...\n",
            "✓ Baseline_Naive created (j-1 lag)\n",
            "✓ Baseline_MA7 created (7-day MA)\n",
            "✓ Baseline_Dataset created (from Demand_Forecast column)\n",
            "\n",
            "✓ Three baseline forecasts created and ready for evaluation\n",
            "\n",
            "[2] Evaluating Baseline Performance on Test Set...\n",
            "Test baseline dataset: 18,250 rows\n",
            "\n",
            "✓ Baseline Performance (Test Set):\n",
            "           Model     RMSE      MAE         MAPE      WAPE\n",
            "     Naïve (j-1) 7.487012 5.942466 1.013699e+11 40.947706\n",
            " Moving Avg (7d) 5.648364 4.483209 9.571820e+10 30.892419\n",
            "Dataset Forecast 2.971703 2.366204 9.824110e+09 16.304784\n",
            "\n",
            "================================================================================\n",
            "[3] SUPPLY CHAIN ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "[3.1] Descriptive Statistics (Mean & Std Dev)...\n",
            "\n",
            "Overall Statistics:\n",
            "      Units_Sold  Inventory_Level  Supplier_Lead_Time_Days  Reorder_Point  \\\n",
            "mean       20.05           301.77                     7.98         300.07   \n",
            "std         9.07            85.43                     3.91          54.88   \n",
            "min         0.00           107.00                     2.00         201.00   \n",
            "max        59.00           634.00                    14.00         398.00   \n",
            "\n",
            "      Order_Quantity  Unit_Cost  Unit_Price  \n",
            "mean           19.27      12.20       18.26  \n",
            "std            82.34       4.57        7.12  \n",
            "min             0.00       5.02        6.95  \n",
            "max           499.00      19.76       35.10  \n",
            "\n",
            "[3.2] Coefficient of Variation (CV) by SKU...\n",
            "\n",
            "CV Statistics across all SKUs:\n",
            "  Mean CV: 0.452\n",
            "  Median CV: 0.453\n",
            "  Min CV: 0.439\n",
            "  Max CV: 0.467\n",
            "\n",
            "Top 5 SKUs with highest variability:\n",
            "SKU_ID      mean      std       CV\n",
            "SKU_29 20.129863 9.402146 0.467074\n",
            " SKU_7 19.979178 9.263599 0.463663\n",
            "SKU_46 19.873973 9.202135 0.463024\n",
            "SKU_26 19.944110 9.228825 0.462734\n",
            "SKU_21 19.948493 9.225761 0.462479\n",
            "\n",
            "[3.3] Identifying Seasonality...\n",
            "\n",
            "Average Units Sold by Day of Week:\n",
            "  Monday: 20.08\n",
            "  Tuesday: 20.06\n",
            "  Wednesday: 19.99\n",
            "  Thursday: 20.04\n",
            "  Friday: 20.08\n",
            "  Saturday: 20.02\n",
            "  Sunday: 20.11\n",
            "\n",
            "Average Units Sold by Month:\n",
            "  Month  1: 22.80\n",
            "  Month  2: 27.20\n",
            "  Month  3: 29.88\n",
            "  Month  4: 29.84\n",
            "  Month  5: 27.21\n",
            "  Month  6: 22.67\n",
            "  Month  7: 17.41\n",
            "  Month  8: 12.78\n",
            "  Month  9: 10.17\n",
            "  Month 10: 10.27\n",
            "  Month 11: 13.08\n",
            "  Month 12: 17.59\n",
            "\n",
            "[3.4] Impact of Promotion_Flag on Sales...\n",
            "\n",
            "Units Sold by Promotion Status:\n",
            "                     mean        std  count\n",
            "Promotion_Flag                             \n",
            "0               19.504977   8.608529  81980\n",
            "1               24.914887  11.308462   9270\n",
            "\n",
            "✓ Promotion Uplift Effect: 27.74%\n",
            "  (Sales are 27.74% higher during promotions)\n",
            "\n",
            "[3.5] Supplier Lead Time Impact on Stockouts...\n",
            "\n",
            "Correlation (Lead Time vs Stockout): nan\n",
            "  → Weak correlation between lead time and stockouts\n",
            "\n",
            "Top 5 Lead Times with Highest Stockout Rates:\n",
            "                         Stockout_Rate  Count\n",
            "Supplier_Lead_Time_Days                      \n",
            "2                                  0.0   8030\n",
            "3                                  0.0   8030\n",
            "4                                  0.0   8395\n",
            "5                                  0.0   5475\n",
            "6                                  0.0   4745\n",
            "\n",
            "================================================================================\n",
            "[4] ABC/XYZ SEGMENTATION\n",
            "================================================================================\n",
            "\n",
            "[4.1] ABC Classification (by Revenue)...\n",
            "\n",
            "ABC Classification Distribution:\n",
            "  Class A:   37 SKUs ( 74.0%) →  79.2% of revenue\n",
            "  Class B:    9 SKUs ( 18.0%) →  15.2% of revenue\n",
            "  Class C:    4 SKUs (  8.0%) →   5.6% of revenue\n",
            "\n",
            "[4.2] XYZ Classification (by Coefficient of Variation)...\n",
            "\n",
            "XYZ Classification Distribution:\n",
            "  Class X:   50 SKUs (100.0%)\n",
            "\n",
            "[4.3] Combined ABC/XYZ Matrix:\n",
            "XYZ_Class   X  All\n",
            "ABC_Class         \n",
            "A          37   37\n",
            "B           9    9\n",
            "C           4    4\n",
            "All        50   50\n",
            "\n",
            "✓ ABC/XYZ classification complete\n",
            "  Class interpretation:\n",
            "    - AX: High value, low variability (predictable best-sellers)\n",
            "    - CZ: Low value, high variability (difficult to forecast, low priority)\n",
            "\n",
            "✓ ABC/XYZ classes added to main dataframe\n",
            "\n",
            "================================================================================\n",
            "[5] FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "[5.1] Creating Time Features from Date...\n",
            "✓ Time features: week_day, month, week_number, day_trend\n",
            "\n",
            "[5.2] Creating Sales Lag Features...\n",
            "✓ Lag features: lag_sell_j1, lag_sell_j7, lag_sell_j14\n",
            "\n",
            "[5.3] Creating Moving Average Features...\n",
            "✓ Moving averages: ma_7j, ma_28j\n",
            "\n",
            "[5.4] Creating Volatility Features...\n",
            "✓ Volatility features: volatility_j7, volatility_j14\n",
            "\n",
            "[5.5] Creating Inventory Features...\n",
            "✓ Inventory feature: days_of_stock\n",
            "\n",
            "[5.6] Creating Promotion Features...\n",
            "✓ Promotion features: promotion_of_the_day, j1_promotion, promotion_density\n",
            "\n",
            "[5.7] Creating Lead Time & Supplier Features...\n",
            "✓ Lead time features: average_lead_time, leadtime_variability\n",
            "\n",
            "================================================================================\n",
            "[6] CATEGORICAL ENCODING\n",
            "================================================================================\n",
            "\n",
            "[6.1] Label Encoding for SKU_ID and Supplier_ID...\n",
            "✓ SKU_ID encoded: 50 unique values → 0 to 49\n",
            "✓ Supplier_ID encoded: 10 unique values → 0 to 9\n",
            "\n",
            "[6.2] Encoding Warehouse_ID and Region...\n",
            "Warehouse_ID cardinality: 5\n",
            "Region cardinality: 4\n",
            "✓ Warehouse_ID one-hot encoded: 4 dummy features\n",
            "✓ Region one-hot encoded: 3 dummy features\n",
            "\n",
            "[6.3] Encoding ABC/XYZ Classes...\n",
            "✓ ABC/XYZ classes encoded (A=3, B=2, C=1; X=3, Y=2, Z=1)\n",
            "\n",
            "[7] Handling Missing Values in Engineered Features...\n",
            "  lag_sell_j1: 50 NaN values filled with 0\n",
            "  lag_sell_j7: 350 NaN values filled with 0\n",
            "  lag_sell_j14: 700 NaN values filled with 0\n",
            "  ma_7j: 50 NaN values filled with 0\n",
            "  ma_28j: 50 NaN values filled with 0\n",
            "  volatility_j7: 100 NaN values filled with 0\n",
            "  volatility_j14: 100 NaN values filled with 0\n",
            "  days_of_stock: 50 NaN values filled with 0\n",
            "  promotion_density: 50 NaN values filled with 0\n",
            "\n",
            "✓ All engineered features have no missing values\n",
            "\n",
            "[8] Verifying All Features Are Numeric...\n",
            "\n",
            "Numeric columns: 46\n",
            "Non-numeric columns: 9\n",
            "\n",
            "Non-numeric columns (will not be used in modeling):\n",
            "  ['SKU_ID', 'Date', 'Warehouse_ID', 'Supplier_ID', 'Region', 'ABC_Class', 'XYZ_Class', 'ABC_XYZ_Class', 'Date_dt']\n",
            "\n",
            "[9] Defining Feature List for ML Modeling...\n",
            "\n",
            "✓ Total features for modeling: 33\n",
            "\n",
            "Feature groups:\n",
            "  Time: 4\n",
            "  Lag: 3\n",
            "  Moving Average: 2\n",
            "  Volatility: 2\n",
            "  Inventory: 3\n",
            "  Promotion: 3\n",
            "  Lead Time: 3\n",
            "  Classification: 3\n",
            "  IDs: 2\n",
            "  Warehouse: 5\n",
            "  Region: 3\n",
            "\n",
            "⚠️ WARNING: Non-numeric features found: ['Warehouse_ID']\n",
            "\n",
            "[10] Updating Train-Test Split...\n",
            "✓ Training set: 73,000 rows\n",
            "  Date range: 2024-01-01 00:00:00 to 2024-10-18 00:00:00\n",
            "✓ Test set: 18,250 rows\n",
            "  Date range: 2024-10-19 00:00:00 to 2024-12-30 00:00:00\n",
            "✓ test_baseline updated with 18,250 rows and all engineered features\n",
            "\n",
            "================================================================================\n",
            "BLOCK B COMPLETE ✓\n",
            "================================================================================\n",
            "\n",
            "Key outputs created:\n",
            "  ✓ baseline_metrics_df: Performance metrics for 3 baselines\n",
            "  ✓ sku_revenue: ABC/XYZ classification per SKU\n",
            "  ✓ df: Full dataset with all features\n",
            "  ✓ train_df: Training set with features\n",
            "  ✓ test_df: Test set with features\n",
            "  ✓ test_baseline: Test set with baseline forecasts\n",
            "  ✓ all_model_features: List of numeric features for ML modeling\n",
            "\n",
            "Baseline Performance Summary:\n",
            "           Model     RMSE         MAPE      WAPE\n",
            "     Naïve (j-1) 7.487012 1.013699e+11 40.947706\n",
            " Moving Avg (7d) 5.648364 9.571820e+10 30.892419\n",
            "Dataset Forecast 2.971703 9.824110e+09 16.304784\n",
            "\n",
            "Total features ready for modeling: 32\n",
            "All features are numeric: True\n",
            "\n",
            "✅ Ready for Block C: Forecast Model & Metrics\n",
            "\n",
            "================================================================================\n",
            "BLOCK C: FORECAST MODEL & METRICS\n",
            "================================================================================\n",
            "\n",
            "[1] Adding Naive Seasonality Baseline (j-7)...\n",
            "✓ Baseline_Naive_Season created (j-7 lag)\n",
            "\n",
            "[1.1] Evaluating All Baselines on Test Set...\n",
            "\n",
            "✓ All Baseline Performance (Test Set):\n",
            "             Model     RMSE      MAE         MAPE      WAPE\n",
            "       Naïve (j-1) 7.487012 5.942466 1.013699e+11 40.947706\n",
            "   Moving Avg (7d) 5.648364 4.483209 9.571820e+10 30.892419\n",
            "Naïve Season (j-7) 7.470791 5.923397 9.189041e+10 40.816311\n",
            "  Dataset Forecast 2.971703 2.366204 9.824110e+09 16.304784\n",
            "\n",
            "================================================================================\n",
            "[2] PREPARING DATA FOR ML MODEL\n",
            "================================================================================\n",
            "\n",
            "[2.1] Data Structure Validation...\n",
            "✓ No missing values in features\n",
            "✓ No missing values in target\n",
            "\n",
            "✓ Clean dataset: 91,250 rows\n",
            "✓ Training set: 73,000 rows\n",
            "✓ Test set: 18,250 rows\n",
            "\n",
            "[2.2] Preparing Feature Matrices...\n",
            "\n",
            "✓ X_train shape: (73000, 32)\n",
            "✓ y_train shape: (73000,)\n",
            "✓ X_test shape: (18250, 32)\n",
            "✓ y_test shape: (18250,)\n",
            "\n",
            "✓ X_train dtype: float64\n",
            "✓ y_train dtype: int64\n",
            "\n",
            "✓ Inf values in X_train: 0\n",
            "✓ NaN values in X_train: 0\n",
            "✓ Inf values in y_train: 0\n",
            "✓ NaN values in y_train: 0\n",
            "\n",
            "✓ Data validation complete - ready for training\n",
            "\n",
            "================================================================================\n",
            "[3] MULTI-HORIZON FORECAST SETUP\n",
            "================================================================================\n",
            "\n",
            "Forecasting horizons: [1, 7, 14] days (j+1, j+7, j+14)\n",
            "\n",
            "[3.1] Creating Future Targets...\n",
            "✓ Created target: Units_Sold_j1 (Units_Sold shifted by -1 days)\n",
            "✓ Created target: Units_Sold_j7 (Units_Sold shifted by -7 days)\n",
            "✓ Created target: Units_Sold_j14 (Units_Sold shifted by -14 days)\n",
            "\n",
            "[3.2] Removing Rows with NaN Targets...\n",
            "✓ Rows removed: 700 (0.77%)\n",
            "✓ Remaining rows: 90,550\n",
            "\n",
            "✓ Clean training set: 73,000 rows\n",
            "✓ Clean test set: 17,550 rows\n",
            "\n",
            "================================================================================\n",
            "[4] XGBOOST MODEL TRAINING\n",
            "================================================================================\n",
            "\n",
            "XGBoost Hyperparameters:\n",
            "  n_estimators: 200\n",
            "  max_depth: 6\n",
            "  learning_rate: 0.1\n",
            "  min_child_weight: 3\n",
            "  subsample: 0.8\n",
            "  colsample_bytree: 0.8\n",
            "  gamma: 0.1\n",
            "  reg_alpha: 0.1\n",
            "  reg_lambda: 1.0\n",
            "  random_state: 42\n",
            "  n_jobs: -1\n",
            "  verbosity: 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[4.1] Training XGBoost for horizon j+1...\n",
            "  Training samples: 73,000\n",
            "  Test samples: 17,550\n",
            "  Features: 32\n",
            "  Training in progress...\n",
            "  ✓ Model trained successfully\n",
            "  Test Set Performance:\n",
            "    RMSE: 7.07\n",
            "    MAE: 5.62\n",
            "    MAPE: 88514647706.29%\n",
            "    WAPE: 39.23%\n",
            "\n",
            "[4.2] Training XGBoost for horizon j+7...\n",
            "  Training samples: 73,000\n",
            "  Test samples: 17,550\n",
            "  Features: 32\n",
            "  Training in progress...\n",
            "  ✓ Model trained successfully\n",
            "  Test Set Performance:\n",
            "    RMSE: 6.72\n",
            "    MAE: 5.33\n",
            "    MAPE: 85752764363.19%\n",
            "    WAPE: 36.81%\n",
            "\n",
            "[4.3] Training XGBoost for horizon j+14...\n",
            "  Training samples: 73,000\n",
            "  Test samples: 17,550\n",
            "  Features: 32\n",
            "  Training in progress...\n",
            "  ✓ Model trained successfully\n",
            "  Test Set Performance:\n",
            "    RMSE: 7.03\n",
            "    MAE: 5.58\n",
            "    MAPE: 79402713005.04%\n",
            "    WAPE: 38.04%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "✓ All horizon models trained successfully\n",
            "\n",
            "================================================================================\n",
            "[5] TIME-SERIES CROSS-VALIDATION\n",
            "================================================================================\n",
            "\n",
            "[5.1] Walk-Forward Validation (Expanding Window)...\n",
            "Using TimeSeriesSplit with 5 folds\n",
            "\n",
            "  Validating horizon j+1...\n",
            "    Cross-Validation Results:\n",
            "      RMSE: 5.59 ± 0.07\n",
            "      MAE:  4.40\n",
            "      WAPE: 20.57%\n",
            "\n",
            "  Validating horizon j+7...\n",
            "    Cross-Validation Results:\n",
            "      RMSE: 5.59 ± 0.08\n",
            "      MAE:  4.41\n",
            "      WAPE: 20.64%\n",
            "\n",
            "  Validating horizon j+14...\n",
            "    Cross-Validation Results:\n",
            "      RMSE: 5.58 ± 0.07\n",
            "      MAE:  4.40\n",
            "      WAPE: 20.65%\n",
            "\n",
            "✓ Cross-Validation Summary:\n",
            "Horizon  RMSE_mean  RMSE_std  MAE_mean    MAPE_mean  WAPE_mean\n",
            "    j+1   5.591969  0.071238  4.399476 6.935315e+10  20.574245\n",
            "    j+7   5.591919  0.083753  4.406161 7.110687e+10  20.644201\n",
            "   j+14   5.584166  0.066492  4.396581 7.188427e+10  20.650008\n",
            "\n",
            "================================================================================\n",
            "[6] TECHNICAL METRICS COMPARISON\n",
            "================================================================================\n",
            "\n",
            "[6.1] Model Comparison for j+1 Horizon...\n",
            "\n",
            "j+1 Horizon - All Models Comparison:\n",
            "             Model     RMSE      MAE         MAPE     SMAPE      WAPE\n",
            "       Naïve (j-1) 7.454019 5.923647 1.015385e+11 46.737846 41.355881\n",
            "           MA (7d) 5.645552 4.482165 9.720798e+10 34.825843 31.292192\n",
            "Naïve Season (j-7) 7.476394 5.934986 9.589744e+10 47.129067 41.435044\n",
            "  Dataset Forecast 8.019020 6.393038 1.064399e+11 50.781919 44.632929\n",
            "           XGBoost 7.066554 5.618751 8.851465e+10 43.976600 39.227253\n",
            "\n",
            "🎯 XGBoost Performance:\n",
            "  Best Baseline RMSE: 5.65\n",
            "  XGBoost RMSE: 7.07\n",
            "  Improvement: -25.17%\n",
            "\n",
            "================================================================================\n",
            "[7] BUSINESS METRICS - SUPPLY CHAIN KPIs\n",
            "================================================================================\n",
            "\n",
            "[7.1] Calculating KPIs for Baseline Scenario...\n",
            "(Using current inventory levels and actual demand)\n",
            "\n",
            "📊 Baseline Scenario Results:\n",
            "  Fill Rate: 100.00%\n",
            "  Weighted Fill Rate: 100.00%\n",
            "  Stockout Rate: 0.00%\n",
            "  Weighted Stockout Rate: 0.00%\n",
            "  Holding Cost: €35,568.76\n",
            "  Stockout Cost: €0.00\n",
            "  Total Cost: €35,568.76\n",
            "  Total Unmet Demand: 0 units\n",
            "\n",
            "[7.2] Simulating KPIs with XGBoost Forecast...\n",
            "(Note: This is a simplified simulation using j+1 forecast)\n",
            "\n",
            "📊 XGBoost Scenario Results:\n",
            "  Fill Rate: 100.00%\n",
            "  Weighted Fill Rate: 100.00%\n",
            "  Stockout Rate: 0.00%\n",
            "  Weighted Stockout Rate: 0.00%\n",
            "  Holding Cost: €35,568.76\n",
            "  Stockout Cost: €0.00\n",
            "  Total Cost: €35,568.76\n",
            "  Total Unmet Demand: 0 units\n",
            "\n",
            "================================================================================\n",
            "[8] BUSINESS IMPACT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "🎯 IMPACT OF XGBOOST FORECASTING vs BASELINE:\n",
            "\n",
            "💰 Cost Impact:\n",
            "  Total Cost Savings: €0.00 (+0.00%)\n",
            "    Holding Cost Change: €0.00\n",
            "    Stockout Cost Change: €0.00\n",
            "\n",
            "📈 Service Level Impact:\n",
            "  Fill Rate Change: +0.00 percentage points\n",
            "  Stockout Rate Reduction: +0.00 percentage points\n",
            "\n",
            "📦 Operational Impact:\n",
            "  Unmet Demand Reduction: 0 units\n",
            "\n",
            "📊 Side-by-Side Comparison:\n",
            "                Scenario  Fill_Rate  Weighted_Fill_Rate  Stockout_Rate  Weighted_Stockout_Rate  Holding_Cost  Stockout_Cost   Total_Cost\n",
            "Baseline (Current State)      100.0               100.0            0.0                     0.0  35568.761479            0.0 35568.761479\n",
            "        XGBoost Forecast      100.0               100.0            0.0                     0.0  35568.761479            0.0 35568.761479\n",
            "\n",
            "================================================================================\n",
            "[9] FEATURE IMPORTANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "🔍 Top 20 Most Important Features (j+1 model):\n",
            "             Feature  Importance\n",
            "               month    0.610307\n",
            "           day_trend    0.183598\n",
            "         week_number    0.114118\n",
            "              ma_28j    0.028150\n",
            "               ma_7j    0.007477\n",
            "      Warehouse_WH_4    0.002566\n",
            "     Inventory_Level    0.002336\n",
            "                  CV    0.002287\n",
            "       volatility_j7    0.002267\n",
            "         Region_West    0.002260\n",
            "       days_of_stock    0.002253\n",
            "         lag_sell_j7    0.002249\n",
            "       Reorder_Point    0.002238\n",
            "      volatility_j14    0.002235\n",
            "leadtime_variability    0.002233\n",
            "      SKU_ID_encoded    0.002223\n",
            "        lag_sell_j14    0.002216\n",
            "         lag_sell_j1    0.002173\n",
            " Supplier_ID_encoded    0.002150\n",
            "   promotion_density    0.002140\n",
            "\n",
            "📊 Feature Importance by Group:\n",
            "  Time: 0.9101\n",
            "  Lag: 0.0066\n",
            "  Moving_Avg: 0.0356\n",
            "  Volatility: 0.0045\n",
            "  Inventory: 0.0068\n",
            "  Promotion: 0.0063\n",
            "  Lead_Time: 0.0064\n",
            "  Classification: 0.0044\n",
            "\n",
            "================================================================================\n",
            "BLOCK C COMPLETE ✓\n",
            "================================================================================\n",
            "\n",
            "📦 Key Outputs Created:\n",
            "  ✓ models: Dictionary of XGBoost models {j1, j7, j14}\n",
            "  ✓ predictions_test: Test predictions for each horizon\n",
            "  ✓ test_df_clean: Test data with predictions and targets\n",
            "  ✓ comparison_j1_df: Technical metrics comparison (all models)\n",
            "  ✓ cv_results_df: Cross-validation results\n",
            "  ✓ baseline_kpis: Business KPIs for baseline scenario\n",
            "  ✓ xgb_kpis: Business KPIs for XGBoost scenario\n",
            "  ✓ baseline_sku_kpis: SKU-level KPIs (baseline)\n",
            "  ✓ xgb_sku_kpis: SKU-level KPIs (XGBoost)\n",
            "  ✓ feature_importance: Feature ranking\n",
            "  ✓ impact_comparison: Business impact summary\n",
            "\n",
            "🎯 Final Performance Summary:\n",
            "  Technical: -25.17% RMSE improvement over best baseline\n",
            "  Business: €0.00 total cost savings (+0.00%)\n",
            "  Service: +0.00pp fill rate improvement\n",
            "  Operations: 0 units less unmet demand\n",
            "\n",
            "✅ Ready for Block D: Inventory Simulation & Monte Carlo\n",
            "\n",
            "================================================================================\n",
            "BLOCK D: INVENTORY SIMULATION & MONTE CARLO\n",
            "================================================================================\n",
            "\n",
            "[1] CALCULATING SAFETY STOCK AND REORDER POINTS...\n",
            "\n",
            "Service Level Targets by ABC Class:\n",
            "  Class A: 90% (z = 1.28)\n",
            "  Class B: 85% (z = 1.06)\n",
            "  Class C: 80% (z = 0.84)\n",
            "\n",
            "[1.1] Computing Safety Stock per SKU...\n",
            "Safety stock calculated for 50 SKUs\n",
            "\n",
            "[2] CALCULATING REORDER POINTS...\n",
            "Calculating Reorder_Point: avg_demand × lead_time + safety_stock...\n",
            "Reorder points calculated\n",
            "\n",
            "[3] SETTING REORDER QUANTITY LOGIC...\n",
            "Using dataset Order_Quantity values...\n",
            "Reorder quantities set\n",
            "\n",
            "[4] PREPARING SIMULATION DATA...\n",
            "Initial inventory set for 50 SKUs\n",
            "\n",
            "[4.1] Building Supplier Lead Time Distributions...\n",
            "Lead time distributions created for 10 suppliers\n",
            "\n",
            "[5] DEFINING INVENTORY SIMULATION FUNCTION...\n",
            "Inventory simulation function defined\n",
            "\n",
            "================================================================================\n",
            "[6] XGBOOST j+1 FORECAST SIMULATION (Deterministic)\n",
            "================================================================================\n",
            "\n",
            "Running XGBoost j+1 forecast simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XGBoost j+1 Sim: 100%|██████████| 50/50 [00:00<00:00, 119.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost j+1 simulation complete: 50 SKUs\n",
            "\n",
            "================================================================================\n",
            "[7] XGBOOST j+7 FORECAST SIMULATION (Deterministic)\n",
            "================================================================================\n",
            "\n",
            "Running XGBoost j+7 forecast simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XGBoost j+7 Sim: 100%|██████████| 50/50 [00:00<00:00, 113.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost j+7 simulation complete: 50 SKUs\n",
            "\n",
            "[7.5] XGBOOST j+14 FORECAST SIMULATION...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XGBoost j+14 Sim: 100%|██████████| 50/50 [00:00<00:00, 108.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost j+14 simulation complete: 50 SKUs\n",
            "\n",
            "================================================================================\n",
            "[8] MONTE CARLO SIMULATION\n",
            "================================================================================\n",
            "\n",
            "[8.1] Selecting SKUs for Monte Carlo...\n",
            "\n",
            "Top 50 SKUs selected for full Monte Carlo (1000 runs)\n",
            "Remaining 0 SKUs will use reduced Monte Carlo (200 runs)\n",
            "\n",
            "[9] RUNNING MONTE CARLO SIMULATION...\n",
            "\n",
            "[9.1] Monte Carlo for Top 50 SKUs (1000 runs each)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MC Top SKUs: 100%|██████████| 50/50 [06:54<00:00,  8.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 50 top SKUs\n",
            "\n",
            "[9.2] Monte Carlo for Other SKUs (200 runs each)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MC Other SKUs: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 0 other SKUs\n",
            "\n",
            "Total Monte Carlo results: 50 SKUs\n",
            "\n",
            "================================================================================\n",
            "[10] BUSINESS IMPACT ESTIMATION - HORIZON j+7\n",
            "================================================================================\n",
            "\n",
            "[10.1] Calculating Technical Metrics for j+7...\n",
            "\n",
            "Technical Metrics (j+7):\n",
            "             Model     RMSE     SMAPE\n",
            "       Naïve (j-1) 7.484758 46.580694\n",
            "Naïve Season (j-7) 7.518501 47.076439\n",
            "   Moving Avg (7d) 5.656137 34.467402\n",
            "  Dataset Forecast 7.982031 50.142293\n",
            "  XGBoost ML (j+7) 6.716289 41.027527\n",
            "\n",
            "[10.2] Aggregating KPIs for j+7...\n",
            "\n",
            "Comparison Table (j+7):\n",
            "             Model     RMSE     SMAPE  Fill_Rate_%  Stockout_Rate_%  Holding_Cost_€  Stockout_Cost_€  Total_Cost_€      WAPE\n",
            "  Dataset Forecast 7.982031 50.142293        100.0              0.0    21986.623062              0.0  21986.623062 43.848054\n",
            "   Moving Avg (7d) 5.656137 34.467402        100.0              0.0    21986.623062              0.0  21986.623062 30.941810\n",
            "       Naïve (j-1) 7.484758 46.580694        100.0              0.0    21986.623062              0.0  21986.623062 41.029607\n",
            "Naïve Season (j-7) 7.518501 47.076439        100.0              0.0    21986.623062              0.0  21986.623062 41.348826\n",
            "  XGBoost ML (j+7) 6.716289 41.027527        100.0              0.0    21415.326179              0.0  21415.326179 36.807106\n",
            "\n",
            "================================================================================\n",
            "BLOCK D COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Key Outputs Created:\n",
            "  xgb_sim_df - XGBoost j+1 simulation results\n",
            "  xgb_j7_sim_df - XGBoost j+7 simulation results\n",
            "  mc_combined_df - Monte Carlo simulation results\n",
            "  comparison_j7_table - j+7 comparison table\n",
            "\n",
            "Ready for Block E: Business Impact & Export\n",
            "\n",
            "================================================================================\n",
            "BLOCK E: BUSINESS IMPACT & EXPORT\n",
            "================================================================================\n",
            "\n",
            "[1] CALCULATING TECHNICAL METRICS FOR j+1 HORIZON...\n",
            "\n",
            "Technical Metrics (j+1 Horizon):\n",
            "             Model      MAE     RMSE     SMAPE      WAPE\n",
            "       Naïve (j-1) 5.923647 7.454019 46.737846 41.355881\n",
            "Naïve Season (j-7) 5.934986 7.476394 47.129067 41.435044\n",
            "   Moving Avg (7d) 4.482165 5.645552 34.825843 31.292192\n",
            "  Dataset Forecast 6.393038 8.019020 50.781919 44.632929\n",
            "        XGBoost ML 5.618751 7.066554 43.976600 39.227253\n",
            "\n",
            "================================================================================\n",
            "[2] RUNNING INVENTORY SIMULATIONS FOR ALL MODELS (j+1)\n",
            "================================================================================\n",
            "\n",
            "[2.1] Naïve (j-1) simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Naïve Sim: 100%|██████████| 50/50 [00:00<00:00, 188.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 50 SKUs\n",
            "\n",
            "[2.2] Naïve Season (j-7) simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Naïve Season Sim: 100%|██████████| 50/50 [00:00<00:00, 188.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 50 SKUs\n",
            "\n",
            "[2.3] Moving Average simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MA Sim: 100%|██████████| 50/50 [00:00<00:00, 179.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 50 SKUs\n",
            "\n",
            "[2.4] Dataset Forecast simulation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dataset Sim: 100%|██████████| 50/50 [00:00<00:00, 192.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed: 50 SKUs\n",
            "\n",
            "All baseline simulations complete\n",
            "\n",
            "================================================================================\n",
            "[3] AGGREGATING BUSINESS KPIs (j+1, DEMAND-WEIGHTED)\n",
            "================================================================================\n",
            "\n",
            "Business KPIs (j+1, Demand-Weighted):\n",
            "             Model  Fill_Rate_%  Stockout_Rate_%  Holding_Cost_€  Stockout_Cost_€  Total_Cost_€\n",
            "       Naïve (j-1)    99.995608         0.028495    18310.712201           74.943  18385.655201\n",
            "Naïve Season (j-7)    99.991113         0.052641    18580.966117          150.348  18731.314117\n",
            "   Moving Avg (7d)   100.000000         0.000000    18442.821935            0.000  18442.821935\n",
            "  Dataset Forecast    99.967395         0.102272    18365.352420          825.531  19190.883420\n",
            "        XGBoost ML   100.000000         0.000000    21986.623062            0.000  21986.623062\n",
            "\n",
            "[4] CREATING COMPARISON TABLE (j+1)...\n",
            "\n",
            "Comparison Table (j+1):\n",
            "             Model     RMSE     SMAPE  Fill_Rate_%  Stockout_Rate_%  Holding_Cost_€  Stockout_Cost_€  Total_Cost_€      WAPE\n",
            "  Dataset Forecast 8.019020 50.781919    99.967395         0.102272    18365.352420          825.531  19190.883420 44.632929\n",
            "   Moving Avg (7d) 5.645552 34.825843   100.000000         0.000000    18442.821935            0.000  18442.821935 31.292192\n",
            "       Naïve (j-1) 7.454019 46.737846    99.995608         0.028495    18310.712201           74.943  18385.655201 41.355881\n",
            "Naïve Season (j-7) 7.476394 47.129067    99.991113         0.052641    18580.966117          150.348  18731.314117 41.435044\n",
            "        XGBoost ML 7.066554 43.976600   100.000000         0.000000    21986.623062            0.000  21986.623062 39.227253\n",
            "\n",
            "[5] MEASURING ML IMPROVEMENTS (j+1)...\n",
            "\n",
            "Comparing ML vs Best Baseline (Naïve (j-1))...\n",
            "\n",
            "Technical: RMSE +5.20%, SMAPE +5.91%\n",
            "Service: Fill Rate +0.00pp, Stockout +0.03pp\n",
            "Cost: €-3,600.97 (-19.59%)\n",
            "\n",
            "================================================================================\n",
            "[6] PREPARING DATA FOR EXPORT\n",
            "================================================================================\n",
            "\n",
            "[6.1] SKU Segmentation...\n",
            "SKU segmentation: 50 SKUs\n",
            "\n",
            "[6.2] Forecast Results j+1...\n",
            "Forecast j+1: 17550 rows\n",
            "\n",
            "[6.3] Simulation Results j+1...\n",
            "Simulation j+1: 250 rows\n",
            "\n",
            "[6.4] Forecast Results j+7...\n",
            "Forecast j+7: 17550 rows\n",
            "\n",
            "[6.5] Simulation Results j+7...\n",
            "Simulation j+7: 250 rows\n",
            "\n",
            "[7] CREATING VALIDATION REPORT...\n",
            "Validation report created\n",
            "\n",
            "================================================================================\n",
            "[8] EXPORTING TO EXCEL\n",
            "================================================================================\n",
            "\n",
            "Creating: demand_forecasting_supply.xlsx...\n",
            "  Sheet 'Forecast_J1'\n",
            "  Sheet 'Simulation_J1'\n",
            "  Sheet 'KPI_J1'\n",
            "  Sheet 'Forecast_J7'\n",
            "  Sheet 'Simulation_J7'\n",
            "  Sheet 'KPI_J7'\n",
            "  Sheet 'SKU_Segmentation'\n",
            "  Sheet 'Monte_Carlo'\n",
            "  Sheet 'Metadata'\n",
            "\n",
            "Excel file created: demand_forecasting_supply.xlsx\n",
            "\n",
            "[9] EXPORTING ADDITIONAL FILES...\n",
            "Quick check: quick_check.csv\n",
            "Validation report: validation_report.txt\n",
            "Full forecast: forecast_results_full.csv\n",
            "\n",
            "================================================================================\n",
            "BLOCK E COMPLETE\n",
            "================================================================================\n",
            "\n",
            "FILES GENERATED:\n",
            "  1. demand_forecasting_supply.xlsx\n",
            "  2. quick_check.csv\n",
            "  3. validation_report.txt\n",
            "  4. forecast_results_full.csv\n",
            "\n",
            "EXCEL STRUCTURE:\n",
            "  Forecast_J1 - Daily forecasts (horizon j+1)\n",
            "  Simulation_J1 - Inventory simulations (horizon j+1)\n",
            "  KPI_J1 - Comparison table (horizon j+1)\n",
            "  Forecast_J7 - Daily forecasts (horizon j+7)\n",
            "  Simulation_J7 - Inventory simulations (horizon j+7)\n",
            "  KPI_J7 - Comparison table (horizon j+7)\n",
            "  SKU_Segmentation - ABC/XYZ classification\n",
            "  Monte_Carlo - Stochastic simulation results\n",
            "  Metadata - Configuration parameters\n",
            "\n",
            "================================================================================\n",
            "BUSINESS IMPACT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "HORIZON j+1 (Short-term):\n",
            "  Best Baseline: Naïve (j-1)\n",
            "  RMSE: +5.20%\n",
            "  Fill Rate: +0.00pp\n",
            "  Cost Savings: €-3,600.97 (-19.59%)\n",
            "\n",
            "ANSWER: Does ML forecasting reduce costs and improve service?\n",
            "\n",
            "================================================================================\n",
            "PROJECT COMPLETE - ALL 5 BLOCKS EXECUTED\n",
            "================================================================================\n",
            "\n",
            "SUMMARY:\n",
            "  Block A - Data Loading & Preprocessing\n",
            "  Block B - Feature Engineering & Baselines\n",
            "  Block C - Forecast Models & Metrics\n",
            "  Block D - Inventory Simulation & Monte Carlo\n",
            "  Block E - Business Impact & Export\n",
            "\n",
            "The complete demand forecasting and supply chain optimization system is ready\n",
            "All results exported and ready for dashboard integration\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a720865-663e-4a55-8e91-0a90dab2b6f3\", \"demand_forecasting_supply.xlsx\", 1300233)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Téléchargement lancé pour : demand_forecasting_supply.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_824a7eeb-7245-43f3-a092-0e5045d88a11\", \"quick_check.csv\", 8514)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Téléchargement lancé pour : quick_check.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5aed0cc5-377f-4734-b2e9-94ffd0778c3e\", \"validation_report.txt\", 2026)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Téléchargement lancé pour : validation_report.txt\n"
          ]
        }
      ]
    }
  ]
}